#!/usr/bin/env python3
"""
í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ ë° í…ì„œ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
train.parquet íŒŒì¼ì„ ì½ì–´ì„œ FeatureProcessorë¡œ ë³€í™˜í•œ í›„ torch í…ì„œë¡œ ì €ì¥
"""

import pandas as pd
import torch
import numpy as np
import os
from data_loader import FeatureProcessor
import yaml
import json
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import time

def process_batch_save_files(args):
    """ë‹¨ì¼ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ë©€í‹°í”„ë¡œì„¸ì‹±ìš©)"""
    batch_data, batch_indices, processor, folders = args
    
    # FeatureProcessorë¡œ ë³€í™˜
    x_categorical, x_numerical, sequences, nan_mask = processor.transform(batch_data)
    
    # ê° ìƒ˜í”Œë³„ë¡œ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
    for j, (idx, row) in enumerate(batch_data.iterrows()):
        sample_id = batch_indices[j]
        
        # ê°œë³„ í…ì„œ ì €ì¥
        torch.save(x_categorical[j], os.path.join(folders['categorical'], f'{sample_id}.pt'))
        torch.save(x_numerical[j], os.path.join(folders['numerical'], f'{sample_id}.pt'))
        torch.save(sequences[j], os.path.join(folders['sequences'], f'{sample_id}.pt'))
        torch.save(nan_mask[j], os.path.join(folders['nan_masks'], f'{sample_id}.pt'))
        torch.save(torch.tensor(row['clicked'], dtype=torch.float32), 
                  os.path.join(folders['targets'], f'{sample_id}.pt'))
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ í˜•íƒœ ì •ë³´ ë°˜í™˜
    return {
        'categorical_shape': x_categorical[0].shape,
        'numerical_shape': x_numerical[0].shape,
        'sequence_shape': sequences[0].shape,
        'nan_mask_shape': nan_mask[0].shape
    }

def preprocess_train_data_and_save_tensors():
    """train.parquetë¥¼ ì „ì²˜ë¦¬í•˜ê³  ê°œë³„ í…ì„œ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ê²½ë¡œ ì„¤ì •
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    print("Loading train.parquet...")
    # train.parquet íŒŒì¼ ë¡œë“œ
    train_path = os.path.join(script_dir, 'train.parquet')
    df = pd.read_parquet(train_path)
    print(f"Loaded {len(df)} samples")
    
    # IDë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¬í• ë‹¹
    print("Reassigning IDs from 0...")
    df['ID'] = range(len(df))
    print(f"ID range: {df['ID'].min()} to {df['ID'].max()}")
    
    # FeatureProcessor ì´ˆê¸°í™” ë° í•™ìŠµ
    print("Initializing FeatureProcessor...")
    processor = FeatureProcessor()
    
    print("Fitting FeatureProcessor...")
    processor.fit(df)
    
    # ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ìƒˆë¡œìš´ êµ¬ì¡°)
    data_dir = os.path.join(script_dir, 'data')
    categorical_dir = os.path.join(data_dir, 'categorical')
    numerical_dir = os.path.join(data_dir, 'numerical')
    sequences_dir = os.path.join(data_dir, 'sequences')
    nan_masks_dir = os.path.join(data_dir, 'nan_masks')
    targets_dir = os.path.join(data_dir, 'targets')
    
    os.makedirs(categorical_dir, exist_ok=True)
    os.makedirs(numerical_dir, exist_ok=True)
    os.makedirs(sequences_dir, exist_ok=True)
    os.makedirs(nan_masks_dir, exist_ok=True)
    os.makedirs(targets_dir, exist_ok=True)
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
    # ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ê³ ë ¤í•˜ì—¬ ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
    num_workers = min(cpu_count(), 64)  # ìµœëŒ€ 8ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©
    batch_size = max(2000, len(df) // (num_workers * 4))  # í”„ë¡œì„¸ìŠ¤ë‹¹ 4ê°œ ë°°ì¹˜ ì •ë„
    total_batches = (len(df) + batch_size - 1) // batch_size
    
    print(f"ğŸš€ ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬: {total_batches}ê°œ ë°°ì¹˜ë¥¼ {num_workers}ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì €ì¥: ë°°ì¹˜ë³„ë¡œ ì¦‰ì‹œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€")
    print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ ìƒ˜í”Œ/ë°°ì¹˜")
    
    # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    start_time = time.time()
    
    # ë©”íƒ€ë°ì´í„°ìš© ë³€ìˆ˜ë“¤
    total_samples = len(df)
    sample_shapes = None
    
    # í´ë” ì •ë³´ ë”•ì…”ë„ˆë¦¬
    folders = {
        'categorical': categorical_dir,
        'numerical': numerical_dir,
        'sequences': sequences_dir,
        'nan_masks': nan_masks_dir,
        'targets': targets_dir
    }
    
    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
    batch_args = []
    for i in range(total_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, len(df))
        batch_df = df.iloc[start_idx:end_idx].copy()
        batch_indices = batch_df['ID'].values
        batch_args.append((batch_df, batch_indices, processor, folders))
    
    # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with Pool(processes=num_workers) as pool:
        # tqdmê³¼ í•¨ê»˜ ì§„í–‰ìƒí™© í‘œì‹œ
        results = list(tqdm(
            pool.imap(process_batch_save_files, batch_args),
            total=total_batches,
            desc="ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ë° ì €ì¥"
        ))
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ ê²°ê³¼ì—ì„œ ìƒ˜í”Œ í˜•íƒœ ì •ë³´ ì¶”ì¶œ
    if results:
        sample_shapes = results[0]
    
    # ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ
    end_time = time.time()
    processing_time = end_time - start_time
    samples_per_second = total_samples / processing_time
    
    print(f"âš¡ ì²˜ë¦¬ ì„±ëŠ¥: {processing_time:.2f}ì´ˆ, {samples_per_second:.0f} ìƒ˜í”Œ/ì´ˆ")
    
    # FeatureProcessor ì €ì¥ (ë‚˜ì¤‘ì— test ë°ì´í„° ì²˜ë¦¬ìš©)
    torch.save(processor, os.path.join(data_dir, 'feature_processor.pt'))
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'num_samples': total_samples,
        'categorical_features': processor.categorical_features,
        'numerical_features': processor.numerical_features,
        'sequential_feature': processor.sequential_feature,
        'categorical_cardinalities': processor.categorical_cardinalities,
        'sample_shapes': sample_shapes,
        'storage_structure': 'individual_files',
        'processing_info': {
            'batch_size': batch_size,
            'total_batches': total_batches,
            'num_workers': num_workers,
            'processing_time_seconds': processing_time,
            'samples_per_second': samples_per_second
        },
        'folders': {
            'categorical': categorical_dir,
            'numerical': numerical_dir,
            'sequences': sequences_dir,
            'nan_masks': nan_masks_dir,
            'targets': targets_dir
        }
    }
    
    with open(os.path.join(data_dir, 'metadata.json'), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {data_dir}/")
    print(f"ğŸ“ ì €ì¥ êµ¬ì¡°:")
    print(f"  - categorical/{'{ID}.pt'}")
    print(f"  - numerical/{'{ID}.pt'}")
    print(f"  - sequences/{'{ID}.pt'}")
    print(f"  - nan_masks/{'{ID}.pt'}")
    print(f"  - targets/{'{ID}.pt'}")
    print(f"ğŸ“Š ì´ {total_samples}ê°œ ìƒ˜í”Œ ì €ì¥")
    print(f"ğŸš€ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ {num_workers}ê°œ í”„ë¡œì„¸ìŠ¤ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ì²˜ë¦¬ ì™„ë£Œ")
    
    return metadata

if __name__ == "__main__":
    # ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œ ì•ˆì „í•œ ì‹¤í–‰ì„ ìœ„í•œ ì„¤ì •
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    
    metadata = preprocess_train_data_and_save_tensors()
    print("Train data preprocessing completed successfully!")
