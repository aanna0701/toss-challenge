#!/usr/bin/env python3
"""
ì „ì²´ train setì—ì„œ mean/stdë¥¼ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
í‘œì¤€í™”(normalization)ì— ì‚¬ìš©í•  í†µê³„ê°’ì„ ê³„ì‚°í•˜ê³  JSON í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
from datetime import datetime
import argparse


def load_parquet_chunked_precise(file_path, chunk_size=100000):
    """ì²­í¬ ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ì—¬ double precisionìœ¼ë¡œ ì •í™•í•œ í†µê³„ ê³„ì‚°"""
    print(f"ğŸ“Š {file_path} ì²­í¬ ë‹¨ìœ„ë¡œ ë¡œë“œ ì¤‘...")
    
    try:
        import pyarrow.parquet as pq
        parquet_file = pq.ParquetFile(file_path)
        total_rows = parquet_file.metadata.num_rows
        total_chunks = (total_rows + chunk_size - 1) // chunk_size
        
        print(f"ğŸ“ˆ ì´ {total_rows:,} í–‰, {total_chunks}ê°œ ì²­í¬ë¡œ ì²˜ë¦¬")
        
        # í†µê³„ ëˆ„ì ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (double precision ì‚¬ìš©)
        stats_accumulator = {}
        processed_rows = 0
        
        # ì²« ë²ˆì§¸ ì²­í¬ë¡œ ì»¬ëŸ¼ ì •ë³´ íŒŒì•…
        first_batch = next(parquet_file.iter_batches(batch_size=1000))
        first_df = first_batch.to_pandas()
        numeric_cols = first_df.select_dtypes(include=[np.number]).columns
        print(f"ğŸ“‹ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìˆ˜: {len(numeric_cols)}")
        
        # ê° ì»¬ëŸ¼ë³„ ëˆ„ì  í†µê³„ ì´ˆê¸°í™” (double precision)
        for col in numeric_cols:
            stats_accumulator[col] = {
                'sum': 0.0,           # double precision
                'sum_sq': 0.0,        # double precision  
                'count': 0,
                'min': np.inf,
                'max': -np.inf
            }
        
        # ì²­í¬ë³„ ì²˜ë¦¬
        for chunk_idx, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):
            chunk_df = batch.to_pandas()
            processed_rows += len(chunk_df)
            
            print(f"\rğŸ“Š ì²˜ë¦¬ ì¤‘: {chunk_idx+1}/{total_chunks} ì²­í¬ "
                  f"({(chunk_idx+1)/total_chunks*100:.1f}%)", end="", flush=True)
            
            # ê° ì»¬ëŸ¼ë³„ë¡œ ëˆ„ì  í†µê³„ ê³„ì‚° (double precision)
            for col in numeric_cols:
                if col in chunk_df.columns:
                    col_data = chunk_df[col].dropna()
                    if len(col_data) > 0:
                        # double precisionìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°
                        col_data_double = col_data.astype(np.float64)
                        
                        stats = stats_accumulator[col]
                        stats['sum'] += col_data_double.sum()
                        stats['sum_sq'] += (col_data_double ** 2).sum()
                        stats['count'] += len(col_data_double)
                        stats['min'] = min(stats['min'], col_data_double.min())
                        stats['max'] = max(stats['max'], col_data_double.max())
        
        print(f"\nâœ… ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
        
        # ìµœì¢… í†µê³„ ê³„ì‚° (double precision)
        final_stats = {}
        for col, stats in stats_accumulator.items():
            if stats['count'] > 0:
                # double precisionìœ¼ë¡œ ì •í™•í•œ mean/std ê³„ì‚°
                mean = stats['sum'] / stats['count']
                variance = (stats['sum_sq'] / stats['count']) - (mean ** 2)
                std = np.sqrt(max(0, variance))  # ìŒìˆ˜ ë°©ì§€
                
                final_stats[col] = {
                    'mean': float(mean),      # double precision ìœ ì§€
                    'std': float(std),        # double precision ìœ ì§€
                    'min': float(stats['min']),
                    'max': float(stats['max']),
                    'count': int(stats['count'])
                }
        
        print(f"âœ… ì •í™•í•œ í†µê³„ ê³„ì‚° ì™„ë£Œ: {len(final_stats)}ê°œ ì»¬ëŸ¼")
        return final_stats
        
    except Exception as e:
        print(f"âŒ {file_path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def compute_normalization_stats(train_data_path, output_path, feature_cols=None, exclude_cols=None):
    """
    ì „ì²´ train setì—ì„œ normalization í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        train_data_path: train.parquet íŒŒì¼ ê²½ë¡œ
        output_path: ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
        feature_cols: ê³„ì‚°í•  íŠ¹ì • ì»¬ëŸ¼ë“¤ (Noneì´ë©´ ëª¨ë“  ìˆ«ì ì»¬ëŸ¼)
        exclude_cols: ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ID, target ë“±)
    """
    
    print("ğŸš€ Normalization í†µê³„ ê³„ì‚° ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° íŒŒì¼: {train_data_path}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ê¸°ë³¸ ì„¤ì •
    if exclude_cols is None:
        exclude_cols = {'ID', 'clicked', 'seq'}  # ID, target, sequence ì»¬ëŸ¼ ì œì™¸
    
    try:
        # ë°ì´í„° ë¡œë“œ (ì²­í¬ ë‹¨ìœ„, double precision)
        print("\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        stats = load_parquet_chunked_precise(train_data_path)
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ í•„í„°ë§
        if exclude_cols:
            stats = {col: stat for col, stat in stats.items() if col not in exclude_cols}
        
        # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê³„ì‚°í•˜ëŠ” ê²½ìš°
        if feature_cols:
            stats = {col: stat for col, stat in stats.items() if col in feature_cols}
        
        print(f"âœ… ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì™„ë£Œ (double precision)")
        print(f"ğŸ“ˆ ê³„ì‚°ëœ ì»¬ëŸ¼ ìˆ˜: {len(stats)}")
        print(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {list(stats.keys())[:10]}{'...' if len(stats) > 10 else ''}")
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'data_file': train_data_path,
                'total_columns': len(stats),
                'excluded_columns': list(exclude_cols) if exclude_cols else [],
                'description': 'Normalization statistics for feature scaling'
            },
            'statistics': stats
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… í†µê³„ ê³„ì‚° ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(stats)}ê°œ ì»¬ëŸ¼ ì²˜ë¦¬")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“‹ ìš”ì•½ ì •ë³´:")
        for i, (col, stat) in enumerate(list(stats.items())[:5]):
            print(f"  {col}: mean={stat['mean']:.4f}, std={stat['std']:.4f}")
        if len(stats) > 5:
            print(f"  ... ë° {len(stats) - 5}ê°œ ì»¬ëŸ¼ ë”")
        
        return result
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Train set normalization í†µê³„ ê³„ì‚°')
    parser.add_argument('--train_data', required=True, help='Train ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì˜ˆ: data/train.parquet)')
    parser.add_argument('--output', default='results/normalization_stats.json', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--exclude', nargs='*', default=['ID', 'clicked', 'seq'], help='ì œì™¸í•  ì»¬ëŸ¼ë“¤')
    
    args = parser.parse_args()
    
    # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    output_path = args.output
    if not os.path.isabs(output_path):
        output_path = os.path.join(os.getcwd(), output_path)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # í†µê³„ ê³„ì‚° ì‹¤í–‰
    compute_normalization_stats(
        train_data_path=args.train_data,
        output_path=output_path,
        exclude_cols=set(args.exclude)
    )


if __name__ == "__main__":
    main()
