#!/usr/bin/env python3
"""
ì „ì²´ train setì—ì„œ mean/stdë¥¼ ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
í‘œì¤€í™”(normalization)ì— ì‚¬ìš©í•  í†µê³„ê°’ì„ ê³„ì‚°í•˜ê³  JSON í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import json
import os
from pathlib import Path
import yaml
from datetime import datetime
import argparse


def load_config(config_path="../config.yaml"):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def safe_load_parquet(file_path, sample_size=None, use_sampling=False):
    """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
    print(f"âš ï¸  {file_path} ëŒ€ìš©ëŸ‰ ë°ì´í„° - ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬...")
    
    try:
        import pyarrow.parquet as pq
        parquet_file = pq.ParquetFile(file_path)
        total_rows = parquet_file.metadata.num_rows
        print(f"ğŸ“Š ì´ {total_rows:,} í–‰ ì²˜ë¦¬ ì˜ˆì •")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ í†µê³„ ê³„ì‚°
        stats_accumulator = {}
        processed_rows = 0
        
        for batch in parquet_file.iter_batches(batch_size=10000):  # ë°°ì¹˜ í¬ê¸° ì¤„ì„
            chunk_df = batch.to_pandas()
            processed_rows += len(chunk_df)
            
            # ê° ì»¬ëŸ¼ë³„ë¡œ ëˆ„ì  í†µê³„ ê³„ì‚°
            for col in chunk_df.select_dtypes(include=[np.number]).columns:
                if col not in stats_accumulator:
                    stats_accumulator[col] = {
                        'sum': 0.0,
                        'sum_sq': 0.0,
                        'count': 0,
                        'min': np.inf,
                        'max': -np.inf
                    }
                
                col_data = chunk_df[col].dropna()
                if len(col_data) > 0:
                    stats_accumulator[col]['sum'] += col_data.sum()
                    stats_accumulator[col]['sum_sq'] += (col_data ** 2).sum()
                    stats_accumulator[col]['count'] += len(col_data)
                    stats_accumulator[col]['min'] = min(stats_accumulator[col]['min'], col_data.min())
                    stats_accumulator[col]['max'] = max(stats_accumulator[col]['max'], col_data.max())
            
            if processed_rows % 100000 == 0:  # 10ë§Œ í–‰ë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
                print(f"ğŸ“ˆ ì§„í–‰ë¥ : {processed_rows:,}/{total_rows:,} ({processed_rows/total_rows*100:.1f}%)")
        
        print(f"ğŸ“ˆ ìµœì¢… ì§„í–‰ë¥ : {processed_rows:,}/{total_rows:,} (100.0%)")
        
        # ìµœì¢… í†µê³„ ê³„ì‚°
        final_stats = {}
        for col, stats in stats_accumulator.items():
            if stats['count'] > 0:
                mean = stats['sum'] / stats['count']
                variance = (stats['sum_sq'] / stats['count']) - (mean ** 2)
                std = np.sqrt(max(0, variance))  # ìŒìˆ˜ ë°©ì§€
                
                final_stats[col] = {
                    'mean': float(mean),
                    'std': float(std),
                    'min': float(stats['min']),
                    'max': float(stats['max']),
                    'count': int(stats['count'])
                }
        
        return final_stats
        
    except Exception as e:
        print(f"âŒ {file_path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


def compute_normalization_stats(train_data_path, output_path, feature_cols=None, exclude_cols=None):
    """
    ì „ì²´ train setì—ì„œ normalization í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        train_data_path: train.parquet íŒŒì¼ ê²½ë¡œ
        output_path: ê²°ê³¼ë¥¼ ì €ì¥í•  JSON íŒŒì¼ ê²½ë¡œ
        feature_cols: ê³„ì‚°í•  íŠ¹ì • ì»¬ëŸ¼ë“¤ (Noneì´ë©´ ëª¨ë“  ìˆ«ì ì»¬ëŸ¼)
        exclude_cols: ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ID, target ë“±)
    """
    
    print("ğŸš€ Normalization í†µê³„ ê³„ì‚° ì‹œì‘")
    print(f"ğŸ“ ë°ì´í„° íŒŒì¼: {train_data_path}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ê¸°ë³¸ ì„¤ì •
    if exclude_cols is None:
        exclude_cols = {'ID', 'clicked', 'seq'}  # ID, target, sequence ì»¬ëŸ¼ ì œì™¸
    
    try:
        # ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        print("\nğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
        stats = safe_load_parquet(train_data_path)
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ í•„í„°ë§
        if exclude_cols:
            stats = {col: stat for col, stat in stats.items() if col not in exclude_cols}
        
        # íŠ¹ì • ì»¬ëŸ¼ë§Œ ê³„ì‚°í•˜ëŠ” ê²½ìš°
        if feature_cols:
            stats = {col: stat for col, stat in stats.items() if col in feature_cols}
        
        print(f"âœ… ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ ì™„ë£Œ")
        print(f"ğŸ“ˆ ê³„ì‚°ëœ ì»¬ëŸ¼ ìˆ˜: {len(stats)}")
        print(f"ğŸ“‹ ì»¬ëŸ¼ ëª©ë¡: {list(stats.keys())[:10]}{'...' if len(stats) > 10 else ''}")
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'metadata': {
                'created_at': datetime.now().isoformat(),
                'data_file': train_data_path,
                'total_columns': len(stats),
                'excluded_columns': list(exclude_cols) if exclude_cols else [],
                'description': 'Normalization statistics for feature scaling'
            },
            'statistics': stats
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… í†µê³„ ê³„ì‚° ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(stats)}ê°œ ì»¬ëŸ¼ ì²˜ë¦¬")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“‹ ìš”ì•½ ì •ë³´:")
        for i, (col, stat) in enumerate(list(stats.items())[:5]):
            print(f"  {col}: mean={stat['mean']:.4f}, std={stat['std']:.4f}")
        if len(stats) > 5:
            print(f"  ... ë° {len(stats) - 5}ê°œ ì»¬ëŸ¼ ë”")
        
        return result
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Train set normalization í†µê³„ ê³„ì‚°')
    parser.add_argument('--config', default='../config.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', default='results/normalization_stats.json', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--exclude', nargs='*', default=['ID', 'clicked', 'seq'], help='ì œì™¸í•  ì»¬ëŸ¼ë“¤')
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    train_data_path = config['PATHS']['TRAIN_DATA']
    
    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not os.path.isabs(train_data_path):
        train_data_path = os.path.join(os.path.dirname(args.config), train_data_path)
    
    # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    output_path = args.output
    if not os.path.isabs(output_path):
        output_path = os.path.join(os.path.dirname(args.config), 'analysis', output_path)
    
    # í†µê³„ ê³„ì‚° ì‹¤í–‰
    compute_normalization_stats(
        train_data_path=train_data_path,
        output_path=output_path,
        exclude_cols=set(args.exclude)
    )


if __name__ == "__main__":
    main()
