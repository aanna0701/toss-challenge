#!/usr/bin/env python3
"""
EDA ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder
import warnings

warnings.filterwarnings('ignore')

# Set English locale and font settings for plots
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def analyze_categorical_feature(df, feature, target='clicked', max_categories=20):
    """
    ì¹´í…Œê³ ë¦¬í˜• í”¼ì²˜ ë¶„ì„
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature: ë¶„ì„í•  í”¼ì²˜ëª…
        target: íƒ€ê²Ÿ ë³€ìˆ˜ëª…
        max_categories: í‘œì‹œí•  ìµœëŒ€ ì¹´í…Œê³ ë¦¬ ìˆ˜
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    print(f"\nğŸ“Š {feature} ë¶„ì„:")
    
    # ê¸°ë³¸ ì •ë³´
    unique_count = df[feature].nunique()
    missing_count = df[feature].isnull().sum()
    missing_pct = missing_count / len(df) * 100
    
    print(f"   - ê³ ìœ ê°’ ê°œìˆ˜: {unique_count:,}")
    print(f"   - ê²°ì¸¡ê°’: {missing_count:,}ê°œ ({missing_pct:.2f}%)")
    
    # ê°’ë³„ ë¶„í¬
    value_counts = df[feature].value_counts().head(max_categories)
    print(f"   - ìƒìœ„ {min(max_categories, len(value_counts))}ê°œ ê°’:")
    
    results = {'feature': feature, 'categories': {}}
    
    for val, count in value_counts.items():
        pct = count / len(df) * 100
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í´ë¦­ë¥ 
        category_data = df[df[feature] == val]
        ctr = category_data[target].mean()
        
        print(f"     * {val}: {count:,}ê°œ ({pct:.2f}%) - CTR: {ctr:.6f}")
        
        results['categories'][str(val)] = {
            'count': int(count),
            'percentage': float(pct),
            'ctr': float(ctr)
        }
    
    # ì „ì²´ í´ë¦­ë¥ ê³¼ ë¹„êµ
    overall_ctr = df[target].mean()
    print(f"   - ì „ì²´ CTR: {overall_ctr:.6f}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ CTR í¸ì°¨
    ctr_by_category = df.groupby(feature)[target].mean().sort_values(ascending=False)
    max_ctr = ctr_by_category.max()
    min_ctr = ctr_by_category.min()
    
    print(f"   - CTR ë²”ìœ„: {min_ctr:.6f} ~ {max_ctr:.6f}")
    print(f"   - CTR í¸ì°¨: {(max_ctr - min_ctr):.6f}")
    
    results.update({
        'unique_count': int(unique_count),
        'missing_count': int(missing_count),
        'missing_percentage': float(missing_pct),
        'overall_ctr': float(overall_ctr),
        'ctr_range': [float(min_ctr), float(max_ctr)],
        'ctr_deviation': float(max_ctr - min_ctr)
    })
    
    return results

def analyze_numerical_feature(df, feature, target='clicked'):
    """
    ìˆ˜ì¹˜í˜• í”¼ì²˜ ë¶„ì„
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature: ë¶„ì„í•  í”¼ì²˜ëª…
        target: íƒ€ê²Ÿ ë³€ìˆ˜ëª…
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    print(f"\nğŸ“ˆ {feature} ë¶„ì„:")
    
    # ê¸°ë³¸ í†µê³„
    desc = df[feature].describe()
    missing_count = df[feature].isnull().sum()
    missing_pct = missing_count / len(df) * 100
    
    print(f"   - ê²°ì¸¡ê°’: {missing_count:,}ê°œ ({missing_pct:.2f}%)")
    print(f"   - ê¸°ë³¸ í†µê³„:")
    for stat in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:
        print(f"     * {stat}: {desc[stat]:.4f}")
    
    # 0ê°’ ë¹„ìœ¨ (sparse feature ì²´í¬)
    zero_count = (df[feature] == 0).sum()
    zero_pct = zero_count / len(df) * 100
    print(f"   - 0ê°’: {zero_count:,}ê°œ ({zero_pct:.2f}%)")
    
    # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„
    correlation = df[feature].corr(df[target])
    print(f"   - {target}ì™€ ìƒê´€ê´€ê³„: {correlation:.6f}")
    
    # êµ¬ê°„ë³„ í´ë¦­ë¥  ë¶„ì„
    try:
        # 10ë¶„ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
        df_temp = df[[feature, target]].copy()
        df_temp = df_temp.dropna()
        
        if len(df_temp) > 0:
            df_temp['decile'] = pd.qcut(df_temp[feature], q=10, duplicates='drop')
            ctr_by_decile = df_temp.groupby('decile')[target].agg(['count', 'mean'])
            
            print(f"   - êµ¬ê°„ë³„ CTR (10ë¶„ìœ„):")
            for decile, (count, ctr) in ctr_by_decile.iterrows():
                print(f"     * {decile}: {ctr:.6f} ({count:,}ê°œ)")
    
    except Exception as e:
        print(f"   - êµ¬ê°„ë³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    results = {
        'feature': feature,
        'missing_count': int(missing_count),
        'missing_percentage': float(missing_pct),
        'statistics': desc.to_dict(),
        'zero_count': int(zero_count),
        'zero_percentage': float(zero_pct),
        'correlation': float(correlation) if not pd.isna(correlation) else None
    }
    
    return results

def plot_feature_target_relationship(df, feature, target='clicked', figsize=(12, 5)):
    """
    Feature-target relationship visualization
    
    Args:
        df: DataFrame
        feature: Feature name to analyze
        target: Target variable name
        figsize: Figure size
    """
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    
    if df[feature].dtype == 'object' or df[feature].nunique() < 20:
        # Categorical feature
        # 1. Distribution
        value_counts = df[feature].value_counts().head(15)
        axes[0].bar(range(len(value_counts)), value_counts.values, color='lightblue')
        axes[0].set_title(f'{feature} Distribution')
        axes[0].set_xticks(range(len(value_counts)))
        axes[0].set_xticklabels(value_counts.index, rotation=45)
        axes[0].set_ylabel('Count')
        
        # 2. Click rate by category
        ctr_by_category = df.groupby(feature)[target].mean().sort_values(ascending=False).head(15)
        axes[1].bar(range(len(ctr_by_category)), ctr_by_category.values, color='orange')
        axes[1].set_title(f'Click Rate by {feature}')
        axes[1].set_xticks(range(len(ctr_by_category)))
        axes[1].set_xticklabels(ctr_by_category.index, rotation=45)
        axes[1].set_ylabel('Click Rate')
        
    else:
        # Numerical feature
        # 1. Histogram
        axes[0].hist(df[feature].dropna(), bins=50, alpha=0.7, color='lightgreen')
        axes[0].set_title(f'{feature} Distribution')
        axes[0].set_xlabel(feature)
        axes[0].set_ylabel('Frequency')
        
        # 2. Click rate by bins
        try:
            df_temp = df[[feature, target]].dropna()
            df_temp['bins'] = pd.cut(df_temp[feature], bins=20)
            ctr_by_bin = df_temp.groupby('bins')[target].mean()
            
            x_pos = range(len(ctr_by_bin))
            axes[1].plot(x_pos, ctr_by_bin.values, marker='o', color='red')
            axes[1].set_title(f'Click Rate by {feature} Bins')
            axes[1].set_xlabel('Bins')
            axes[1].set_ylabel('Click Rate')
            axes[1].set_xticks(x_pos[::max(1, len(x_pos)//5)])
            
        except Exception as e:
            axes[1].text(0.5, 0.5, f'Binning analysis failed\n{str(e)}', 
                        ha='center', va='center', transform=axes[1].transAxes)
    
    plt.tight_layout()
    plt.show()

def compare_feature_groups(df, groups_dict, target='clicked'):
    """
    í”¼ì²˜ ê·¸ë£¹ ê°„ ë¹„êµ ë¶„ì„
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        groups_dict: ê·¸ë£¹ë³„ í”¼ì²˜ ë”•ì…”ë„ˆë¦¬
        target: íƒ€ê²Ÿ ë³€ìˆ˜ëª…
    
    Returns:
        dict: ê·¸ë£¹ ë¹„êµ ê²°ê³¼
    """
    print("\nğŸ“Š í”¼ì²˜ ê·¸ë£¹ ë¹„êµ ë¶„ì„")
    print("="*50)
    
    group_stats = {}
    
    for group_name, features in groups_dict.items():
        if not features or group_name == 'target':
            continue
        
        # í•´ë‹¹ ê·¸ë£¹ì˜ ìˆ˜ì¹˜í˜• í”¼ì²˜ë“¤ë§Œ
        numeric_features = [f for f in features if f in df.columns and df[f].dtype in ['int64', 'float64']]
        
        if not numeric_features:
            continue
        
        group_data = df[numeric_features]
        
        # ê·¸ë£¹ í†µê³„
        stats = {
            'feature_count': len(numeric_features),
            'mean_correlation': group_data.corrwith(df[target]).abs().mean(),
            'max_correlation': group_data.corrwith(df[target]).abs().max(),
            'sparse_ratio': (group_data == 0).mean().mean(),  # í‰ê·  0ê°’ ë¹„ìœ¨
            'missing_ratio': group_data.isnull().mean().mean()  # í‰ê·  ê²°ì¸¡ê°’ ë¹„ìœ¨
        }
        
        print(f"\nğŸ·ï¸ {group_name}:")
        print(f"   - í”¼ì²˜ ìˆ˜: {stats['feature_count']}")
        print(f"   - í‰ê·  ìƒê´€ê´€ê³„: {stats['mean_correlation']:.6f}")
        print(f"   - ìµœëŒ€ ìƒê´€ê´€ê³„: {stats['max_correlation']:.6f}")
        print(f"   - í‰ê·  Sparse ë¹„ìœ¨: {stats['sparse_ratio']:.3f}")
        print(f"   - í‰ê·  ê²°ì¸¡ê°’ ë¹„ìœ¨: {stats['missing_ratio']:.3f}")
        
        # ìƒìœ„ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤
        correlations = group_data.corrwith(df[target]).abs().sort_values(ascending=False)
        top_features = correlations.head(3)
        print(f"   - ìƒìœ„ ìƒê´€ê´€ê³„ í”¼ì²˜:")
        for feat, corr in top_features.items():
            print(f"     * {feat}: {corr:.6f}")
        
        group_stats[group_name] = stats
        group_stats[group_name]['top_features'] = top_features.to_dict()
    
    return group_stats

def detect_outliers(df, feature, method='iqr'):
    """
    ì´ìƒì¹˜ íƒì§€
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        feature: ë¶„ì„í•  í”¼ì²˜ëª…
        method: íƒì§€ ë°©ë²• ('iqr', 'zscore')
    
    Returns:
        dict: ì´ìƒì¹˜ ì •ë³´
    """
    if df[feature].dtype not in ['int64', 'float64']:
        return {'error': 'Only numeric features supported'}
    
    data = df[feature].dropna()
    
    if method == 'iqr':
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        
    elif method == 'zscore':
        z_scores = np.abs(stats.zscore(data))
        outliers = data[z_scores > 3]
        lower_bound = data.mean() - 3 * data.std()
        upper_bound = data.mean() + 3 * data.std()
    
    outlier_count = len(outliers)
    outlier_pct = outlier_count / len(data) * 100
    
    result = {
        'method': method,
        'outlier_count': outlier_count,
        'outlier_percentage': outlier_pct,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'total_samples': len(data)
    }
    
    print(f"\nğŸ” {feature} ì´ìƒì¹˜ íƒì§€ ({method}):")
    print(f"   - ì´ìƒì¹˜ ê°œìˆ˜: {outlier_count:,}ê°œ ({outlier_pct:.2f}%)")
    print(f"   - ì •ìƒ ë²”ìœ„: {lower_bound:.4f} ~ {upper_bound:.4f}")
    
    return result

def create_feature_importance_plot(correlations, title="Feature Importance", top_n=20):
    """
    Feature importance visualization
    
    Args:
        correlations: Correlation Series
        title: Plot title
        top_n: Number of top features to display
    """
    top_corr = correlations.abs().sort_values(ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 8))
    y_pos = np.arange(len(top_corr))
    
    plt.barh(y_pos, top_corr.values, color='lightcoral')
    plt.yticks(y_pos, top_corr.index)
    plt.xlabel('Absolute Correlation with Target')
    plt.title(title)
    plt.gca().invert_yaxis()
    
    # Display values
    for i, v in enumerate(top_corr.values):
        plt.text(v + 0.001, i, f'{v:.4f}', va='center')
    
    plt.tight_layout()
    plt.show()
