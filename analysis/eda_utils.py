#!/usr/bin/env python3
"""
EDA Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder
import warnings

warnings.filterwarnings('ignore')

# Set English locale and font settings for plots
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def analyze_categorical_feature(df, feature, target='clicked', max_categories=20):
    """
    Ïπ¥ÌÖåÍ≥†Î¶¨Ìòï ÌîºÏ≤ò Î∂ÑÏÑù
    
    Args:
        df: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ
        feature: Î∂ÑÏÑùÌï† ÌîºÏ≤òÎ™Ö
        target: ÌÉÄÍ≤ü Î≥ÄÏàòÎ™Ö
        max_categories: ÌëúÏãúÌï† ÏµúÎåÄ Ïπ¥ÌÖåÍ≥†Î¶¨ Ïàò
    
    Returns:
        dict: Î∂ÑÏÑù Í≤∞Í≥º
    """
    print(f"\nüìä {feature} Î∂ÑÏÑù:")
    
    # Í∏∞Î≥∏ Ï†ïÎ≥¥
    unique_count = df[feature].nunique()
    missing_count = df[feature].isnull().sum()
    missing_pct = missing_count / len(df) * 100
    
    print(f"   - Í≥†Ïú†Í∞í Í∞úÏàò: {unique_count:,}")
    print(f"   - Í≤∞Ï∏°Í∞í: {missing_count:,}Í∞ú ({missing_pct:.2f}%)")
    
    # Í∞íÎ≥Ñ Î∂ÑÌè¨
    value_counts = df[feature].value_counts().head(max_categories)
    print(f"   - ÏÉÅÏúÑ {min(max_categories, len(value_counts))}Í∞ú Í∞í:")
    
    results = {'feature': feature, 'categories': {}}
    
    for val, count in value_counts.items():
        pct = count / len(df) * 100
        
        # Ìï¥Îãπ Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÌÅ¥Î¶≠Î•†
        category_data = df[df[feature] == val]
        ctr = category_data[target].mean()
        
        print(f"     * {val}: {count:,}Í∞ú ({pct:.2f}%) - CTR: {ctr:.6f}")
        
        results['categories'][str(val)] = {
            'count': int(count),
            'percentage': float(pct),
            'ctr': float(ctr)
        }
    
    # Ï†ÑÏ≤¥ ÌÅ¥Î¶≠Î•†Í≥º ÎπÑÍµê
    overall_ctr = df[target].mean()
    print(f"   - Ï†ÑÏ≤¥ CTR: {overall_ctr:.6f}")
    
    # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ CTR Ìé∏Ï∞®
    ctr_by_category = df.groupby(feature)[target].mean().sort_values(ascending=False)
    max_ctr = ctr_by_category.max()
    min_ctr = ctr_by_category.min()
    
    print(f"   - CTR Î≤îÏúÑ: {min_ctr:.6f} ~ {max_ctr:.6f}")
    print(f"   - CTR Ìé∏Ï∞®: {(max_ctr - min_ctr):.6f}")
    
    results.update({
        'unique_count': int(unique_count),
        'missing_count': int(missing_count),
        'missing_percentage': float(missing_pct),
        'overall_ctr': float(overall_ctr),
        'ctr_range': [float(min_ctr), float(max_ctr)],
        'ctr_deviation': float(max_ctr - min_ctr)
    })
    
    return results

def analyze_numerical_feature(df, feature, target='clicked'):
    """
    ÏàòÏπòÌòï ÌîºÏ≤ò Î∂ÑÏÑù
    
    Args:
        df: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ
        feature: Î∂ÑÏÑùÌï† ÌîºÏ≤òÎ™Ö
        target: ÌÉÄÍ≤ü Î≥ÄÏàòÎ™Ö
    
    Returns:
        dict: Î∂ÑÏÑù Í≤∞Í≥º
    """
    print(f"\nüìà {feature} Î∂ÑÏÑù:")
    
    # Í∏∞Î≥∏ ÌÜµÍ≥Ñ
    desc = df[feature].describe()
    missing_count = df[feature].isnull().sum()
    missing_pct = missing_count / len(df) * 100
    
    print(f"   - Í≤∞Ï∏°Í∞í: {missing_count:,}Í∞ú ({missing_pct:.2f}%)")
    print(f"   - Í∏∞Î≥∏ ÌÜµÍ≥Ñ:")
    for stat in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:
        print(f"     * {stat}: {desc[stat]:.4f}")
    
    # 0Í∞í ÎπÑÏú® (sparse feature Ï≤¥ÌÅ¨)
    zero_count = (df[feature] == 0).sum()
    zero_pct = zero_count / len(df) * 100
    print(f"   - 0Í∞í: {zero_count:,}Í∞ú ({zero_pct:.2f}%)")
    
    # ÌÉÄÍ≤üÍ≥ºÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
    correlation = df[feature].corr(df[target])
    print(f"   - {target}ÏôÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ: {correlation:.6f}")
    
    # Íµ¨Í∞ÑÎ≥Ñ ÌÅ¥Î¶≠Î•† Î∂ÑÏÑù
    try:
        # 10Î∂ÑÏúÑÎ°ú ÎÇòÎàÑÏñ¥ Î∂ÑÏÑù
        df_temp = df[[feature, target]].copy()
        df_temp = df_temp.dropna()
        
        if len(df_temp) > 0:
            df_temp['decile'] = pd.qcut(df_temp[feature], q=10, duplicates='drop')
            ctr_by_decile = df_temp.groupby('decile')[target].agg(['count', 'mean'])
            
            print(f"   - Íµ¨Í∞ÑÎ≥Ñ CTR (10Î∂ÑÏúÑ):")
            for decile, (count, ctr) in ctr_by_decile.iterrows():
                print(f"     * {decile}: {ctr:.6f} ({count:,}Í∞ú)")
    
    except Exception as e:
        print(f"   - Íµ¨Í∞ÑÎ≥Ñ Î∂ÑÏÑù Ïã§Ìå®: {e}")
    
    results = {
        'feature': feature,
        'missing_count': int(missing_count),
        'missing_percentage': float(missing_pct),
        'statistics': desc.to_dict(),
        'zero_count': int(zero_count),
        'zero_percentage': float(zero_pct),
        'correlation': float(correlation) if not pd.isna(correlation) else None
    }
    
    return results

def plot_feature_target_relationship(df, feature, target='clicked', figsize=(12, 5)):
    """
    Feature-target relationship visualization
    
    Args:
        df: DataFrame
        feature: Feature name to analyze
        target: Target variable name
        figsize: Figure size
    """
    fig, axes = plt.subplots(1, 2, figsize=figsize)
    
    if df[feature].dtype == 'object' or df[feature].nunique() < 20:
        # Categorical feature
        # 1. Distribution
        value_counts = df[feature].value_counts().head(15)
        axes[0].bar(range(len(value_counts)), value_counts.values, color='lightblue')
        axes[0].set_title(f'{feature} Distribution')
        axes[0].set_xticks(range(len(value_counts)))
        axes[0].set_xticklabels(value_counts.index, rotation=45)
        axes[0].set_ylabel('Count')
        
        # 2. Click rate by category
        ctr_by_category = df.groupby(feature)[target].mean().sort_values(ascending=False).head(15)
        axes[1].bar(range(len(ctr_by_category)), ctr_by_category.values, color='orange')
        axes[1].set_title(f'Click Rate by {feature}')
        axes[1].set_xticks(range(len(ctr_by_category)))
        axes[1].set_xticklabels(ctr_by_category.index, rotation=45)
        axes[1].set_ylabel('Click Rate')
        
    else:
        # Numerical feature
        # 1. Histogram
        axes[0].hist(df[feature].dropna(), bins=50, alpha=0.7, color='lightgreen')
        axes[0].set_title(f'{feature} Distribution')
        axes[0].set_xlabel(feature)
        axes[0].set_ylabel('Frequency')
        
        # 2. Click rate by bins
        try:
            df_temp = df[[feature, target]].dropna()
            df_temp['bins'] = pd.cut(df_temp[feature], bins=20)
            ctr_by_bin = df_temp.groupby('bins')[target].mean()
            
            x_pos = range(len(ctr_by_bin))
            axes[1].plot(x_pos, ctr_by_bin.values, marker='o', color='red')
            axes[1].set_title(f'Click Rate by {feature} Bins')
            axes[1].set_xlabel('Bins')
            axes[1].set_ylabel('Click Rate')
            axes[1].set_xticks(x_pos[::max(1, len(x_pos)//5)])
            
        except Exception as e:
            axes[1].text(0.5, 0.5, f'Binning analysis failed\n{str(e)}', 
                        ha='center', va='center', transform=axes[1].transAxes)
    
    plt.tight_layout()
    plt.show()

def compare_feature_groups(df, groups_dict, target='clicked'):
    """
    ÌîºÏ≤ò Í∑∏Î£π Í∞Ñ ÎπÑÍµê Î∂ÑÏÑù
    
    Args:
        df: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ
        groups_dict: Í∑∏Î£πÎ≥Ñ ÌîºÏ≤ò ÎîïÏÖîÎÑàÎ¶¨
        target: ÌÉÄÍ≤ü Î≥ÄÏàòÎ™Ö
    
    Returns:
        dict: Í∑∏Î£π ÎπÑÍµê Í≤∞Í≥º
    """
    print("\nüìä ÌîºÏ≤ò Í∑∏Î£π ÎπÑÍµê Î∂ÑÏÑù")
    print("="*50)
    
    group_stats = {}
    
    for group_name, features in groups_dict.items():
        if not features or group_name == 'target':
            continue
        
        # Ìï¥Îãπ Í∑∏Î£πÏùò ÏàòÏπòÌòï ÌîºÏ≤òÎì§Îßå
        numeric_features = [f for f in features if f in df.columns and df[f].dtype in ['int64', 'float64']]
        
        if not numeric_features:
            continue
        
        group_data = df[numeric_features]
        
        # Í∑∏Î£π ÌÜµÍ≥Ñ
        stats = {
            'feature_count': len(numeric_features),
            'mean_correlation': group_data.corrwith(df[target]).abs().mean(),
            'max_correlation': group_data.corrwith(df[target]).abs().max(),
            'sparse_ratio': (group_data == 0).mean().mean(),  # ÌèâÍ∑† 0Í∞í ÎπÑÏú®
            'missing_ratio': group_data.isnull().mean().mean()  # ÌèâÍ∑† Í≤∞Ï∏°Í∞í ÎπÑÏú®
        }
        
        print(f"\nüè∑Ô∏è {group_name}:")
        print(f"   - ÌîºÏ≤ò Ïàò: {stats['feature_count']}")
        print(f"   - ÌèâÍ∑† ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ: {stats['mean_correlation']:.6f}")
        print(f"   - ÏµúÎåÄ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ: {stats['max_correlation']:.6f}")
        print(f"   - ÌèâÍ∑† Sparse ÎπÑÏú®: {stats['sparse_ratio']:.3f}")
        print(f"   - ÌèâÍ∑† Í≤∞Ï∏°Í∞í ÎπÑÏú®: {stats['missing_ratio']:.3f}")
        
        # ÏÉÅÏúÑ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌîºÏ≤òÎì§
        correlations = group_data.corrwith(df[target]).abs().sort_values(ascending=False)
        top_features = correlations.head(3)
        print(f"   - ÏÉÅÏúÑ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌîºÏ≤ò:")
        for feat, corr in top_features.items():
            print(f"     * {feat}: {corr:.6f}")
        
        group_stats[group_name] = stats
        group_stats[group_name]['top_features'] = top_features.to_dict()
    
    return group_stats

def detect_outliers(df, feature, method='iqr'):
    """
    Ïù¥ÏÉÅÏπò ÌÉêÏßÄ
    
    Args:
        df: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ
        feature: Î∂ÑÏÑùÌï† ÌîºÏ≤òÎ™Ö
        method: ÌÉêÏßÄ Î∞©Î≤ï ('iqr', 'zscore')
    
    Returns:
        dict: Ïù¥ÏÉÅÏπò Ï†ïÎ≥¥
    """
    if df[feature].dtype not in ['int64', 'float64']:
        return {'error': 'Only numeric features supported'}
    
    data = df[feature].dropna()
    
    if method == 'iqr':
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        
    elif method == 'zscore':
        z_scores = np.abs(stats.zscore(data))
        outliers = data[z_scores > 3]
        lower_bound = data.mean() - 3 * data.std()
        upper_bound = data.mean() + 3 * data.std()
    
    outlier_count = len(outliers)
    outlier_pct = outlier_count / len(data) * 100
    
    result = {
        'method': method,
        'outlier_count': outlier_count,
        'outlier_percentage': outlier_pct,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'total_samples': len(data)
    }
    
    print(f"\nüîç {feature} Ïù¥ÏÉÅÏπò ÌÉêÏßÄ ({method}):")
    print(f"   - Ïù¥ÏÉÅÏπò Í∞úÏàò: {outlier_count:,}Í∞ú ({outlier_pct:.2f}%)")
    print(f"   - Ï†ïÏÉÅ Î≤îÏúÑ: {lower_bound:.4f} ~ {upper_bound:.4f}")
    
    return result

def create_feature_importance_plot(correlations, title="Feature Importance", top_n=20):
    """
    Feature importance visualization
    
    Args:
        correlations: Correlation Series
        title: Plot title
        top_n: Number of top features to display
    """
    top_corr = correlations.abs().sort_values(ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 8))
    y_pos = np.arange(len(top_corr))
    
    plt.barh(y_pos, top_corr.values, color='lightcoral')
    plt.yticks(y_pos, top_corr.index)
    plt.xlabel('Absolute Correlation with Target')
    plt.title(title)
    plt.gca().invert_yaxis()
    
    # Display values
    for i, v in enumerate(top_corr.values):
        plt.text(v + 0.001, i, f'{v:.4f}', va='center')
    
    plt.tight_layout()
    plt.show()
