#!/usr/bin/env python3
"""
í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- í”¼ì²˜ë³„ ê°’ ë¶„í¬ ë¶„ì„ (ë¶„ë³„ë ¥ ë‚®ì€ í”¼ì²˜ ì‹ë³„)
- í”¼ì²˜ë³„ í´ë¦­ë¥ ê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ (ì—°ê´€ì„± ë‚®ì€ í”¼ì²˜ ì‹ë³„)
"""

import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from pathlib import Path
import sys
import os
import json
from collections import defaultdict
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

warnings.filterwarnings('ignore')

# Set English locale and font settings for plots
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import seed_everything
from analysis.chunk_utils import OnlineStats, ChunkCorrelationCalculator, ChunkCategoricalAnalyzer

# ê²°ê³¼ ì €ì¥ í´ë”
RESULTS_DIR = Path("analysis/results")
RESULTS_DIR.mkdir(exist_ok=True)

class FeatureQualityAnalyzer:
    """í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path="./train.parquet", chunk_size=100000):
        """
        ì´ˆê¸°í™”
        Args:
            data_path: train.parquet íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
        """
        self.data_path = data_path
        self.chunk_size = chunk_size
        self.parquet_file = None
        self.total_rows = 0
        self.total_chunks = 0
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.feature_stats = defaultdict(lambda: OnlineStats())
        self.correlation_calculator = ChunkCorrelationCalculator()
        self.categorical_analyzer = ChunkCategoricalAnalyzer()
        self.categorical_features = []
        self.numerical_features = []
        
        self.initialize()
    
    def initialize(self):
        """ì´ˆê¸°í™”"""
        print("ğŸ” í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ ì´ˆê¸°í™”...")
        
        try:
            self.parquet_file = pq.ParquetFile(self.data_path)
            self.total_rows = self.parquet_file.metadata.num_rows
            self.total_chunks = (self.total_rows + self.chunk_size - 1) // self.chunk_size
            
            print(f"âœ… íŒŒì¼ ì •ë³´:")
            print(f"   - ì „ì²´ í–‰ ìˆ˜: {self.total_rows:,}")
            print(f"   - ì²­í¬ í¬ê¸°: {self.chunk_size:,}")
            print(f"   - ì´ ì²­í¬ ìˆ˜: {self.total_chunks}")
            
            # ì²« ë²ˆì§¸ ì²­í¬ë¡œ í”¼ì²˜ ì •ë³´ íŒŒì•…
            first_batch = next(self.parquet_file.iter_batches(batch_size=1000))
            first_df = first_batch.to_pandas()
            self.categorize_features(first_df.columns.tolist())
            
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def categorize_features(self, columns):
        """í”¼ì²˜ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        # íƒ€ê²Ÿê³¼ ì‹œí€€ìŠ¤ ì œì™¸
        exclude_cols = ['clicked', 'seq']
        
        for col in columns:
            if col not in exclude_cols:
                if col.startswith(('l_feat_', 'feat_', 'history_')):
                    self.numerical_features.append(col)
                else:
                    # ë°ì´í„° íƒ€ì…ìœ¼ë¡œ íŒë‹¨
                    first_batch = next(self.parquet_file.iter_batches(batch_size=100))
                    first_df = first_batch.to_pandas()
                    if pd.api.types.is_numeric_dtype(first_df[col]):
                        self.numerical_features.append(col)
                    else:
                        self.categorical_features.append(col)
        
        print(f"\nğŸ“‹ í”¼ì²˜ ë¶„ë¥˜:")
        print(f"   - ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(self.numerical_features)}ê°œ")
        print(f"   - ì¹´í…Œê³ ë¦¬í˜• í”¼ì²˜: {len(self.categorical_features)}ê°œ")
        if self.categorical_features:
            print(f"   - ì¹´í…Œê³ ë¦¬í˜• í”¼ì²˜ ì˜ˆì‹œ: {self.categorical_features[:5]}")
    
    def analyze_chunks(self):
        """ì²­í¬ë³„ ë¶„ì„ ìˆ˜í–‰"""
        print("\n" + "="*60)
        print("ğŸ“Š ì²­í¬ë³„ í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # ì²­í¬ë³„ ì²˜ë¦¬
        for chunk_idx, batch in enumerate(self.parquet_file.iter_batches(batch_size=self.chunk_size)):
            print(f"\rğŸ“Š ì²˜ë¦¬ ì¤‘: {chunk_idx+1}/{self.total_chunks} ì²­í¬ "
                  f"({(chunk_idx+1)/self.total_chunks*100:.1f}%)", end="", flush=True)
            
            chunk_df = batch.to_pandas()
            self.process_chunk(chunk_df)
        
        print(f"\nâœ… ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
    
    def process_chunk(self, chunk_df):
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬"""
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ í†µê³„ ì—…ë°ì´íŠ¸
        for col in self.numerical_features:
            if col in chunk_df.columns:
                self.feature_stats[col].update_batch(chunk_df[col])
        
        # ë²”ì£¼í˜• í”¼ì²˜ ë¶„ì„ ì—…ë°ì´íŠ¸
        self.categorical_analyzer.update_chunk(chunk_df, self.categorical_features)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°ê¸° ì—…ë°ì´íŠ¸
        self.correlation_calculator.update_chunk(chunk_df)
    
    def analyze_distribution_quality(self):
        """í”¼ì²˜ ë¶„í¬ í’ˆì§ˆ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ“ˆ í”¼ì²˜ ë¶„í¬ í’ˆì§ˆ ë¶„ì„")
        print("="*60)
        
        distribution_issues = {
            'low_variance': [],      # ë¶„ì‚°ì´ ë§¤ìš° ë‚®ì€ í”¼ì²˜
            'constant': [],          # ìƒìˆ˜ í”¼ì²˜
            'binary_extreme': [],    # 0 ë˜ëŠ” 1ë§Œ ìˆëŠ” í”¼ì²˜ (ê·¹ë‹¨ì )
            'sparse_extreme': []     # 99% ì´ìƒì´ 0ì¸ í”¼ì²˜
        }
        
        for col in self.numerical_features:
            if col not in self.feature_stats:
                continue
            
            stats = self.feature_stats[col].get_stats()
            
            if stats['count'] == 0:
                continue
            
            # 1. ìƒìˆ˜ í”¼ì²˜ ì²´í¬
            if stats['std'] == 0:
                distribution_issues['constant'].append({
                    'feature': col,
                    'value': stats['mean'],
                    'count': stats['count']
                })
                continue
            
            # 2. ë¶„ì‚°ì´ ë§¤ìš° ë‚®ì€ í”¼ì²˜ (CV < 0.01)
            cv = stats['std'] / abs(stats['mean']) if stats['mean'] != 0 else float('inf')
            if cv < 0.01 and stats['mean'] != 0:
                distribution_issues['low_variance'].append({
                    'feature': col,
                    'mean': stats['mean'],
                    'std': stats['std'],
                    'cv': cv
                })
            
            # 3. ê·¹ë‹¨ì ìœ¼ë¡œ sparseí•œ í”¼ì²˜ ì²´í¬ (0ê°’ ë¹„ìœ¨ì´ 99% ì´ìƒ)
            # 0ê°’ ê°œìˆ˜ë¥¼ ì¶”ì • (ì •í™•í•œ ê³„ì‚°ì€ ë³„ë„ í•„ìš”)
            if stats['min'] == 0 and stats['mean'] < 0.01:
                distribution_issues['sparse_extreme'].append({
                    'feature': col,
                    'mean': stats['mean'],
                    'std': stats['std'],
                    'estimated_zero_ratio': 1 - stats['mean'] if stats['mean'] < 1 else 0
                })
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_distribution_issues(distribution_issues)
        
        return distribution_issues
    
    def print_distribution_issues(self, distribution_issues):
        """ë¶„í¬ ë¬¸ì œì  ì¶œë ¥"""
        print(f"\nğŸš¨ ë¶„í¬ í’ˆì§ˆ ë¬¸ì œ í”¼ì²˜ë“¤:")
        
        # ìƒìˆ˜ í”¼ì²˜
        if distribution_issues['constant']:
            print(f"\nâŒ ìƒìˆ˜ í”¼ì²˜ ({len(distribution_issues['constant'])}ê°œ):")
            for item in distribution_issues['constant'][:10]:  # ìƒìœ„ 10ê°œë§Œ
                print(f"   - {item['feature']}: ê°’={item['value']:.6f}, ê°œìˆ˜={item['count']:,}")
            if len(distribution_issues['constant']) > 10:
                print(f"   ... ì´ {len(distribution_issues['constant'])}ê°œ")
        
        # ë‚®ì€ ë¶„ì‚° í”¼ì²˜
        if distribution_issues['low_variance']:
            print(f"\nâš ï¸ ë‚®ì€ ë¶„ì‚° í”¼ì²˜ ({len(distribution_issues['low_variance'])}ê°œ):")
            for item in sorted(distribution_issues['low_variance'], key=lambda x: x['cv'])[:10]:
                print(f"   - {item['feature']}: CV={item['cv']:.6f}, í‰ê· ={item['mean']:.4f}, í‘œì¤€í¸ì°¨={item['std']:.6f}")
            if len(distribution_issues['low_variance']) > 10:
                print(f"   ... ì´ {len(distribution_issues['low_variance'])}ê°œ")
        
        # ê·¹ë‹¨ì  sparse í”¼ì²˜
        if distribution_issues['sparse_extreme']:
            print(f"\nâš ï¸ ê·¹ë‹¨ì  Sparse í”¼ì²˜ ({len(distribution_issues['sparse_extreme'])}ê°œ):")
            for item in sorted(distribution_issues['sparse_extreme'], key=lambda x: x['estimated_zero_ratio'], reverse=True)[:10]:
                print(f"   - {item['feature']}: 0ê°’ ë¹„ìœ¨â‰ˆ{item['estimated_zero_ratio']:.3f}, í‰ê· ={item['mean']:.6f}")
            if len(distribution_issues['sparse_extreme']) > 10:
                print(f"   ... ì´ {len(distribution_issues['sparse_extreme'])}ê°œ")
    
    def analyze_correlation_quality(self):
        """í”¼ì²˜-í´ë¦­ë¥  ìƒê´€ê´€ê³„ í’ˆì§ˆ ë¶„ì„"""
        print("\n" + "="*60)
        print("ğŸ”— í”¼ì²˜-í´ë¦­ë¥  ìƒê´€ê´€ê³„ í’ˆì§ˆ ë¶„ì„")
        print("="*60)
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = self.correlation_calculator.get_correlations(top_n=1000)
        
        # ìƒê´€ê´€ê³„ê°€ ë§¤ìš° ë‚®ì€ í”¼ì²˜ë“¤ ì‹ë³„
        correlation_issues = {
            'very_low_correlation': [],  # |correlation| < 0.001
            'zero_correlation': [],      # |correlation| < 0.0001
            'negative_correlation': []   # correlation < -0.001
        }
        
        for feature, corr in correlations.items():
            abs_corr = abs(corr)
            
            if abs_corr < 0.0001:
                correlation_issues['zero_correlation'].append({
                    'feature': feature,
                    'correlation': corr
                })
            elif abs_corr < 0.001:
                correlation_issues['very_low_correlation'].append({
                    'feature': feature,
                    'correlation': corr
                })
            
            if corr < -0.001:
                correlation_issues['negative_correlation'].append({
                    'feature': feature,
                    'correlation': corr
                })
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_correlation_issues(correlation_issues, correlations)
        
        return correlation_issues, correlations
    
    def analyze_categorical_quality(self):
        """ë²”ì£¼í˜• í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ (ANOVA ê¸°ë°˜)"""
        print("\n" + "="*60)
        print("ğŸ“Š ë²”ì£¼í˜• í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ (ANOVA)")
        print("="*60)
        
        categorical_analysis = self.categorical_analyzer.get_category_analysis()
        
        categorical_issues = {
            'low_variance': [],      # ì¹´í…Œê³ ë¦¬ë³„ CTR í¸ì°¨ê°€ ë‚®ì€ í”¼ì²˜
            'dominant_category': [], # í•œ ì¹´í…Œê³ ë¦¬ê°€ 95% ì´ìƒì¸ í”¼ì²˜
            'few_categories': [],    # ì¹´í…Œê³ ë¦¬ê°€ 2ê°œ ì´í•˜ì¸ í”¼ì²˜
            'low_anova_f': []        # ANOVA F-statisticì´ ë‚®ì€ í”¼ì²˜
        }
        
        for feature, categories in categorical_analysis.items():
            if not categories:
                continue
            
            # ì¹´í…Œê³ ë¦¬ ê°œìˆ˜
            num_categories = len(categories)
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            ctr_values = [stats['target_rate'] for stats in categories.values()]
            counts = [stats['count'] for stats in categories.values()]
            total_count = sum(counts)
            
            # CTR í¸ì°¨ ê³„ì‚°
            ctr_std = np.std(ctr_values)
            ctr_mean = np.mean(ctr_values)
            ctr_cv = ctr_std / ctr_mean if ctr_mean > 0 else 0
            
            # ê°€ì¥ í° ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨
            max_category_ratio = max(counts) / total_count
            
            # ANOVA F-statistic ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            # Between-group variance / Within-group variance
            if len(ctr_values) > 1 and ctr_std > 0:
                # ê°„ë‹¨í•œ F-statistic ê·¼ì‚¬ì¹˜
                anova_f = (ctr_std ** 2) / (ctr_mean * (1 - ctr_mean)) if ctr_mean > 0 and ctr_mean < 1 else 0
            else:
                anova_f = 0
            
            # ë¬¸ì œì  ì‹ë³„
            if ctr_cv < 0.01:  # CTR í¸ì°¨ê°€ ë§¤ìš° ë‚®ìŒ
                categorical_issues['low_variance'].append({
                    'feature': feature,
                    'ctr_cv': ctr_cv,
                    'ctr_std': ctr_std,
                    'num_categories': num_categories
                })
            
            if max_category_ratio > 0.95:  # í•œ ì¹´í…Œê³ ë¦¬ê°€ 95% ì´ìƒ
                categorical_issues['dominant_category'].append({
                    'feature': feature,
                    'max_ratio': max_category_ratio,
                    'num_categories': num_categories
                })
            
            if num_categories <= 2:  # ì¹´í…Œê³ ë¦¬ê°€ 2ê°œ ì´í•˜
                categorical_issues['few_categories'].append({
                    'feature': feature,
                    'num_categories': num_categories,
                    'categories': list(categories.keys())
                })
            
            if anova_f < 0.001:  # ANOVA F-statisticì´ ë§¤ìš° ë‚®ìŒ
                categorical_issues['low_anova_f'].append({
                    'feature': feature,
                    'anova_f': anova_f,
                    'ctr_cv': ctr_cv,
                    'num_categories': num_categories
                })
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_categorical_issues(categorical_issues)
        
        return categorical_issues, categorical_analysis
    
    def print_categorical_issues(self, categorical_issues):
        """ë²”ì£¼í˜• í”¼ì²˜ ë¬¸ì œì  ì¶œë ¥"""
        print(f"\nğŸš¨ ë²”ì£¼í˜• í”¼ì²˜ í’ˆì§ˆ ë¬¸ì œë“¤:")
        
        # ë‚®ì€ CTR í¸ì°¨
        if categorical_issues['low_variance']:
            print(f"\nâš ï¸ ë‚®ì€ CTR í¸ì°¨ í”¼ì²˜ ({len(categorical_issues['low_variance'])}ê°œ):")
            for item in sorted(categorical_issues['low_variance'], key=lambda x: x['ctr_cv'])[:10]:
                print(f"   - {item['feature']}: CTR CV={item['ctr_cv']:.6f}, ì¹´í…Œê³ ë¦¬ ìˆ˜={item['num_categories']}")
            if len(categorical_issues['low_variance']) > 10:
                print(f"   ... ì´ {len(categorical_issues['low_variance'])}ê°œ")
        
        # ì§€ë°°ì  ì¹´í…Œê³ ë¦¬
        if categorical_issues['dominant_category']:
            print(f"\nâš ï¸ ì§€ë°°ì  ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ({len(categorical_issues['dominant_category'])}ê°œ):")
            for item in sorted(categorical_issues['dominant_category'], key=lambda x: x['max_ratio'], reverse=True)[:10]:
                print(f"   - {item['feature']}: ìµœëŒ€ ë¹„ìœ¨={item['max_ratio']:.3f}, ì¹´í…Œê³ ë¦¬ ìˆ˜={item['num_categories']}")
            if len(categorical_issues['dominant_category']) > 10:
                print(f"   ... ì´ {len(categorical_issues['dominant_category'])}ê°œ")
        
        # ì ì€ ì¹´í…Œê³ ë¦¬
        if categorical_issues['few_categories']:
            print(f"\nâš ï¸ ì ì€ ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ({len(categorical_issues['few_categories'])}ê°œ):")
            for item in categorical_issues['few_categories']:
                print(f"   - {item['feature']}: ì¹´í…Œê³ ë¦¬ ìˆ˜={item['num_categories']}, ì¹´í…Œê³ ë¦¬={item['categories']}")
        
        # ë‚®ì€ ANOVA F-statistic
        if categorical_issues['low_anova_f']:
            print(f"\nâŒ ë‚®ì€ ANOVA F-statistic í”¼ì²˜ ({len(categorical_issues['low_anova_f'])}ê°œ):")
            for item in sorted(categorical_issues['low_anova_f'], key=lambda x: x['anova_f'])[:10]:
                print(f"   - {item['feature']}: F-stat={item['anova_f']:.6f}, CTR CV={item['ctr_cv']:.6f}")
            if len(categorical_issues['low_anova_f']) > 10:
                print(f"   ... ì´ {len(categorical_issues['low_anova_f'])}ê°œ")
    
    def print_correlation_issues(self, correlation_issues, correlations):
        """ìƒê´€ê´€ê³„ ë¬¸ì œì  ì¶œë ¥"""
        print(f"\nğŸš¨ ìƒê´€ê´€ê³„ í’ˆì§ˆ ë¬¸ì œ í”¼ì²˜ë“¤:")
        
        # ê±°ì˜ 0ì— ê°€ê¹Œìš´ ìƒê´€ê´€ê³„
        if correlation_issues['zero_correlation']:
            print(f"\nâŒ ê±°ì˜ 0ì¸ ìƒê´€ê´€ê³„ í”¼ì²˜ ({len(correlation_issues['zero_correlation'])}ê°œ):")
            for item in sorted(correlation_issues['zero_correlation'], key=lambda x: abs(x['correlation']))[:10]:
                print(f"   - {item['feature']}: {item['correlation']:.8f}")
            if len(correlation_issues['zero_correlation']) > 10:
                print(f"   ... ì´ {len(correlation_issues['zero_correlation'])}ê°œ")
        
        # ë§¤ìš° ë‚®ì€ ìƒê´€ê´€ê³„
        if correlation_issues['very_low_correlation']:
            print(f"\nâš ï¸ ë§¤ìš° ë‚®ì€ ìƒê´€ê´€ê³„ í”¼ì²˜ ({len(correlation_issues['very_low_correlation'])}ê°œ):")
            for item in sorted(correlation_issues['very_low_correlation'], key=lambda x: abs(x['correlation']))[:10]:
                print(f"   - {item['feature']}: {item['correlation']:.6f}")
            if len(correlation_issues['very_low_correlation']) > 10:
                print(f"   ... ì´ {len(correlation_issues['very_low_correlation'])}ê°œ")
        
        # ìŒì˜ ìƒê´€ê´€ê³„
        if correlation_issues['negative_correlation']:
            print(f"\nâš ï¸ ìŒì˜ ìƒê´€ê´€ê³„ í”¼ì²˜ ({len(correlation_issues['negative_correlation'])}ê°œ):")
            for item in sorted(correlation_issues['negative_correlation'], key=lambda x: x['correlation'])[:10]:
                print(f"   - {item['feature']}: {item['correlation']:.6f}")
            if len(correlation_issues['negative_correlation']) > 10:
                print(f"   ... ì´ {len(correlation_issues['negative_correlation'])}ê°œ")
        
        # ìƒìœ„ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤
        top_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)[:10]
        print(f"\nâœ… ìƒìœ„ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤:")
        for feature, corr in top_correlations:
            print(f"   - {feature}: {corr:.6f}")
    
    def create_visualizations(self, distribution_issues, correlation_issues, correlations, categorical_issues=None):
        """ì‹œê°í™” ìƒì„±"""
        print(f"\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
        
        try:
            if categorical_issues:
                fig, axes = plt.subplots(3, 2, figsize=(15, 18))
            else:
                fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. ìƒê´€ê´€ê³„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
            corr_values = list(correlations.values())
            axes[0, 0].hist(corr_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0, 0].axvline(x=0.001, color='red', linestyle='--', label='|corr| = 0.001')
            axes[0, 0].axvline(x=-0.001, color='red', linestyle='--')
            axes[0, 0].axvline(x=0.0001, color='orange', linestyle='--', label='|corr| = 0.0001')
            axes[0, 0].axvline(x=-0.0001, color='orange', linestyle='--')
            axes[0, 0].set_title('Feature-Target Correlation Distribution')
            axes[0, 0].set_xlabel('Correlation')
            axes[0, 0].set_ylabel('Count')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. ë¬¸ì œ í”¼ì²˜ ì¹´ìš´íŠ¸
            issue_counts = [
                len(distribution_issues['constant']),
                len(distribution_issues['low_variance']),
                len(distribution_issues['sparse_extreme']),
                len(correlation_issues['zero_correlation']),
                len(correlation_issues['very_low_correlation'])
            ]
            issue_labels = ['Constant', 'Low Variance', 'Extreme Sparse', 'Zero Corr', 'Very Low Corr']
            
            axes[0, 1].bar(range(len(issue_counts)), issue_counts, color=['red', 'orange', 'yellow', 'purple', 'pink'])
            axes[0, 1].set_title('Feature Quality Issues Count')
            axes[0, 1].set_xticks(range(len(issue_labels)))
            axes[0, 1].set_xticklabels(issue_labels, rotation=45)
            axes[0, 1].set_ylabel('Count')
            
            # 3. ìƒìœ„ ìƒê´€ê´€ê³„ í”¼ì²˜ë“¤
            top_20 = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)[:20]
            top_features = [item[0] for item in top_20]
            top_corrs = [item[1] for item in top_20]
            
            y_pos = np.arange(len(top_features))
            colors = ['red' if c < 0 else 'blue' for c in top_corrs]
            axes[1, 0].barh(y_pos, top_corrs, color=colors, alpha=0.7)
            axes[1, 0].set_yticks(y_pos)
            axes[1, 0].set_yticklabels([f[:15] + '...' if len(f) > 15 else f for f in top_features])
            axes[1, 0].set_title('Top 20 Feature Correlations')
            axes[1, 0].set_xlabel('Correlation')
            axes[1, 0].axvline(x=0, color='black', linestyle='-', alpha=0.3)
            
            # 4. ìƒê´€ê´€ê³„ vs ë¶„ì‚° ì‚°ì ë„ (ìƒ˜í”Œ)
            sample_features = list(correlations.keys())[:100]  # ìƒìœ„ 100ê°œë§Œ
            sample_corrs = [correlations[f] for f in sample_features]
            sample_stds = [self.feature_stats[f].get_stats()['std'] for f in sample_features]
            
            axes[1, 1].scatter(sample_stds, [abs(c) for c in sample_corrs], alpha=0.6, color='green')
            axes[1, 1].set_xlabel('Standard Deviation')
            axes[1, 1].set_ylabel('Absolute Correlation')
            axes[1, 1].set_title('Correlation vs Feature Variance')
            axes[1, 1].grid(True, alpha=0.3)
            
            # ë²”ì£¼í˜• ë¶„ì„ ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
            if categorical_issues:
                # 5. ë²”ì£¼í˜• í”¼ì²˜ ë¬¸ì œ ì¹´ìš´íŠ¸
                cat_issue_counts = [
                    len(categorical_issues['low_variance']),
                    len(categorical_issues['dominant_category']),
                    len(categorical_issues['few_categories']),
                    len(categorical_issues['low_anova_f'])
                ]
                cat_issue_labels = ['Low CTR Var', 'Dominant Cat', 'Few Cats', 'Low ANOVA F']
                
                axes[2, 0].bar(range(len(cat_issue_counts)), cat_issue_counts, color=['orange', 'red', 'purple', 'brown'])
                axes[2, 0].set_title('Categorical Feature Issues Count')
                axes[2, 0].set_xticks(range(len(cat_issue_labels)))
                axes[2, 0].set_xticklabels(cat_issue_labels, rotation=45)
                axes[2, 0].set_ylabel('Count')
                
                # 6. ë²”ì£¼í˜• í”¼ì²˜ë³„ ì¹´í…Œê³ ë¦¬ ìˆ˜ ë¶„í¬
                cat_features = list(categorical_issues.keys())
                cat_counts = [len(categorical_issues[key]) for key in cat_features]
                axes[2, 1].pie(cat_counts, labels=cat_issue_labels, autopct='%1.1f%%', startangle=90)
                axes[2, 1].set_title('Categorical Issues Distribution')
            
            plt.tight_layout()
            plt.savefig(RESULTS_DIR / 'feature_quality_analysis.png', dpi=300, bbox_inches='tight')
            print(f"âœ… ì‹œê°í™”ê°€ {RESULTS_DIR}/feature_quality_analysis.pngì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def save_results(self, distribution_issues, correlation_issues, correlations, categorical_issues=None, categorical_analysis=None):
        """ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        results = {
            'analysis_info': {
                'total_numerical_features': len(self.numerical_features),
                'total_categorical_features': len(self.categorical_features),
                'total_chunks_processed': self.total_chunks,
                'total_rows_processed': self.total_rows
            },
            'distribution_issues': dict(distribution_issues),
            'correlation_issues': dict(correlation_issues),
            'all_correlations': correlations,
            'summary': {
                'constant_features': len(distribution_issues['constant']),
                'low_variance_features': len(distribution_issues['low_variance']),
                'sparse_extreme_features': len(distribution_issues['sparse_extreme']),
                'zero_correlation_features': len(correlation_issues['zero_correlation']),
                'very_low_correlation_features': len(correlation_issues['very_low_correlation']),
                'negative_correlation_features': len(correlation_issues['negative_correlation'])
            }
        }
        
        # ë²”ì£¼í˜• ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        if categorical_issues:
            results['categorical_issues'] = dict(categorical_issues)
            results['categorical_analysis'] = categorical_analysis
            results['summary'].update({
                'low_ctr_variance_categorical': len(categorical_issues['low_variance']),
                'dominant_category_categorical': len(categorical_issues['dominant_category']),
                'few_categories_categorical': len(categorical_issues['few_categories']),
                'low_anova_f_categorical': len(categorical_issues['low_anova_f'])
            })
        
        # JSONìœ¼ë¡œ ì €ì¥
        with open(RESULTS_DIR / 'feature_quality_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… ê²°ê³¼ê°€ {RESULTS_DIR}/feature_quality_analysis.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # ì²­í¬ë³„ ë¶„ì„
        self.analyze_chunks()
        
        # ë¶„í¬ í’ˆì§ˆ ë¶„ì„
        distribution_issues = self.analyze_distribution_quality()
        
        # ìƒê´€ê´€ê³„ í’ˆì§ˆ ë¶„ì„
        correlation_issues, correlations = self.analyze_correlation_quality()
        
        # ë²”ì£¼í˜• í”¼ì²˜ í’ˆì§ˆ ë¶„ì„
        categorical_issues = None
        categorical_analysis = None
        if self.categorical_features:
            categorical_issues, categorical_analysis = self.analyze_categorical_quality()
        
        # ì‹œê°í™” ìƒì„±
        self.create_visualizations(distribution_issues, correlation_issues, correlations, categorical_issues)
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(distribution_issues, correlation_issues, correlations, categorical_issues, categorical_analysis)
        
        print(f"\nğŸ‰ í”¼ì²˜ í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"   - {RESULTS_DIR}/feature_quality_analysis.json")
        print(f"   - {RESULTS_DIR}/feature_quality_analysis.png")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    seed_everything(42)
    
    # ì¸ì íŒŒì‹±
    import argparse
    parser = argparse.ArgumentParser(description='í”¼ì²˜ í’ˆì§ˆ ë¶„ì„')
    parser.add_argument('--chunk_size', type=int, default=100000, help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--data_path', type=str, default='./train.parquet', help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()
    
    # ë¶„ì„ ì‹¤í–‰
    analyzer = FeatureQualityAnalyzer(data_path=args.data_path, chunk_size=args.chunk_size)
    analyzer.run_analysis()

if __name__ == "__main__":
    main()
