#!/usr/bin/env python3
"""
ì²­í¬ ë‹¨ìœ„ ë¶„ì„ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from collections import defaultdict
import warnings

warnings.filterwarnings('ignore')

class OnlineStats:
    """ì˜¨ë¼ì¸ í†µê³„ ê³„ì‚° í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.n = 0
        self.mean = 0.0
        self.M2 = 0.0
        self.min_val = float('inf')
        self.max_val = float('-inf')
        self.sum_val = 0.0
    
    def update(self, value):
        """ìƒˆë¡œìš´ ê°’ìœ¼ë¡œ í†µê³„ ì—…ë°ì´íŠ¸ (Welford's algorithm)"""
        if pd.isna(value):
            return
            
        self.n += 1
        self.sum_val += value
        self.min_val = min(self.min_val, value)
        self.max_val = max(self.max_val, value)
        
        delta = value - self.mean
        self.mean += delta / self.n
        delta2 = value - self.mean
        self.M2 += delta * delta2
    
    def update_batch(self, values):
        """ë°°ì¹˜ë¡œ í†µê³„ ì—…ë°ì´íŠ¸"""
        values = pd.Series(values).dropna()
        for val in values:
            self.update(val)
    
    def get_stats(self):
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        if self.n < 1:
            return {
                'count': 0, 'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'sum': 0
            }
        
        variance = self.M2 / self.n if self.n > 0 else 0
        std = np.sqrt(variance)
        
        return {
            'count': self.n,
            'mean': self.mean,
            'std': std,
            'min': self.min_val if self.min_val != float('inf') else 0,
            'max': self.max_val if self.max_val != float('-inf') else 0,
            'sum': self.sum_val
        }

class ChunkCorrelationCalculator:
    """ì²­í¬ ë‹¨ìœ„ ìƒê´€ê´€ê³„ ê³„ì‚°ê¸°"""
    
    def __init__(self, target_col='clicked'):
        self.target_col = target_col
        self.feature_stats = defaultdict(OnlineStats)
        self.target_stats = OnlineStats()
        self.covariance_stats = defaultdict(lambda: {'sum_xy': 0, 'n': 0})
    
    def update_chunk(self, chunk_df):
        """ì²­í¬ ë°ì´í„°ë¡œ ìƒê´€ê´€ê³„ í†µê³„ ì—…ë°ì´íŠ¸"""
        if self.target_col not in chunk_df.columns:
            return
        
        target_data = chunk_df[self.target_col].dropna()
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
        numeric_cols = chunk_df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col != self.target_col]
        
        for col in numeric_cols:
            feature_data = chunk_df[col].dropna()
            
            # ê³µí†µ ì¸ë±ìŠ¤ ì°¾ê¸°
            common_idx = target_data.index.intersection(feature_data.index)
            if len(common_idx) == 0:
                continue
            
            target_common = target_data[common_idx]
            feature_common = feature_data[common_idx]
            
            # ê°œë³„ í†µê³„ ì—…ë°ì´íŠ¸
            self.feature_stats[col].update_batch(feature_common)
            
            # ê³µë¶„ì‚° í†µê³„ ì—…ë°ì´íŠ¸
            cov_stats = self.covariance_stats[col]
            cov_stats['sum_xy'] += (target_common * feature_common).sum()
            cov_stats['n'] += len(common_idx)
        
        # íƒ€ê²Ÿ í†µê³„ ì—…ë°ì´íŠ¸
        self.target_stats.update_batch(target_data)
    
    def get_correlations(self, top_n=20):
        """ìƒê´€ê´€ê³„ ê³„ì‚° ë° ìƒìœ„ Nê°œ ë°˜í™˜"""
        correlations = {}
        target_stats = self.target_stats.get_stats()
        
        if target_stats['count'] == 0:
            return {}
        
        for col, feature_stat in self.feature_stats.items():
            feature_stats = feature_stat.get_stats()
            cov_stats = self.covariance_stats[col]
            
            if feature_stats['count'] == 0 or cov_stats['n'] == 0:
                continue
            
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            n = min(target_stats['count'], feature_stats['count'], cov_stats['n'])
            if n <= 1:
                continue
            
            # E[XY] - E[X]E[Y]
            covariance = (cov_stats['sum_xy'] / cov_stats['n']) - (target_stats['mean'] * feature_stats['mean'])
            
            # í‘œì¤€í¸ì°¨ì˜ ê³±
            std_product = target_stats['std'] * feature_stats['std']
            
            if std_product > 0:
                correlation = covariance / std_product
                correlations[col] = correlation
        
        # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œ ë°˜í™˜
        sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
        return dict(sorted_corr[:top_n])

class ChunkCategoricalAnalyzer:
    """ì²­í¬ ë‹¨ìœ„ ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ë¶„ì„ê¸°"""
    
    def __init__(self, target_col='clicked'):
        self.target_col = target_col
        self.category_stats = defaultdict(lambda: defaultdict(lambda: {'count': 0, 'target_sum': 0}))
    
    def update_chunk(self, chunk_df, categorical_cols):
        """ì²­í¬ ë°ì´í„°ë¡œ ì¹´í…Œê³ ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        if self.target_col not in chunk_df.columns:
            return
        
        for col in categorical_cols:
            if col not in chunk_df.columns:
                continue
            
            # ì¹´í…Œê³ ë¦¬ë³„ íƒ€ê²Ÿ í†µê³„
            grouped = chunk_df.groupby(col)[self.target_col].agg(['count', 'sum'])
            
            for category, (count, target_sum) in grouped.iterrows():
                self.category_stats[col][str(category)]['count'] += count
                self.category_stats[col][str(category)]['target_sum'] += target_sum
    
    def get_category_analysis(self):
        """ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        results = {}
        
        for col, categories in self.category_stats.items():
            col_results = {}
            total_count = sum(stats['count'] for stats in categories.values())
            
            for category, stats in categories.items():
                count = stats['count']
                target_sum = stats['target_sum']
                
                col_results[category] = {
                    'count': count,
                    'percentage': count / total_count * 100 if total_count > 0 else 0,
                    'target_rate': target_sum / count if count > 0 else 0,
                    'target_count': target_sum
                }
            
            # ì¹´ìš´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            col_results = dict(sorted(col_results.items(), 
                                    key=lambda x: x[1]['count'], reverse=True))
            results[col] = col_results
        
        return results

def analyze_parquet_schema(file_path):
    """Parquet íŒŒì¼ì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¶„ì„"""
    try:
        parquet_file = pq.ParquetFile(file_path)
        schema = parquet_file.schema
        
        print("ğŸ“‹ Parquet íŒŒì¼ ìŠ¤í‚¤ë§ˆ ì •ë³´:")
        print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {len(schema)}")
        print(f"   - ì´ í–‰ ìˆ˜: {parquet_file.metadata.num_rows:,}")
        print(f"   - íŒŒì¼ í¬ê¸°: {parquet_file.metadata.serialized_size / 1024**2:.2f} MB")
        print(f"   - Row Group ìˆ˜: {parquet_file.metadata.num_row_groups}")
        
        # ì»¬ëŸ¼ íƒ€ì…ë³„ ë¶„í¬
        type_counts = defaultdict(int)
        for i in range(len(schema)):
            col_type = str(schema.field(i).type)
            type_counts[col_type] += 1
        
        print("\nğŸ·ï¸ ì»¬ëŸ¼ íƒ€ì… ë¶„í¬:")
        for col_type, count in sorted(type_counts.items()):
            print(f"   - {col_type}: {count}ê°œ")
        
        # Row Group ì •ë³´
        print(f"\nğŸ“¦ Row Group ì •ë³´:")
        for i in range(min(3, parquet_file.metadata.num_row_groups)):  # ìƒìœ„ 3ê°œë§Œ
            rg = parquet_file.metadata.row_group(i)
            print(f"   - Group {i}: {rg.num_rows:,} í–‰, {rg.total_byte_size / 1024**2:.2f} MB")
        
        return {
            'total_rows': parquet_file.metadata.num_rows,
            'total_columns': len(schema),
            'file_size_mb': parquet_file.metadata.serialized_size / 1024**2,
            'num_row_groups': parquet_file.metadata.num_row_groups,
            'column_types': dict(type_counts)
        }
        
    except Exception as e:
        print(f"âŒ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None

def estimate_memory_usage(file_path, chunk_size=100000):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
    try:
        parquet_file = pq.ParquetFile(file_path)
        
        # ì‘ì€ ìƒ˜í”Œë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        sample_batch = next(parquet_file.iter_batches(batch_size=min(1000, chunk_size)))
        sample_df = sample_batch.to_pandas()
        
        sample_memory = sample_df.memory_usage(deep=True).sum()
        estimated_chunk_memory = sample_memory * (chunk_size / len(sample_df))
        
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •:")
        print(f"   - ìƒ˜í”Œ í¬ê¸°: {len(sample_df):,} í–‰")
        print(f"   - ìƒ˜í”Œ ë©”ëª¨ë¦¬: {sample_memory / 1024**2:.2f} MB")
        print(f"   - ì²­í¬ ë©”ëª¨ë¦¬ (ì˜ˆìƒ): {estimated_chunk_memory / 1024**2:.2f} MB")
        
        return estimated_chunk_memory
        
    except Exception as e:
        print(f"âŒ ë©”ëª¨ë¦¬ ì¶”ì • ì‹¤íŒ¨: {e}")
        return None

def recommend_chunk_size(file_path, target_memory_mb=500):
    """ê¶Œì¥ ì²­í¬ í¬ê¸° ê³„ì‚°"""
    try:
        parquet_file = pq.ParquetFile(file_path)
        total_rows = parquet_file.metadata.num_rows
        
        # ìƒ˜í”Œë¡œ í–‰ë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        sample_batch = next(parquet_file.iter_batches(batch_size=1000))
        sample_df = sample_batch.to_pandas()
        memory_per_row = sample_df.memory_usage(deep=True).sum() / len(sample_df)
        
        # ëª©í‘œ ë©”ëª¨ë¦¬ì— ë§ëŠ” ì²­í¬ í¬ê¸° ê³„ì‚°
        target_memory_bytes = target_memory_mb * 1024 * 1024
        recommended_chunk_size = int(target_memory_bytes / memory_per_row)
        
        # í•©ë¦¬ì ì¸ ë²”ìœ„ë¡œ ì œí•œ
        recommended_chunk_size = max(10000, min(1000000, recommended_chunk_size))
        
        print(f"ğŸ¯ ê¶Œì¥ ì²­í¬ í¬ê¸°:")
        print(f"   - ëª©í‘œ ë©”ëª¨ë¦¬: {target_memory_mb} MB")
        print(f"   - í–‰ë‹¹ ë©”ëª¨ë¦¬: {memory_per_row:.2f} bytes")
        print(f"   - ê¶Œì¥ ì²­í¬ í¬ê¸°: {recommended_chunk_size:,} í–‰")
        print(f"   - ì˜ˆìƒ ì´ ì²­í¬ ìˆ˜: {(total_rows + recommended_chunk_size - 1) // recommended_chunk_size}")
        
        return recommended_chunk_size
        
    except Exception as e:
        print(f"âŒ ì²­í¬ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 100000  # ê¸°ë³¸ê°’
