#!/usr/bin/env python3
"""
train.parquet ë¶„ì„: ê° featureê°€ categoricalì¸ì§€ numericalì¸ì§€ ìë™ íŒë³„

íŒë³„ ê¸°ì¤€:
1. ì •ìˆ˜ë§Œ ìˆìŒ + ê³ ìœ ê°’ ê°œìˆ˜ ì ìŒ (< 50) â†’ categorical
2. ì†Œìˆ˜ ìˆìŒ â†’ numerical
3. ì •ìˆ˜ë§Œ ìˆìŒ + ê³ ìœ ê°’ ë§ìŒ (>= 50) â†’ numerical (ID ê°™ì€ ê²½ìš°)

ì¶œë ¥: JSON íŒŒì¼ë¡œ ì €ì¥
"""
import cudf
import pandas as pd
import numpy as np
import json
from pathlib import Path

print("=" * 80)
print("ğŸ” Feature Type Analysis: Categorical vs Numerical")
print("=" * 80)

# Load train.parquet (first 100K rows for type analysis)
print("\nğŸ“– Loading train.parquet (first 100K rows for analysis)...")
print("   Note: 100K rows is sufficient for determining data types")

# Use cudf with row_groups to avoid OOM
try:
    gdf = cudf.read_parquet('data/train.parquet', num_rows=100000)
    df = gdf.to_pandas()
    print(f"âœ… Loaded: {len(df):,} rows x {len(df.columns)} columns\n")
except Exception as e:
    print(f"âŒ cudf failed: {e}")
    print("   Trying dask-cudf...")
    import dask_cudf
    ddf = dask_cudf.read_parquet('data/train.parquet', split_row_groups=True)
    # Take first 100K rows
    gdf = ddf.head(100000, npartitions=-1)
    df = gdf.to_pandas()
    print(f"âœ… Loaded via dask-cudf: {len(df):,} rows x {len(df.columns)} columns\n")

# ë¶„ì„ ê²°ê³¼ ì €ì¥
results = {
    'metadata': {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'analysis_date': pd.Timestamp.now().isoformat()
    },
    'categorical': [],
    'numerical': [],
    'special': [],  # clicked, seq, ID ë“±
    'details': {}
}

# íŠ¹ìˆ˜ ì»¬ëŸ¼ (ë¶„ì„ì—ì„œ ì œì™¸)
SPECIAL_COLS = ['clicked', 'seq', 'ID']

print("ğŸ”¬ Analyzing each feature...\n")
print(f"{'Column':<20} {'Type':<12} {'Unique':<10} {'Has Float':<12} {'Classification':<15}")
print("-" * 80)

for col in df.columns:
    if col in SPECIAL_COLS:
        results['special'].append(col)
        results['details'][col] = {
            'classification': 'special',
            'reason': 'Special column (target, sequence, or ID)'
        }
        print(f"{col:<20} {'special':<12} {'-':<10} {'-':<12} {'special':<15}")
        continue
    
    # ê¸°ë³¸ ì •ë³´
    dtype = str(df[col].dtype)
    n_unique = df[col].nunique()
    n_missing = df[col].isna().sum()
    
    # NaN ì œê±°í•œ ë°ì´í„°ë¡œ ë¶„ì„
    non_null_data = df[col].dropna()
    
    if len(non_null_data) == 0:
        # ëª¨ë‘ NaNì¸ ê²½ìš°
        results['numerical'].append(col)
        results['details'][col] = {
            'classification': 'numerical',
            'reason': 'All NaN - default to numerical',
            'dtype': dtype,
            'n_unique': n_unique,
            'n_missing': n_missing,
            'missing_ratio': n_missing / len(df)
        }
        print(f"{col:<20} {'all_nan':<12} {n_unique:<10} {'-':<12} {'numerical':<15}")
        continue
    
    # Float ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
    has_float = False
    try:
        # ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ ì‹œë„
        numeric_data = pd.to_numeric(non_null_data, errors='coerce')
        non_null_numeric = numeric_data.dropna()
        
        if len(non_null_numeric) > 0:
            # ì •ìˆ˜ì™€ ë¹„êµí•´ì„œ ì°¨ì´ê°€ ìˆìœ¼ë©´ float
            has_float = not (non_null_numeric == non_null_numeric.astype(int)).all()
    except:
        has_float = True  # ë³€í™˜ ì‹¤íŒ¨í•˜ë©´ floatë¡œ ê°„ì£¼
    
    # ë¶„ë¥˜ ê¸°ì¤€
    classification = None
    reason = None
    
    if has_float:
        # ì†Œìˆ˜ì  ìˆìŒ â†’ numerical
        classification = 'numerical'
        reason = 'Has floating point values'
    elif n_unique < 50:
        # ì •ìˆ˜ë§Œ ìˆê³  ê³ ìœ ê°’ ì ìŒ â†’ categorical
        classification = 'categorical'
        reason = f'Integer only with {n_unique} unique values'
    else:
        # ì •ìˆ˜ë§Œ ìˆì§€ë§Œ ê³ ìœ ê°’ ë§ìŒ â†’ numerical (ID ê°™ì€ ê²½ìš°)
        classification = 'numerical'
        reason = f'Integer only but {n_unique} unique values (likely ID or continuous)'
    
    # ê²°ê³¼ ì €ì¥
    if classification == 'categorical':
        results['categorical'].append(col)
    else:
        results['numerical'].append(col)
    
    results['details'][col] = {
        'classification': classification,
        'reason': reason,
        'dtype': dtype,
        'n_unique': n_unique,
        'n_missing': n_missing,
        'missing_ratio': n_missing / len(df),
        'has_float': has_float
    }
    
    print(f"{col:<20} {dtype:<12} {n_unique:<10} {str(has_float):<12} {classification:<15}")

# ê²°ê³¼ ìš”ì•½
print("\n" + "=" * 80)
print("ğŸ“Š Analysis Summary")
print("=" * 80)

print(f"\nğŸ“Œ Categorical Features ({len(results['categorical'])}ê°œ):")
for col in sorted(results['categorical']):
    detail = results['details'][col]
    print(f"  - {col:<20} (unique: {detail['n_unique']}, {detail['reason']})")

print(f"\nğŸ“ˆ Numerical Features ({len(results['numerical'])}ê°œ):")
# ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 20ê°œë§Œ
if len(results['numerical']) > 20:
    print(f"  (showing first 20 out of {len(results['numerical'])})")
    for col in sorted(results['numerical'])[:20]:
        detail = results['details'][col]
        has_float_str = "float" if detail['has_float'] else "int"
        print(f"  - {col:<20} ({has_float_str}, unique: {detail['n_unique']})")
    print(f"  ... and {len(results['numerical']) - 20} more")
else:
    for col in sorted(results['numerical']):
        detail = results['details'][col]
        has_float_str = "float" if detail['has_float'] else "int"
        print(f"  - {col:<20} ({has_float_str}, unique: {detail['n_unique']})")

print(f"\nğŸ”– Special Columns ({len(results['special'])}ê°œ):")
for col in results['special']:
    print(f"  - {col}")

# JSON ì €ì¥
output_path = Path('analysis/results/feature_type_analysis.json')
output_path.parent.mkdir(parents=True, exist_ok=True)

with open(output_path, 'w') as f:
    json.dump(results, f, indent=2)

print(f"\nğŸ’¾ Results saved to: {output_path}")

# Python ì½”ë“œë¡œ ì¶œë ¥ (dataset_split_and_preprocess.pyì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)
print("\n" + "=" * 80)
print("ğŸ Python Code (copy to dataset_split_and_preprocess.py)")
print("=" * 80)

print("\n# CATEGORICAL COLUMNS")
print(f"categorical_cols = {results['categorical']}")

print("\n# NUMERICAL COLUMNS (for continuous feature list)")
numerical_cols = sorted([c for c in results['numerical'] if c not in SPECIAL_COLS])
print(f"# Total: {len(numerical_cols)} columns")
print(f"all_continuous = {numerical_cols}")

print("\nâœ… Analysis complete!")
print("\nğŸ’¡ Next steps:")
print("  1. Review the classification above")
print("  2. Update dataset_split_and_preprocess.py with the correct lists")
print("  3. Re-run preprocessing: python dataset_split_and_preprocess.py")

