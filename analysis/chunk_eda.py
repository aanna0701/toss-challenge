#!/usr/bin/env python3
"""
ì²­í¬ ë‹¨ìœ„ EDA - ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¶„ì„
"""

import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from pathlib import Path
import sys
import os
import json
from collections import defaultdict
import warnings

warnings.filterwarnings('ignore')

# Set English locale and font settings for plots
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
import locale
try:
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_ALL, 'C')
    except:
        pass

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils import seed_everything

# ê²°ê³¼ ì €ì¥ í´ë”
RESULTS_DIR = Path("analysis/results")
RESULTS_DIR.mkdir(exist_ok=True)

class ChunkEDA:
    """ì²­í¬ ë‹¨ìœ„ EDA í´ë˜ìŠ¤"""
    
    def __init__(self, data_path="./train.parquet", chunk_size=100000):
        """
        ì´ˆê¸°í™”
        Args:
            data_path: train.parquet íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
        """
        self.data_path = data_path
        self.chunk_size = chunk_size
        self.parquet_file = None
        self.total_rows = 0
        self.total_chunks = 0
        self.feature_groups = {}
        self.stats = defaultdict(dict)
        
        self.initialize()
    
    def initialize(self):
        """ì´ˆê¸°í™”"""
        print("ğŸ“Š ì²­í¬ ë‹¨ìœ„ EDA ì´ˆê¸°í™”...")
        
        try:
            self.parquet_file = pq.ParquetFile(self.data_path)
            self.total_rows = self.parquet_file.metadata.num_rows
            self.total_chunks = (self.total_rows + self.chunk_size - 1) // self.chunk_size
            
            print(f"âœ… íŒŒì¼ ì •ë³´:")
            print(f"   - ì „ì²´ í–‰ ìˆ˜: {self.total_rows:,}")
            print(f"   - ì²­í¬ í¬ê¸°: {self.chunk_size:,}")
            print(f"   - ì´ ì²­í¬ ìˆ˜: {self.total_chunks}")
            
            # ì²« ë²ˆì§¸ ì²­í¬ë¡œ í”¼ì²˜ ì •ë³´ íŒŒì•…
            first_batch = next(self.parquet_file.iter_batches(batch_size=1000))
            first_df = first_batch.to_pandas()
            self.categorize_features(first_df.columns.tolist())
            
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def categorize_features(self, columns):
        """í”¼ì²˜ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        self.feature_groups = {
            'basic': ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour'],
            'sequence': ['seq'],
            'l_feat': [col for col in columns if col.startswith('l_feat_')],
            'feat_e': [col for col in columns if col.startswith('feat_e_')],
            'feat_d': [col for col in columns if col.startswith('feat_d_')],
            'feat_c': [col for col in columns if col.startswith('feat_c_')],
            'feat_b': [col for col in columns if col.startswith('feat_b_')],
            'feat_a': [col for col in columns if col.startswith('feat_a_')],
            'history_a': [col for col in columns if col.startswith('history_a_')],
            'target': ['clicked'],
            'other': []
        }
        
        # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ë“¤ì„ 'other'ì— ì¶”ê°€
        all_categorized = []
        for group_cols in self.feature_groups.values():
            all_categorized.extend(group_cols)
        
        self.feature_groups['other'] = [col for col in columns if col not in all_categorized]
        
        print("\nğŸ“‹ í”¼ì²˜ ê·¸ë£¹ë³„ ê°œìˆ˜:")
        for group, cols in self.feature_groups.items():
            if cols:
                print(f"   - {group}: {len(cols)}ê°œ")
    
    def analyze_chunks(self):
        """ì²­í¬ë³„ ë¶„ì„ ìˆ˜í–‰"""
        print("\n" + "="*60)
        print("ğŸ“Š ì²­í¬ë³„ ë°ì´í„° ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        # í†µê³„ ì´ˆê¸°í™”
        self.stats = {
            'basic_info': {
                'total_rows': self.total_rows,
                'total_chunks': self.total_chunks,
                'chunk_size': self.chunk_size
            },
            'target_stats': {'clicked_0': 0, 'clicked_1': 0},
            'missing_stats': defaultdict(int),
            'categorical_stats': defaultdict(lambda: defaultdict(int)),
            'numerical_stats': defaultdict(lambda: {
                'count': 0, 'sum': 0, 'sum_sq': 0, 'min': float('inf'), 'max': float('-inf')
            }),
            'sequence_stats': {'total_length': 0, 'total_count': 0, 'empty_count': 0},
            'dtypes': {}
        }
        
        # ì²­í¬ë³„ ì²˜ë¦¬
        for chunk_idx, batch in enumerate(self.parquet_file.iter_batches(batch_size=self.chunk_size)):
            print(f"\rğŸ“Š ì²˜ë¦¬ ì¤‘: {chunk_idx+1}/{self.total_chunks} ì²­í¬ "
                  f"({(chunk_idx+1)/self.total_chunks*100:.1f}%)", end="", flush=True)
            
            chunk_df = batch.to_pandas()
            self.process_chunk(chunk_df, chunk_idx)
        
        print(f"\nâœ… ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!")
        self.finalize_stats()
    
    def process_chunk(self, chunk_df, chunk_idx):
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬"""
        # ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ë°ì´í„° íƒ€ì… ì €ì¥
        if chunk_idx == 0:
            self.stats['dtypes'] = chunk_df.dtypes.to_dict()
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„
        if 'clicked' in chunk_df.columns:
            target_counts = chunk_df['clicked'].value_counts()
            self.stats['target_stats']['clicked_0'] += target_counts.get(0, 0)
            self.stats['target_stats']['clicked_1'] += target_counts.get(1, 0)
        
        # ê²°ì¸¡ê°’ í†µê³„
        missing = chunk_df.isnull().sum()
        for col, count in missing.items():
            self.stats['missing_stats'][col] += count
        
        # ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ í†µê³„
        categorical_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']
        for col in categorical_cols:
            if col in chunk_df.columns:
                value_counts = chunk_df[col].value_counts()
                for val, count in value_counts.items():
                    self.stats['categorical_stats'][col][str(val)] += count
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„
        numeric_cols = chunk_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col not in ['clicked']:  # íƒ€ê²Ÿ ì œì™¸
                data = chunk_df[col].dropna()
                if len(data) > 0:
                    stats = self.stats['numerical_stats'][col]
                    stats['count'] += len(data)
                    stats['sum'] += data.sum()
                    stats['sum_sq'] += (data ** 2).sum()
                    stats['min'] = min(stats['min'], data.min())
                    stats['max'] = max(stats['max'], data.max())
        
        # ì‹œí€€ìŠ¤ í†µê³„
        if 'seq' in chunk_df.columns:
            for seq_str in chunk_df['seq'].astype(str):
                if seq_str and seq_str != 'nan':
                    try:
                        seq_length = len(seq_str.split(','))
                        self.stats['sequence_stats']['total_length'] += seq_length
                        self.stats['sequence_stats']['total_count'] += 1
                    except:
                        self.stats['sequence_stats']['empty_count'] += 1
                else:
                    self.stats['sequence_stats']['empty_count'] += 1
    
    def finalize_stats(self):
        """í†µê³„ ìµœì¢… ê³„ì‚°"""
        print("\nğŸ“Š í†µê³„ ìµœì¢… ê³„ì‚° ì¤‘...")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ í‰ê· , í‘œì¤€í¸ì°¨ ê³„ì‚°
        for col, stats in self.stats['numerical_stats'].items():
            if stats['count'] > 0:
                mean = stats['sum'] / stats['count']
                variance = (stats['sum_sq'] / stats['count']) - (mean ** 2)
                std = np.sqrt(max(0, variance))
                
                stats['mean'] = mean
                stats['std'] = std
        
        # ì‹œí€€ìŠ¤ í‰ê·  ê¸¸ì´ ê³„ì‚°
        seq_stats = self.stats['sequence_stats']
        if seq_stats['total_count'] > 0:
            seq_stats['avg_length'] = seq_stats['total_length'] / seq_stats['total_count']
        else:
            seq_stats['avg_length'] = 0
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ì²­í¬ ë‹¨ìœ„ EDA ê²°ê³¼")
        print("="*60)
        
        # ê¸°ë³¸ ì •ë³´
        print(f"\nğŸ“‹ ê¸°ë³¸ ì •ë³´:")
        print(f"   - ì „ì²´ í–‰ ìˆ˜: {self.stats['basic_info']['total_rows']:,}")
        print(f"   - ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜: {self.stats['basic_info']['total_chunks']}")
        print(f"   - ì²­í¬ í¬ê¸°: {self.stats['basic_info']['chunk_size']:,}")
        
        # ë°ì´í„° íƒ€ì…
        print(f"\nğŸ·ï¸ ë°ì´í„° íƒ€ì…:")
        dtype_counts = pd.Series(self.stats['dtypes']).value_counts()
        for dtype, count in dtype_counts.items():
            print(f"   - {dtype}: {count}ê°œ")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜
        print(f"\nğŸ¯ íƒ€ê²Ÿ ë³€ìˆ˜ (ì „ì²´ ë°ì´í„°):")
        total_samples = self.stats['target_stats']['clicked_0'] + self.stats['target_stats']['clicked_1']
        for label in [0, 1]:
            count = self.stats['target_stats'][f'clicked_{label}']
            pct = count / total_samples * 100 if total_samples > 0 else 0
            print(f"   - clicked={label}: {count:,}ê°œ ({pct:.2f}%)")
        
        ctr = self.stats['target_stats']['clicked_1'] / total_samples if total_samples > 0 else 0
        print(f"   - ì „ì²´ í´ë¦­ë¥  (CTR): {ctr:.6f}")
        
        # ê²°ì¸¡ê°’
        print(f"\nğŸ•³ï¸ ê²°ì¸¡ê°’ (ì „ì²´ ë°ì´í„°):")
        missing_cols = [(col, count) for col, count in self.stats['missing_stats'].items() if count > 0]
        missing_cols.sort(key=lambda x: x[1], reverse=True)
        
        if missing_cols:
            print(f"   - ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {len(missing_cols)}ê°œ")
            for col, count in missing_cols[:10]:
                pct = count / self.total_rows * 100
                print(f"     * {col}: {count:,}ê°œ ({pct:.2f}%)")
            if len(missing_cols) > 10:
                print(f"     ... ì´ {len(missing_cols)}ê°œ ì»¬ëŸ¼")
        else:
            print("   - ê²°ì¸¡ê°’ ì—†ìŒ! âœ…")
        
        # ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜
        print(f"\nğŸ” ì£¼ìš” ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜:")
        for col, value_counts in self.stats['categorical_stats'].items():
            if value_counts:
                total_count = sum(value_counts.values())
                sorted_values = sorted(value_counts.items(), key=lambda x: x[1], reverse=True)
                top_value, top_count = sorted_values[0]
                top_pct = top_count / total_count * 100
                
                print(f"   - {col}: {len(sorted_values)}ê°œ ê³ ìœ ê°’, ìµœë¹ˆê°’ '{top_value}' ({top_pct:.1f}%)")
        
        # ì‹œí€€ìŠ¤ ë¶„ì„
        print(f"\nğŸ”¢ ì‹œí€€ìŠ¤ ë¶„ì„ (ì „ì²´ ë°ì´í„°):")
        seq_stats = self.stats['sequence_stats']
        print(f"   - í‰ê·  ê¸¸ì´: {seq_stats['avg_length']:.2f}")
        print(f"   - ì´ ì‹œí€€ìŠ¤ ìˆ˜: {seq_stats['total_count']:,}")
        print(f"   - ë¹ˆ ì‹œí€€ìŠ¤: {seq_stats['empty_count']:,}ê°œ")
        if seq_stats['total_count'] > 0:
            empty_pct = seq_stats['empty_count'] / (seq_stats['total_count'] + seq_stats['empty_count']) * 100
            print(f"   - ë¹ˆ ì‹œí€€ìŠ¤ ë¹„ìœ¨: {empty_pct:.2f}%")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìš”ì•½
        print(f"\nğŸ“Š ì£¼ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„:")
        for group_name, columns in self.feature_groups.items():
            if group_name in ['target', 'basic', 'sequence', 'other']:
                continue
            
            numeric_cols = [col for col in columns if col in self.stats['numerical_stats']]
            if numeric_cols:
                print(f"\n   ğŸ·ï¸ {group_name} ê·¸ë£¹ ({len(numeric_cols)}ê°œ í”¼ì²˜):")
                
                # ê·¸ë£¹ í†µê³„ ìš”ì•½
                means = [self.stats['numerical_stats'][col]['mean'] for col in numeric_cols[:5]]  # ìƒìœ„ 5ê°œë§Œ
                stds = [self.stats['numerical_stats'][col]['std'] for col in numeric_cols[:5]]
                
                if means:
                    print(f"     - í‰ê· ê°’ ë²”ìœ„: {min(means):.4f} ~ {max(means):.4f}")
                    print(f"     - í‘œì¤€í¸ì°¨ ë²”ìœ„: {min(stds):.4f} ~ {max(stds):.4f}")
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # JSONìœ¼ë¡œ í†µê³„ ì €ì¥
        stats_to_save = {}
        for key, value in self.stats.items():
            if key == 'numerical_stats':
                # numerical_statsëŠ” ë„ˆë¬´ í¬ë¯€ë¡œ ìš”ì•½ë§Œ ì €ì¥
                stats_to_save[key] = {
                    col: {k: v for k, v in stats.items() if k in ['count', 'mean', 'std', 'min', 'max']}
                    for col, stats in value.items()
                }
            else:
                stats_to_save[key] = dict(value) if hasattr(value, 'items') else value
        
        with open(RESULTS_DIR / 'chunk_eda_results.json', 'w', encoding='utf-8') as f:
            json.dump(stats_to_save, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… ê²°ê³¼ê°€ {RESULTS_DIR}/chunk_eda_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def create_summary_plots(self):
        """Summary visualization generation"""
        print(f"\nğŸ“Š Creating summary visualizations...")
        
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. Target distribution
            target_data = [self.stats['target_stats']['clicked_0'], self.stats['target_stats']['clicked_1']]
            axes[0, 0].pie(target_data, labels=['Not Clicked', 'Clicked'], 
                          autopct='%1.2f%%', colors=['skyblue', 'salmon'])
            axes[0, 0].set_title('Overall Click Distribution')
            
            # 2. Data type distribution
            dtype_counts = pd.Series(self.stats['dtypes']).value_counts()
            axes[0, 1].bar(range(len(dtype_counts)), dtype_counts.values, color='lightblue')
            axes[0, 1].set_title('Data Type Distribution')
            axes[0, 1].set_xticks(range(len(dtype_counts)))
            axes[0, 1].set_xticklabels(dtype_counts.index, rotation=45)
            axes[0, 1].set_ylabel('Count')
            
            # 3. Top 10 missing value features
            missing_data = [(col, count) for col, count in self.stats['missing_stats'].items() if count > 0]
            missing_data.sort(key=lambda x: x[1], reverse=True)
            
            if missing_data:
                top_missing = missing_data[:10]
                cols, counts = zip(*top_missing)
                axes[1, 0].barh(range(len(cols)), counts, color='orange')
                axes[1, 0].set_title('Top 10 Features with Missing Values')
                axes[1, 0].set_yticks(range(len(cols)))
                axes[1, 0].set_yticklabels(cols)
                axes[1, 0].invert_yaxis()
                axes[1, 0].set_xlabel('Missing Count')
            else:
                axes[1, 0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('Missing Values Status')
            
            # 4. Feature group counts
            group_counts = {k: len(v) for k, v in self.feature_groups.items() if v}
            axes[1, 1].bar(range(len(group_counts)), list(group_counts.values()), color='lightgreen')
            axes[1, 1].set_title('Feature Count by Group')
            axes[1, 1].set_xticks(range(len(group_counts)))
            axes[1, 1].set_xticklabels(list(group_counts.keys()), rotation=45)
            axes[1, 1].set_ylabel('Feature Count')
            
            plt.tight_layout()
            plt.savefig(RESULTS_DIR / 'chunk_eda_summary.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print(f"âœ… Visualization saved to {RESULTS_DIR}/chunk_eda_summary.png")
            
        except Exception as e:
            print(f"âš ï¸  Error creating visualization: {e}")
    
    def run_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì²­í¬ ë‹¨ìœ„ EDA ì‹œì‘")
        print("="*60)
        
        # ì²­í¬ë³„ ë¶„ì„
        self.analyze_chunks()
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results()
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
        
        # ì‹œê°í™” ìƒì„±
        self.create_summary_plots()
        
        print(f"\nğŸ‰ ì²­í¬ ë‹¨ìœ„ EDA ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼:")
        print(f"   - {RESULTS_DIR}/chunk_eda_results.json")
        print(f"   - {RESULTS_DIR}/chunk_eda_summary.png")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    seed_everything(42)
    
    # ì²­í¬ í¬ê¸°ë¥¼ ì¸ìë¡œ ë°›ì„ ìˆ˜ ìˆë„ë¡
    import argparse
    parser = argparse.ArgumentParser(description='ì²­í¬ ë‹¨ìœ„ EDA')
    parser.add_argument('--chunk_size', type=int, default=100000, help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--data_path', type=str, default='./train.parquet', help='ë°ì´í„° íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()
    
    # EDA ì‹¤í–‰
    eda = ChunkEDA(data_path=args.data_path, chunk_size=args.chunk_size)
    eda.run_analysis()

if __name__ == "__main__":
    main()
