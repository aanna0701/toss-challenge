import os
import torch
import torch.nn as nn
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# from main import CFG, device, initialize
from data_loader import create_data_loaders
from model import create_tabular_transformer_model
from early_stopping import create_early_stopping_from_config
from metrics import evaluate_model, print_metrics, save_training_logs, get_best_checkpoint_info
from gradient_norm import (
    calculate_gradient_norms, print_gradient_norms, save_gradient_norm_logs,
    analyze_gradient_behavior, print_gradient_analysis, check_gradient_issues, print_gradient_issues
)


def train_model(train_df, feature_cols, seq_col, target_col, CFG, device="cuda", results_dir=None):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # 1) split
    tr_df, va_df = train_test_split(train_df, test_size=CFG['VAL_SPLIT'], random_state=42, shuffle=True)

    # 2) Dataset / Loader
    train_loader, val_loader, _, train_dataset, val_dataset, feature_processor = create_data_loaders(
        tr_df, va_df, None, feature_cols, seq_col, target_col, CFG['BATCH_SIZE'], CFG
    )

    # 3) TabularTransformer ëª¨ë¸ ìƒì„±
    categorical_cardinalities = list(feature_processor.categorical_cardinalities.values())
    num_categorical_features = len(feature_processor.categorical_features)
    num_numerical_features = len(feature_processor.numerical_features)
    
    model = create_tabular_transformer_model(
        num_categorical_features=num_categorical_features,
        categorical_cardinalities=categorical_cardinalities,
        num_numerical_features=num_numerical_features,
        lstm_hidden=CFG['MODEL']['TRANSFORMER']['LSTM_HIDDEN'],
        hidden_dim=CFG['MODEL']['TRANSFORMER']['HIDDEN_DIM'],
        n_heads=CFG['MODEL']['TRANSFORMER']['N_HEADS'],
        n_layers=CFG['MODEL']['TRANSFORMER']['N_LAYERS'],
        ffn_size_factor=CFG['MODEL']['TRANSFORMER']['FFN_SIZE_FACTOR'],
        attention_dropout=CFG['MODEL']['TRANSFORMER']['ATTENTION_DROPOUT'],
        ffn_dropout=CFG['MODEL']['TRANSFORMER']['FFN_DROPOUT'],
        residual_dropout=CFG['MODEL']['TRANSFORMER']['RESIDUAL_DROPOUT'],
        device=device
    )

    criterion = nn.BCEWithLogitsLoss()

    # Weight decay ì ìš© (íŠ¹ì • íŒŒë¼ë¯¸í„° ì œì™¸)
    weight_decay_params = []
    no_decay_params = []

    no_decay_keys = ['class_token', 'column_embeddings', 'nan_token', 'bias', 'norm', 'ln', 'embedding']

    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        lname = name.lower()
        if any(k in lname for k in no_decay_keys):
            no_decay_params.append(param)
        else:
            weight_decay_params.append(param)

    optimizer = torch.optim.AdamW([
        {'params': weight_decay_params, 'weight_decay': CFG['WEIGHT_DECAY']},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ], lr=CFG['LEARNING_RATE'])
    
    print(f"ğŸ”§ Optimizer ì„¤ì •:")
    print(f"   â€¢ Learning Rate: {CFG['LEARNING_RATE']}")
    print(f"   â€¢ Weight Decay: {CFG['WEIGHT_DECAY']}")
    print(f"   â€¢ Weight Decay ì ìš© íŒŒë¼ë¯¸í„°: {len(weight_decay_params)}ê°œ")
    print(f"   â€¢ Weight Decay ì œì™¸ íŒŒë¼ë¯¸í„°: {len(no_decay_params)}ê°œ")

    # Early Stopping ì„¤ì •
    early_stopping = create_early_stopping_from_config(CFG)
    if early_stopping:
        print(f"ğŸ›‘ Early Stopping í™œì„±í™”:")
        print(f"   â€¢ Monitor: {CFG['EARLY_STOPPING']['MONITOR']}")
        print(f"   â€¢ Patience: {CFG['EARLY_STOPPING']['PATIENCE']}")
        print(f"   â€¢ Min Delta: {CFG['EARLY_STOPPING']['MIN_DELTA']}")
        print(f"   â€¢ Mode: {CFG['EARLY_STOPPING']['MODE']}")
    else:
        print("ğŸš€ Early Stopping ë¹„í™œì„±í™” - ì „ì²´ ì—í¬í¬ í›ˆë ¨")

    # í›ˆë ¨ ë¡œê·¸ ì´ˆê¸°í™”
    training_logs = []
    gradient_norm_logs = []
    
    # Checkpoint ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if results_dir is None:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_dir = CFG['PATHS']['RESULTS_DIR'].replace('{datetime}', timestamp)
        os.makedirs(checkpoint_dir, exist_ok=True)
    else:
        checkpoint_dir = results_dir
    print(f"ğŸ“ Checkpoint ë””ë ‰í† ë¦¬: {checkpoint_dir}")

    # Gradient norm ì„¤ì • í™•ì¸
    gradient_norm_enabled = CFG['GRADIENT_NORM']['ENABLED']
    gradient_components = CFG['GRADIENT_NORM']['COMPONENTS']
    
    if gradient_norm_enabled:
        print(f"ğŸ“Š Gradient Norm ì¸¡ì • í™œì„±í™”:")
        print(f"   â€¢ ì¸¡ì • êµ¬ì„± ìš”ì†Œ: {gradient_components}")
        print(f"   â€¢ ë¡œê·¸ ì €ì¥: {CFG['GRADIENT_NORM']['SAVE_LOGS']}")

    # 4) Training Loop
    for epoch in range(1, CFG['EPOCHS']+1):
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        train_loss = 0.0
        epoch_gradient_norms = []
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Train Epoch {epoch}")):
            optimizer.zero_grad()
            
            # TabularTransformer ëª¨ë¸ìš© ë°°ì¹˜ ì²˜ë¦¬
            x_categorical = batch.get('x_categorical').to(device)
            x_numerical = batch.get('x_numerical').to(device)
            seqs = batch.get('seqs').to(device)
            seq_lens = batch.get('seq_lengths').to(device)
            nan_mask = batch.get('nan_mask').to(device)
            ys = batch.get('ys').to(device)
            logits = model(
                x_categorical=x_categorical,
                x_numerical=x_numerical,
                x_seq=seqs,
                seq_lengths=seq_lens,
                nan_mask=nan_mask
            )
            
            loss = criterion(logits, ys)
            loss.backward()
            
            # Gradient norm ì¸¡ì • (backward í›„, step ì „)
            if gradient_norm_enabled:
                gradient_norms = calculate_gradient_norms(model, gradient_components)
                epoch_gradient_norms.append(gradient_norms)
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ ìƒì„¸ ì¶œë ¥
                if batch_idx == 0:
                    print_gradient_norms(gradient_norms, f"[Epoch {epoch}] ")
                    
                    # Gradient ë¬¸ì œ ì²´í¬
                    issues = check_gradient_issues(gradient_norms)
                    print_gradient_issues(issues)
            
            optimizer.step()
            train_loss += loss.item() * ys.size(0)
        
        train_loss /= len(train_dataset)
        
        # ì—í¬í¬ë³„ í‰ê·  gradient norm ê³„ì‚°
        if gradient_norm_enabled and epoch_gradient_norms:
            avg_gradient_norms = {}
            for component in gradient_components:
                component_norms = [gn[component] for gn in epoch_gradient_norms if component in gn]
                avg_gradient_norms[f'{component}_grad_norm'] = np.mean(component_norms) if component_norms else 0.0
            
            # Gradient norm ë¡œê·¸ ì €ì¥
            gradient_log_entry = {
                'epoch': epoch,
                **avg_gradient_norms
            }
            gradient_norm_logs.append(gradient_log_entry)

        # ê²€ì¦ ë‹¨ê³„ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        val_metrics = evaluate_model(model, val_loader, device, "tabular_transformer")
        
        # ë¡œê·¸ ì¶œë ¥
        print(f"[Epoch {epoch}] Train Loss: {train_loss:.4f}")
        print_metrics(val_metrics, "Val ")
        
        # í›ˆë ¨ ë¡œê·¸ ì €ì¥
        log_entry = {
            'epoch': epoch,
            'train_loss': train_loss,
            'val_loss': val_metrics['loss'],
            'val_ap': val_metrics['ap'],
            'val_wll': val_metrics['wll'],
            'val_score': val_metrics['score']
        }
        training_logs.append(log_entry)
        
        # 5 epochë§ˆë‹¤ checkpoint ì €ì¥
        if epoch % 5 == 0:
            save_checkpoint(model, epoch, optimizer, train_loss, val_metrics, checkpoint_dir, CFG=CFG)
        
        # Early Stopping ì²´í¬ (Score ê¸°ì¤€)
        monitor_value = val_metrics[CFG['EARLY_STOPPING']['MONITOR'].replace('val_', '')]
        if early_stopping:
            if early_stopping(monitor_value, model):
                print(f"ğŸ í›ˆë ¨ ì¡°ê¸° ì¢…ë£Œ (Epoch {epoch}/{CFG['EPOCHS']})")
                # ì¡°ê¸° ì¢…ë£Œ ì‹œì—ë„ checkpoint ì €ì¥
                save_checkpoint(model, epoch, optimizer, train_loss, val_metrics, checkpoint_dir, CFG=CFG)
                break

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    if early_stopping:
        best_score = early_stopping.get_best_score()
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {CFG['EARLY_STOPPING']['MONITOR']} = {best_score:.6f}")
    
    # ìµœì¢… checkpoint ì €ì¥ (í›ˆë ¨ ì™„ë£Œ ì‹œ)
    print(f"ğŸ’¾ ìµœì¢… checkpoint ì €ì¥ ì¤‘...")
    save_checkpoint(model, epoch, optimizer, train_loss, val_metrics, checkpoint_dir, CFG=CFG)
    
    # Best checkpoint ì €ì¥ (ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜)
    if early_stopping and early_stopping.get_best_weights() is not None:
        print(f"ğŸ† Best checkpoint ì €ì¥ ì¤‘...")
        best_checkpoint_path = os.path.join(checkpoint_dir, "best.pth")
        torch.save(early_stopping.get_best_weights(), best_checkpoint_path)
        print(f"âœ… Best checkpoint ì €ì¥ ì™„ë£Œ: {best_checkpoint_path}")
        print(f"   â€¢ Best {CFG['EARLY_STOPPING']['MONITOR']}: {early_stopping.get_best_score():.6f}")
    else:
        print(f"âš ï¸  Best checkpoint ì €ì¥ ê±´ë„ˆëœ€ (Early Stopping ë¹„í™œì„±í™” ë˜ëŠ” ê°€ì¤‘ì¹˜ ì—†ìŒ)")

    # í›ˆë ¨ ë¡œê·¸ ì €ì¥
    if CFG['METRICS']['SAVE_LOGS']:
        # results_dirê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        if results_dir is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = CFG['PATHS']['RESULTS_DIR'].replace('{datetime}', timestamp)
            os.makedirs(results_dir, exist_ok=True)
        log_filepath = results_dir + "/" + CFG['METRICS']['LOG_FILE']
        save_training_logs(training_logs, log_filepath)
        
        # ìµœê³  ì„±ëŠ¥ ì •ë³´ ì¶œë ¥
        best_info = get_best_checkpoint_info(training_logs)
        if best_info:
            print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸:")
            print(f"   â€¢ Epoch: {best_info['epoch']}")
            print(f"   â€¢ Val Score: {best_info['val_score']:.6f}")
            print(f"   â€¢ Val AP: {best_info['val_ap']:.6f}")
            print(f"   â€¢ Val WLL: {best_info['val_wll']:.6f}")

    # Gradient norm ë¡œê·¸ ì €ì¥ ë° ë¶„ì„
    if gradient_norm_enabled and CFG['GRADIENT_NORM']['SAVE_LOGS'] and gradient_norm_logs:
        # RESULTS_DIRì—ì„œ {datetime}ì„ ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì¹˜í™˜ (ì´ë¯¸ ìœ„ì—ì„œ ìƒì„±ë¨)
        gradient_log_filepath = results_dir + "/" + CFG['GRADIENT_NORM']['LOG_FILE']
        save_gradient_norm_logs(gradient_norm_logs, gradient_log_filepath)
        
        # Gradient í–‰ë™ ë¶„ì„
        gradient_analysis = analyze_gradient_behavior(gradient_norm_logs)
        print_gradient_analysis(gradient_analysis)

    return model

def save_model(model, path="model.pth"):
    """ëª¨ë¸ ì €ì¥ í•¨ìˆ˜"""
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def save_checkpoint(model, epoch, optimizer, train_loss, val_metrics, checkpoint_dir, CFG):
    """Checkpoint ì €ì¥ í•¨ìˆ˜"""
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_metrics': val_metrics,
        'config': CFG
    }
    
    torch.save(checkpoint, checkpoint_path)
    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    return checkpoint_path

def load_checkpoint(checkpoint_path, model, optimizer=None):
    """Checkpoint ë¡œë“œ í•¨ìˆ˜"""
    checkpoint = torch.load(checkpoint_path, weights_only=False)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f"ğŸ“‚ Checkpoint loaded: {checkpoint_path}")
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Train Loss: {checkpoint['train_loss']:.4f}")
    print(f"   Val Score: {checkpoint['val_metrics']['score']:.4f}")
    
    return checkpoint['epoch'], checkpoint['train_loss'], checkpoint['val_metrics']

def load_model(model, path="model.pth"):
    """ëª¨ë¸ ì €ì¥ í•¨ìˆ˜"""
    model.load_state_dict(torch.load(path, weights_only=True))
    print(f"Model loaded from {path}")
    return model
