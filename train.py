import os
from datetime import datetime

import torch
import torch.optim as optim
import torch.nn as nn
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from data_loader import (
    FeatureProcessor,
    ClickDataset,
    collate_fn_transformer_train,
    save_feature_processor,
    load_test_data,
)
from torch.utils.data import DataLoader
from early_stopping import create_early_stopping_from_config
from gradient_norm import (
    analyze_gradient_behavior,
    calculate_gradient_norms,
    print_gradient_analysis,
    save_gradient_norm_logs,
)
from metrics import (
    evaluate_model,
    print_metrics,
)
from model import create_tabular_transformer_model, create_widedeep_ctr_model

def print_model_summary(model, log_file_path=None):
    """ëª¨ë¸ì˜ ìƒì„¸ êµ¬ì¡°ë¥¼ ì¶œë ¥í•˜ê³  ë¡œê·¸ íŒŒì¼ì— ì €ì¥"""
    print("ğŸ” ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
    
    # ê¸°ë³¸ summary ìƒì„±
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # ë ˆì´ì–´ë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    layer_info = []
    for name, module in model.named_modules():
        if len(list(module.children())) == 0:  # leaf module
            param_count = sum(p.numel() for p in module.parameters())
            trainable_count = sum(p.numel() for p in module.parameters() if p.requires_grad)
            
            # ëª¨ë“ˆ íƒ€ì…ê³¼ ì´ë¦„
            module_type = type(module).__name__
            if name:
                layer_name = f"{name} ({module_type})"
            else:
                layer_name = f"({module_type})"
            
            # ì¶œë ¥ í˜•íƒœ ì¶”ì •
            if hasattr(module, 'out_features'):
                output_shape = f"(*, {module.out_features})"
            elif hasattr(module, 'num_embeddings'):
                output_shape = f"(*, {module.embedding_dim})"
            elif hasattr(module, 'hidden_size'):
                output_shape = f"(*, {module.hidden_size})"
            else:
                output_shape = "(*, *)"
            
            layer_info.append({
                'name': layer_name,
                'output_shape': output_shape,
                'param_count': param_count,
                'trainable': trainable_count > 0
            })
    
    # Summary í…ìŠ¤íŠ¸ ìƒì„±
    summary_lines = []
    summary_lines.append(f"ëª¨ë¸ êµ¬ì¡° Summary - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_lines.append("=" * 80)
    summary_lines.append(f"{'Layer (type)':<50} {'Output Shape':<20} {'Param #':<15} {'Trainable':<10}")
    summary_lines.append("=" * 80)
    
    for layer in layer_info:
        line = f"{layer['name']:<50} {layer['output_shape']:<20} {layer['param_count']:<15,} {'Yes' if layer['trainable'] else 'No':<10}"
        summary_lines.append(line)
    
    summary_lines.append("=" * 80)
    summary_lines.append(f"Total params: {total_params:,}")
    summary_lines.append(f"Trainable params: {trainable_params:,}")
    summary_lines.append(f"Non-trainable params: {total_params - trainable_params:,}")
    summary_lines.append("=" * 80)
    
    summary_text = "\n".join(summary_lines)
    
    
    # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
    if log_file_path:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        print(f"ğŸ“‹ ëª¨ë¸ êµ¬ì¡°ê°€ ë¡œê·¸ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {log_file_path}")


def train_model(train_df, target_col, CFG, device, results_dir, fabric=None):
    """ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ (Lightning Fabric ì§€ì›)"""
    
    # 1) split
    tr_df, va_df = train_test_split(train_df, test_size=CFG['VAL_SPLIT'], random_state=42, shuffle=True, stratify=train_df['clicked'])
    
    # Stratified split ê²°ê³¼ í™•ì¸
    print("ğŸ“Š Stratified Split ê²°ê³¼:")
    print(f"   â€¢ ì „ì²´ ë°ì´í„°: {len(train_df):,}ê°œ (clicked=0: {len(train_df[train_df['clicked']==0]):,}ê°œ, clicked=1: {len(train_df[train_df['clicked']==1]):,}ê°œ)")
    print(f"   â€¢ í›ˆë ¨ ë°ì´í„°: {len(tr_df):,}ê°œ (clicked=0: {len(tr_df[tr_df['clicked']==0]):,}ê°œ, clicked=1: {len(tr_df[tr_df['clicked']==1]):,}ê°œ)")
    print(f"   â€¢ ê²€ì¦ ë°ì´í„°: {len(va_df):,}ê°œ (clicked=0: {len(va_df[va_df['clicked']==0]):,}ê°œ, clicked=1: {len(va_df[va_df['clicked']==1]):,}ê°œ)")
    
    # í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
    train_ratio_0 = len(tr_df[tr_df['clicked']==0]) / len(tr_df)
    train_ratio_1 = len(tr_df[tr_df['clicked']==1]) / len(tr_df)
    val_ratio_0 = len(va_df[va_df['clicked']==0]) / len(va_df)
    val_ratio_1 = len(va_df[va_df['clicked']==1]) / len(va_df)
    print(f"   â€¢ í›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨: clicked=0 ({train_ratio_0:.3f}), clicked=1 ({train_ratio_1:.3f})")
    print(f"   â€¢ ê²€ì¦ ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨: clicked=0 ({val_ratio_0:.3f}), clicked=1 ({val_ratio_1:.3f})")

    # 2) Dataset / Loader
    # FeatureProcessor ìƒì„± ë° í•™ìŠµ
    # test ë°ì´í„°ë„ ë¡œë“œí•˜ì—¬ ë²”ì£¼í˜• í”¼ì²˜ì˜ ëª¨ë“  ê°’ì„ íŒŒì•…
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘ (ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”©ìš©)...")
    test_df = load_test_data(CFG)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test_df.shape}")
    
    feature_processor = FeatureProcessor(config=CFG, normalization_stats_path="analysis/results/normalization_stats.json")
    feature_processor.fit(tr_df, test_df)  # trainê³¼ testë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ fit
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ (ë” ì´ìƒ í•„ìš” ì—†ìŒ)
    del test_df
    import gc
    gc.collect()
    print("ğŸ—‘ï¸  í…ŒìŠ¤íŠ¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ")
    
    # FeatureProcessor ì €ì¥ (ì˜ˆì¸¡ ì‹œ ì‚¬ìš©) - rank 0ì—ì„œë§Œ ì €ì¥
    if fabric is None or fabric.is_global_zero:
        feature_processor_path = os.path.join(results_dir, "feature_processor.pkl")
        save_feature_processor(feature_processor, feature_processor_path)
    
    # í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = ClickDataset(tr_df, feature_processor, target_col, has_target=True, has_id=False)
    val_dataset = ClickDataset(va_df, feature_processor, target_col, has_target=True, has_id=False)
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, collate_fn=collate_fn_transformer_train)
    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_transformer_train)
    
    # Fabricì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° DataLoaderë¥¼ ë˜í•‘
    if fabric:
        print(f"ğŸ”§ DataLoaderë¥¼ Fabricìœ¼ë¡œ ë˜í•‘ ì¤‘...")
        train_loader = fabric.setup_dataloaders(train_loader)
        val_loader = fabric.setup_dataloaders(val_loader)
        print(f"âœ… DataLoader Fabric ë˜í•‘ ì™„ë£Œ")

    # 3) ëª¨ë¸ ìƒì„±
    categorical_cardinalities = list(feature_processor.categorical_cardinalities.values())
    num_categorical_features = len(feature_processor.categorical_features)
    num_numerical_features = len(feature_processor.numerical_features)
    
    # ëª¨ë¸ íƒ€ì… ê²°ì •
    model_type = CFG.get('MODEL_TYPE', 'transformer')  # ê¸°ë³¸ê°’: transformer
    
    if model_type == 'widedeep':
        # WideDeepCTR ëª¨ë¸ ìƒì„±
        model = create_widedeep_ctr_model(
            num_features=num_numerical_features,
            cat_cardinalities=categorical_cardinalities,
            emb_dim=CFG['MODEL']['WIDEDEEP']['EMB_DIM'],
            lstm_hidden=CFG['MODEL']['WIDEDEEP']['LSTM_HIDDEN'],
            hidden_units=CFG['MODEL']['WIDEDEEP']['HIDDEN_UNITS'],
            dropout=CFG['MODEL']['WIDEDEEP']['DROPOUT'],
            device=device
        )
        model_type_name = "WideDeepCTR"
    else:
        # TabularTransformer ëª¨ë¸ ìƒì„± (ê¸°ë³¸ê°’)
        model = create_tabular_transformer_model(
            num_categorical_features=num_categorical_features,
            categorical_cardinalities=categorical_cardinalities,
            num_numerical_features=num_numerical_features,
            lstm_hidden=CFG['MODEL']['TRANSFORMER']['HIDDEN_DIM'],
            hidden_dim=CFG['MODEL']['TRANSFORMER']['HIDDEN_DIM'],
            n_heads=CFG['MODEL']['TRANSFORMER']['N_HEADS'],
            n_layers=CFG['MODEL']['TRANSFORMER']['N_LAYERS'],
            ffn_size_factor=CFG['MODEL']['TRANSFORMER']['FFN_SIZE_FACTOR'],
            attention_dropout=CFG['MODEL']['TRANSFORMER']['ATTENTION_DROPOUT'],
            ffn_dropout=CFG['MODEL']['TRANSFORMER']['FFN_DROPOUT'],
            residual_dropout=CFG['MODEL']['TRANSFORMER']['RESIDUAL_DROPOUT'],
            device=device
        )
        model_type_name = "TabularTransformer"
    
    # ëª¨ë¸ ìƒì„± ì§í›„ summary ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ Summary:")
    print(f"   â€¢ ëª¨ë¸ íƒ€ì…: {model_type_name}")
    print(f"   â€¢ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   â€¢ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    if model_type == 'widedeep':
        print(f"   â€¢ Embedding Dimension: {CFG['MODEL']['WIDEDEEP']['EMB_DIM']}")
        print(f"   â€¢ LSTM Hidden Size: {CFG['MODEL']['WIDEDEEP']['LSTM_HIDDEN']}")
        print(f"   â€¢ Hidden Units: {CFG['MODEL']['WIDEDEEP']['HIDDEN_UNITS']}")
        print(f"   â€¢ Dropout Rates: {CFG['MODEL']['WIDEDEEP']['DROPOUT']}")
    else:
        print(f"   â€¢ Hidden Dimension: {CFG['MODEL']['TRANSFORMER']['HIDDEN_DIM']}")
        print(f"   â€¢ Attention Heads: {CFG['MODEL']['TRANSFORMER']['N_HEADS']}")
        print(f"   â€¢ Transformer Layers: {CFG['MODEL']['TRANSFORMER']['N_LAYERS']}")
        print(f"   â€¢ LSTM Hidden Size: {CFG['MODEL']['TRANSFORMER']['HIDDEN_DIM']}")
    
    print(f"   â€¢ ë²”ì£¼í˜• í”¼ì²˜ ìˆ˜: {num_categorical_features}")
    print(f"   â€¢ ìˆ˜ì¹˜í˜• í”¼ì²˜ ìˆ˜: {num_numerical_features}")
    print(f"   â€¢ Device: {device}")
    
        # ìƒì„¸ ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print("\nğŸ” ìƒì„¸ ëª¨ë¸ êµ¬ì¡°:")
    if results_dir:
        model_summary_log_path = os.path.join(results_dir, f"model_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        print_model_summary(model, model_summary_log_path)
    else:
        print_model_summary(model)

    # ëª¨ë¸ì„ Fabricìœ¼ë¡œ ë˜í•‘ (ë©€í‹° GPU ì§€ì›)
    if fabric:
        print(f"ğŸ”§ ëª¨ë¸ì„ Fabricìœ¼ë¡œ ë˜í•‘ ì¤‘...")
        print(f"   â€¢ World Size: {fabric.world_size}")
        print(f"   â€¢ Local Rank: {fabric.local_rank}")
        print(f"   â€¢ Global Rank: {fabric.global_rank}")
        model = fabric.setup_module(model)
        print(f"âœ… ëª¨ë¸ Fabric ë˜í•‘ ì™„ë£Œ")
    else:
        model = model.to(device)
        print(f"ğŸ”§ ëª¨ë¸ì„ {device}ë¡œ ì´ë™")

    criterion = nn.BCEWithLogitsLoss()
    
    # Loss functionì€ Fabricìœ¼ë¡œ ë˜í•‘í•˜ì§€ ì•ŠìŒ (íŒŒë¼ë¯¸í„°ê°€ ì—†ì–´ì„œ DDP ë¶ˆê°€)
    print(f"âœ… Criterion ì„¤ì • ì™„ë£Œ (Fabric ë˜í•‘ ë¶ˆí•„ìš”)")

    # Weight decay ì ìš© (íŠ¹ì • íŒŒë¼ë¯¸í„° ì œì™¸)
    weight_decay_params = []
    no_decay_params = []

    no_decay_keys = ['class_token', 'column_embeddings', 'bias', 'norm', 'ln', 'embedding']

    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        lname = name.lower()
        if any(k in lname for k in no_decay_keys):
            no_decay_params.append(param)
        else:
            weight_decay_params.append(param)

    optimizer = torch.optim.AdamW([
        {'params': weight_decay_params, 'weight_decay': CFG['WEIGHT_DECAY']},
        {'params': no_decay_params, 'weight_decay': 0.0}
    ], lr=CFG['LEARNING_RATE'])
    
    # ì˜µí‹°ë§ˆì´ì €ë¥¼ Fabricìœ¼ë¡œ ë˜í•‘ (ë©€í‹° GPU ì§€ì›)
    if fabric:
        optimizer = fabric.setup_optimizers(optimizer)
        print(f"âœ… ì˜µí‹°ë§ˆì´ì € Fabric ë˜í•‘ ì™„ë£Œ")
    
    # Warmup ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    warmup_enabled = CFG.get('WARMUP', {}).get('ENABLED', False)
    warmup_epochs = CFG.get('WARMUP', {}).get('WARMUP_EPOCHS', 2)
    
    if warmup_enabled:
        print("ğŸ”¥ Warmup ìŠ¤ì¼€ì¤„ëŸ¬ í™œì„±í™”:")
        print(f"   â€¢ Warmup Epochs: {warmup_epochs}")
        print(f"   â€¢ Base Learning Rate: {CFG['LEARNING_RATE']}")
    else:
        print("ğŸš€ Warmup ìŠ¤ì¼€ì¤„ëŸ¬ ë¹„í™œì„±í™”")
    
    print("ğŸ”§ Optimizer ì„¤ì •:")
    print(f"   â€¢ Learning Rate: {CFG['LEARNING_RATE']}")
    print(f"   â€¢ Weight Decay: {CFG['WEIGHT_DECAY']}")
    print(f"   â€¢ Weight Decay ì ìš© íŒŒë¼ë¯¸í„°: {len(weight_decay_params)}ê°œ")
    print(f"   â€¢ Weight Decay ì œì™¸ íŒŒë¼ë¯¸í„°: {len(no_decay_params)}ê°œ")

    # Early Stopping ì„¤ì •
    early_stopping = create_early_stopping_from_config(CFG)
    if early_stopping:
        print("ğŸ›‘ Early Stopping í™œì„±í™”:")
        print(f"   â€¢ Monitor: {CFG['EARLY_STOPPING']['MONITOR']}")
        print(f"   â€¢ Patience: {CFG['EARLY_STOPPING']['PATIENCE']}")
        print(f"   â€¢ Min Delta: {CFG['EARLY_STOPPING']['MIN_DELTA']}")
        print(f"   â€¢ Mode: {CFG['EARLY_STOPPING']['MODE']}")
    else:
        print("ğŸš€ Early Stopping ë¹„í™œì„±í™” - ì „ì²´ ì—í¬í¬ í›ˆë ¨")

    # í›ˆë ¨ ë¡œê·¸ ì´ˆê¸°í™”
    training_logs = []
    gradient_norm_logs = []
    
    # ìƒì„¸ ë¡œê·¸ íŒŒì¼ ì„¤ì • (CSV í˜•íƒœ)
    train_log_path = None
    if results_dir:
        train_log_path = os.path.join(results_dir, "training_logs.csv")
        
        # CSV íŒŒì¼ í—¤ë” ì‘ì„±
        with open(train_log_path, 'w', encoding='utf-8') as f:
            f.write("step,epoch,batch_idx,train_loss,learning_rate,grad_norm_lstm,grad_norm_model,val_loss,val_ap,val_wll,val_score\n")
        
        print(f"ğŸ“Š ìƒì„¸ ë¡œê·¸ íŒŒì¼ ìƒì„± (CSV): {train_log_path}")
    
    # Checkpoint ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if results_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_dir = CFG['PATHS']['RESULTS_DIR'].replace('{datetime}', timestamp)
        os.makedirs(checkpoint_dir, exist_ok=True)
    else:
        checkpoint_dir = results_dir
    print(f"ğŸ“ Checkpoint ë””ë ‰í† ë¦¬: {checkpoint_dir}")

    # Gradient norm ì„¤ì • í™•ì¸
    gradient_norm_enabled = CFG['GRADIENT_NORM']['ENABLED']
    gradient_components = CFG['GRADIENT_NORM']['COMPONENTS']
    
    if gradient_norm_enabled:
        print("ğŸ“Š Gradient Norm ì¸¡ì • í™œì„±í™”:")
        print(f"   â€¢ ì¸¡ì • êµ¬ì„± ìš”ì†Œ: {gradient_components}")
        print(f"   â€¢ ë¡œê·¸ ì €ì¥: {CFG['GRADIENT_NORM']['SAVE_LOGS']}")

    
    # 4) Training Loop with Step-based Logging
    scheduler = optim.lr_scheduler.MultiStepLR(
        optimizer, 
        milestones=[5, 10, 15],
        gamma=0.5           # ê° milestoneì—ì„œ lr * 0.5
    )
    global_step = 0
    steps_per_epoch = len(train_loader)
    total_steps = CFG['EPOCHS'] * steps_per_epoch
    warmup_steps = warmup_epochs * steps_per_epoch if warmup_enabled else 0
    
    print("ğŸ“Š í›ˆë ¨ ì„¤ì •:")
    print(f"   â€¢ ì´ ìŠ¤í…: {total_steps}")
    print(f"   â€¢ ì—í¬í¬ë‹¹ ìŠ¤í…: {steps_per_epoch}")
    print(f"   â€¢ Warmup ìŠ¤í…: {warmup_steps}")
    
    for epoch in range(1, CFG['EPOCHS']+1):
        # í›ˆë ¨ ë‹¨ê³„
        model.train()
        epoch_train_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Train Epoch {epoch}")):
            global_step += 1
            
            # Warmup ìŠ¤ì¼€ì¤„ë§
            if warmup_enabled and global_step <= warmup_steps:
                # Linear warmup: 0ì—ì„œ base_lrê¹Œì§€ ì„ í˜• ì¦ê°€
                warmup_factor = global_step / warmup_steps
                current_lr = CFG['LEARNING_RATE'] * warmup_factor
                for param_group in optimizer.param_groups:
                    param_group['lr'] = current_lr
            elif warmup_enabled and global_step == warmup_steps + 1:
                # Warmup ì™„ë£Œ í›„ base learning rateë¡œ ì„¤ì •
                for param_group in optimizer.param_groups:
                    param_group['lr'] = CFG['LEARNING_RATE']
            
            optimizer.zero_grad()
            
            # TabularTransformer ëª¨ë¸ìš© ë°°ì¹˜ ì²˜ë¦¬
            # Fabricì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë°°ì¹˜ê°€ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ë¨
            x_categorical = batch.get('x_categorical')
            x_numerical = batch.get('x_numerical')
            seqs = batch.get('seqs')
            seq_lens = batch.get('seq_lengths')
            ys = batch.get('ys')
            
            # Fabricì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì´ë™
            if not fabric:
                x_categorical = x_categorical.to(device)
                x_numerical = x_numerical.to(device)
                seqs = seqs.to(device)
                seq_lens = seq_lens.to(device)
                ys = ys.to(device)
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ forward í˜¸ì¶œ ë°©ì‹ ê²°ì •
            if model_type == 'widedeep':
                # WideDeepCTR ëª¨ë¸
                logits = model(
                    num_x=x_numerical,
                    cat_x=x_categorical,
                    seqs=seqs,
                    seq_lengths=seq_lens
                )
            else:
                # TabularTransformer ëª¨ë¸
                logits = model(
                    x_categorical=x_categorical,
                    x_numerical=x_numerical,
                    x_seq=seqs,
                    seq_lengths=seq_lens
                )
            
            loss = criterion(logits, ys)
            
            # Fabricì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° backwardì™€ stepì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            if fabric:
                fabric.backward(loss)
                optimizer.step()
            else:
                loss.backward()
            
            # Gradient norm ì¸¡ì • (backward í›„, step ì „)
            gradient_norms = None
            if gradient_norm_enabled and global_step % 100 == 0:
                gradient_norms = calculate_gradient_norms(model, gradient_components)
            
            # Fabricì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ step í˜¸ì¶œ
            if not fabric:
                optimizer.step()
            
            epoch_train_loss += loss.item() * ys.size(0)
            
            # ìŠ¤í…ë³„ ë¡œê¹… (10 ìŠ¤í…ë§ˆë‹¤ ì €ì¥)
            current_lr = optimizer.param_groups[0]['lr']
            
            # 10 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì €ì¥
            if global_step % 100 == 0:
                log_entry = {
                    'step': global_step,
                    'epoch': epoch,
                    'batch_idx': batch_idx,
                    'train_loss': loss.item(),
                    'learning_rate': current_lr,
                    'gradient_norms': gradient_norms if gradient_norms is not None else {'lstm': 0.0, 'model': 0.0, 'total': 0.0}
                }                
                # ì‹¤ì‹œê°„ìœ¼ë¡œ CSV íŒŒì¼ì— ê¸°ë¡
                if train_log_path:
                    with open(train_log_path, 'a', encoding='utf-8') as f:
                        f.write(f"{log_entry['step']},{log_entry['epoch']},{log_entry['batch_idx']},{log_entry['train_loss']:.6f},{log_entry['learning_rate']:.8f},{log_entry['gradient_norms']['lstm']:.6f},{log_entry['gradient_norms']['model']:.6f},,,,\n")

                print(f"[Step {global_step}] Epoch {epoch}/{CFG['EPOCHS']}, Batch {batch_idx+1}/{steps_per_epoch}")
                print(f"   â€¢ Train Loss: {loss.item():.4f}")
                print(f"   â€¢ Learning Rate: {current_lr:.6f}")
                if warmup_enabled and global_step <= warmup_steps:
                    print(f"   â€¢ Warmup Progress: {global_step}/{warmup_steps} ({global_step/warmup_steps*100:.1f}%)")
        
        epoch_train_loss /= len(train_dataset)

        # ê²€ì¦ ë‹¨ê³„ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í‰ê°€
        eval_model_type = "widedeep_ctr" if model_type == 'widedeep' else "tabular_transformer"
        val_metrics = evaluate_model(model, val_loader, device, eval_model_type, fabric)
        
        # ì—í¬í¬ë³„ ë¡œê·¸ ì¶œë ¥
        print(f"\n[Epoch {epoch}/{CFG['EPOCHS']}] Summary:")
        print(f"   â€¢ Avg Train Loss: {epoch_train_loss:.4f}")
        print(f"   â€¢ Current LR: {optimizer.param_groups[0]['lr']:.6f}")
        print_metrics(val_metrics, "Val ")
        
        # ìƒì„¸ ë¡œê·¸ íŒŒì¼ì— ì—í¬í¬ë³„ ê²€ì¦ ê²°ê³¼ ê¸°ë¡ (ë§¤ ì—í­ë§ˆë‹¤)
        if train_log_path:
            # ê²€ì¦ ê²°ê³¼ë§Œ ì¶”ê°€ (ìŠ¤í…ë³„ ë¡œê·¸ëŠ” ì´ë¯¸ ì‹¤ì‹œê°„ìœ¼ë¡œ ê¸°ë¡ë¨)
            with open(train_log_path, 'a', encoding='utf-8') as f:
                f.write(f"{global_step},{epoch},-1,{epoch_train_loss:.6f},{optimizer.param_groups[0]['lr']:.8f},,,{val_metrics['loss']:.6f},{val_metrics['ap']:.6f},{val_metrics['wll']:.6f},{val_metrics['score']:.6f}\n")
    
        # # 5 epochë§ˆë‹¤ checkpoint ì €ì¥
        # if epoch % 5 == 0:
        #     save_checkpoint(model, epoch, optimizer, epoch_train_loss, val_metrics, checkpoint_dir, CFG=CFG)
        
        # Early Stopping ì²´í¬ (Score ê¸°ì¤€)
        monitor_value = val_metrics[CFG['EARLY_STOPPING']['MONITOR'].replace('val_', '')]
        if early_stopping:
            if early_stopping(monitor_value, model):
                print(f"ğŸ í›ˆë ¨ ì¡°ê¸° ì¢…ë£Œ (Epoch {epoch}/{CFG['EPOCHS']})")
                # ì¡°ê¸° ì¢…ë£Œ ì‹œì—ë„ checkpoint ì €ì¥
                save_checkpoint(model, epoch, optimizer, epoch_train_loss, val_metrics, checkpoint_dir, CFG=CFG)
                break
        
        # Step decay
        current_lr = optimizer.param_groups[0]['lr']
        scheduler.step()
        new_lr = optimizer.param_groups[0]['lr']
        
        if current_lr != new_lr:
            print(f"ğŸ“‰ Epoch {epoch}: Learning Rate ë³€ê²½ {current_lr:.6f} â†’ {new_lr:.6f}")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    if early_stopping:
        best_score = early_stopping.get_best_score()
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥: {CFG['EARLY_STOPPING']['MONITOR']} = {best_score:.6f}")
    
    # ìµœì¢… checkpoint ì €ì¥ (í›ˆë ¨ ì™„ë£Œ ì‹œ)
    print("ğŸ’¾ ìµœì¢… checkpoint ì €ì¥ ì¤‘...")
    save_checkpoint(model, epoch, optimizer, epoch_train_loss, val_metrics, checkpoint_dir, CFG=CFG)
    
    # Best checkpoint ì €ì¥ (ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜)
    if early_stopping and early_stopping.get_best_weights() is not None:
        print("ğŸ† Best checkpoint ì €ì¥ ì¤‘...")
        best_checkpoint_path = os.path.join(checkpoint_dir, "best.pth")
        torch.save(early_stopping.get_best_weights(), best_checkpoint_path)
        print(f"âœ… Best checkpoint ì €ì¥ ì™„ë£Œ: {best_checkpoint_path}")
        print(f"   â€¢ Best {CFG['EARLY_STOPPING']['MONITOR']}: {early_stopping.get_best_score():.6f}")
    else:
        print("âš ï¸  Best checkpoint ì €ì¥ ê±´ë„ˆëœ€ (Early Stopping ë¹„í™œì„±í™” ë˜ëŠ” ê°€ì¤‘ì¹˜ ì—†ìŒ)")

    # í›ˆë ¨ ë¡œê·¸ ì €ì¥ (ìŠ¤í… ê¸°ë°˜) - CSVëŠ” ì´ë¯¸ ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥ë¨
    if CFG['METRICS']['SAVE_LOGS']:
        # results_dirê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        if results_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = CFG['PATHS']['RESULTS_DIR'].replace('{datetime}', timestamp)
            os.makedirs(results_dir, exist_ok=True)
        
 
        print(f"ğŸ“Š CSV ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {train_log_path}")
        
        # Warmup ì •ë³´ ì¶œë ¥
        if warmup_enabled:
            print("ğŸ”¥ Warmup ì™„ë£Œ:")
            print(f"   â€¢ Warmup Steps: {warmup_steps}")
            print(f"   â€¢ Total Steps: {global_step}")
            print(f"   â€¢ Final LR: {CFG['LEARNING_RATE']:.6f}")
        
        # ìµœê³  ì„±ëŠ¥ ì •ë³´ ì¶œë ¥ (ì—í¬í¬ë³„ ê²€ì¦ ë¡œê·¸ì—ì„œ)
        epoch_logs = [log for log in training_logs if 'val_score' in log]
        if epoch_logs:
            best_epoch_log = max(epoch_logs, key=lambda x: x['val_score'])
            print("ğŸ† ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸:")
            print(f"   â€¢ Step: {best_epoch_log['step']}")
            print(f"   â€¢ Epoch: {best_epoch_log['epoch']}")
            print(f"   â€¢ Val Score: {best_epoch_log['val_score']:.6f}")
            print(f"   â€¢ Val AP: {best_epoch_log['val_ap']:.6f}")
            print(f"   â€¢ Val WLL: {best_epoch_log['val_wll']:.6f}")
            print(f"   â€¢ Learning Rate: {best_epoch_log['learning_rate']:.6f}")

    # Gradient norm ë¡œê·¸ ì €ì¥ ë° ë¶„ì„
    if gradient_norm_enabled and CFG['GRADIENT_NORM']['SAVE_LOGS'] and gradient_norm_logs:
        # RESULTS_DIRì—ì„œ {datetime}ì„ ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì¹˜í™˜ (ì´ë¯¸ ìœ„ì—ì„œ ìƒì„±ë¨)
        gradient_log_filepath = results_dir + "/" + CFG['GRADIENT_NORM']['LOG_FILE']
        save_gradient_norm_logs(gradient_norm_logs, gradient_log_filepath)
        
        # Gradient í–‰ë™ ë¶„ì„
        gradient_analysis = analyze_gradient_behavior(gradient_norm_logs)
        print_gradient_analysis(gradient_analysis)

    return model, feature_processor

def save_model(model, path):
    """ëª¨ë¸ ì €ì¥ í•¨ìˆ˜"""
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def save_checkpoint(model, epoch, optimizer, train_loss, val_metrics, checkpoint_dir, CFG):
    """Checkpoint ì €ì¥ í•¨ìˆ˜"""
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_metrics': val_metrics,
        'config': CFG
    }
    
    torch.save(checkpoint, checkpoint_path)
    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    return checkpoint_path

def load_checkpoint(checkpoint_path, model, optimizer=None):
    """Checkpoint ë¡œë“œ í•¨ìˆ˜"""
    checkpoint = torch.load(checkpoint_path, weights_only=False)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f"ğŸ“‚ Checkpoint loaded: {checkpoint_path}")
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Train Loss: {checkpoint['train_loss']:.4f}")
    print(f"   Val Score: {checkpoint['val_metrics']['score']:.4f}")
    
    return checkpoint['epoch'], checkpoint['train_loss'], checkpoint['val_metrics']

def load_model(model, path="model.pth"):
    """ëª¨ë¸ ì €ì¥ í•¨ìˆ˜"""
    model.load_state_dict(torch.load(path, weights_only=True))
    print(f"Model loaded from {path}")
    return model
