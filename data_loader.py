import json
import os
from typing import Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset

class FeatureProcessor:
    """í”¼ì²˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config, normalization_stats_path):
        self.config = config
        
        # Normalization stats ë¡œë“œ
        script_dir = os.path.dirname(os.path.abspath(__file__))
        norm_stats_full_path = os.path.join(script_dir, normalization_stats_path)
        with open(norm_stats_full_path, 'r', encoding='utf-8') as f:
            self.norm_stats = json.load(f)['statistics']
        
        # í”¼ì²˜ ë¶„ë¥˜
        self.categorical_features = self.config['MODEL']['FEATURES']['CATEGORICAL']
        self.sequential_feature = self.config['MODEL']['FEATURES']['SEQUENTIAL']
        self.excluded_features = self.config['MODEL']['FEATURES']['EXCLUDED']
        
        # ë²”ì£¼í˜• í”¼ì²˜ì˜ ì¹´ë””ë„ë¦¬í‹° ê³„ì‚°
        self.categorical_cardinalities = {}
        self.categorical_encoders = {}
        
    def fit(self, df: pd.DataFrame):
        """ë°ì´í„°ì— ë§ì¶° ì¸ì½”ë” í•™ìŠµ"""
        # ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© ì„¤ì •
        for feat in self.categorical_features:
            if feat in df.columns:
                unique_vals = df[feat].dropna().unique()
                # NaNì€ 0, ë‚˜ë¨¸ì§€ëŠ” 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ì •ìˆ˜ë¡œ ë§¤í•‘
                unique_vals_sorted = sorted(unique_vals)
                self.categorical_encoders[feat] = {val: idx + 1 for idx, val in enumerate(unique_vals_sorted)}
                self.categorical_cardinalities[feat] = len(unique_vals_sorted) + 1  # NaNì„ ìœ„í•œ 0 í¬í•¨
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ ìƒì„± (ë²”ì£¼í˜•, ì‹œí€€ìŠ¤, ID, target, ì œì™¸ í”¼ì²˜ ì œì™¸)
        exclude_cols = set(self.categorical_features + [self.sequential_feature, 'ID', 'clicked'] + self.excluded_features)
        self.numerical_features = [col for col in df.columns if col not in exclude_cols]
        
        return self
    
    def transform(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ë°ì´í„° ë³€í™˜"""
        batch_size = len(df)
        
        # ë²”ì£¼í˜• í”¼ì²˜ ì²˜ë¦¬
        categorical_data = []
        for feat in self.categorical_features:
            if feat in df.columns:
                # ë²”ì£¼í˜• ê°’ì„ ë³€í™˜: NaNì€ 0, ë‚˜ë¨¸ì§€ëŠ” 1ë¶€í„° categorical_cardinalitiesê¹Œì§€
                encoded = df[feat].map(self.categorical_encoders[feat]).fillna(0).astype(int)
                categorical_data.append(encoded.values)
            else:
                raise ValueError(f"âŒ ë²”ì£¼í˜• í”¼ì²˜ '{feat}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if categorical_data:
            x_categorical = torch.tensor(np.column_stack(categorical_data), dtype=torch.long)
        else:
            x_categorical = torch.empty(batch_size, 0, dtype=torch.long)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì²˜ë¦¬ (í‘œì¤€í™”) - ë¬´ì¡°ê±´ ì ìš©
        numerical_data = []
        for feat in self.numerical_features:
            if feat in df.columns:
                if feat in self.norm_stats:
                    # í‘œì¤€í™”: (x - mean) / std
                    mean = self.norm_stats[feat]['mean']
                    std = self.norm_stats[feat]['std']
                    # ë°ì´í„°ë¥¼ floatë¡œ ë³€í™˜
                    feat_data = pd.to_numeric(df[feat], errors='coerce')
                    # ê²°ì¸¡ì¹˜ë¥¼ ì œì™¸í•˜ê³  í‘œì¤€í™”í•œ í›„, ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ì›€
                    standardized = (feat_data - mean) / std
                    standardized = standardized.fillna(0)
                    numerical_data.append(standardized.values.astype(np.float32))
                else:
                    # norm_statsì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ
                    raise ValueError(f"âŒ {feat} í”¼ì²˜ì˜ normalization statsê°€ ì—†ìŠµë‹ˆë‹¤! config.yamlê³¼ normalization_stats.jsonì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                raise ValueError(f"âŒ {feat} í”¼ì²˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if numerical_data:
            x_numerical = torch.tensor(np.column_stack(numerical_data), dtype=torch.float32)
        else:
            x_numerical = torch.empty(batch_size, 0, dtype=torch.float32)
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ ì²˜ë¦¬
        if self.sequential_feature in df.columns:
            seq_strings = df[self.sequential_feature].astype(str).values
            sequences = []
            for s in seq_strings:
                if s and s != 'nan':
                    try:
                        arr = np.fromstring(s, sep=",", dtype=np.float32)
                        if arr.size == 0:
                            arr = np.array([0.0], dtype=np.float32)
                    except:
                        arr = np.array([0.0], dtype=np.float32)
                else:
                    arr = np.array([0.0], dtype=np.float32)
                sequences.append(torch.from_numpy(arr))
        else:
            raise ValueError(f"âŒ ì‹œí€€ìŠ¤ í”¼ì²˜ '{self.sequential_feature}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        # NaN ë§ˆìŠ¤í¬ ìƒì„± (ë²”ì£¼í˜• + ìˆ˜ì¹˜í˜• + ì‹œí€€ìŠ¤ ìˆœì„œ)
        nan_mask = []
        
        # ë²”ì£¼í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.categorical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.numerical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        if self.sequential_feature in df.columns:
            nan_mask.append(df[self.sequential_feature].isna().astype(int).values)
        else:
            nan_mask.append(np.ones(batch_size, dtype=int))
        
        nan_mask = torch.tensor(np.column_stack(nan_mask), dtype=torch.float32)
        
        return x_categorical, x_numerical, sequences, nan_mask


class ClickDataset(Dataset):
    def __init__(self, df, feature_processor: FeatureProcessor, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_processor = feature_processor
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        # í”¼ì²˜ ì²˜ë¦¬
        self.x_categorical, self.x_numerical, self.sequences, self.nan_mask = feature_processor.transform(df)

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        item = {
            'x_categorical': self.x_categorical[idx],
            'x_numerical': self.x_numerical[idx],
            'seq': self.sequences[idx],
            'nan_mask': self.nan_mask[idx]
        }
        
        # IDê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        
        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            item['y'] = y
        
        return item



def collate_fn_transformer_train(batch):
    """Transformer ëª¨ë¸ìš© í›ˆë ¨ collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    ys = [item['y'] for item in batch]  # has_target=Trueì¸ ê²½ìš°ë§Œ
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    ys = torch.stack(ys)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ys': ys
    }

def collate_fn_transformer_infer(batch):
    """Transformer ëª¨ë¸ìš© ì¶”ë¡  collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    
    # ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    ids = [item['id'] for item in batch]
    
    # IDì— Noneì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    result = {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ids': ids
    }
    
    return result


def safe_load_parquet(file_path):
    """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜ - í•­ìƒ ì „ì²´ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ - {file_path}")
    try:
        return pd.read_parquet(file_path, engine="pyarrow")
    except Exception as e:
        print(f"âš ï¸  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def load_train_data(config):
    """í›ˆë ¨ ë°ì´í„°ë§Œ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    train = safe_load_parquet(config['PATHS']['TRAIN_DATA'])
    
    print("Train shape:", train.shape)
    
    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"}
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ)
    if 'ID' in train.columns:
        train = train.drop(columns=['ID'])
        print("âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)

    return train, feature_cols, seq_col, target_col


def load_test_data(config):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ë¡œë“œ í•¨ìˆ˜"""
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test = safe_load_parquet(config['PATHS']['TEST_DATA'])
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ID ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨ (ì˜ˆì¸¡ ì‹œ í•„ìš”)
    if 'ID' not in test.columns:
        raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test.shape[0]}ê°œ í–‰, ID ì»¬ëŸ¼ í¬í•¨")
    print("Test shape:", test.shape)
    
    return test
