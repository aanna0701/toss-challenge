import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
import os
from typing import Tuple

EPS_STD = 1e-6  # í‘œì¤€í™” ë¶„ëª¨ ì•ˆì •í™”


class FeatureProcessor:
    """í”¼ì²˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (+1 ì¸ë±ì‹±, seq ê²°ì¸¡ ì •í•©, í‘œì¤€í™” eps)"""

    def __init__(self, CFG: dict, normalization_stats_path: str = "analysis/results/normalization_stats.json"):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.config = CFG

        # Normalization stats ë¡œë“œ
        norm_stats_full_path = os.path.join(script_dir, normalization_stats_path)
        with open(norm_stats_full_path, 'r', encoding='utf-8') as f:
            self.norm_stats = json.load(f)['statistics']

        # í”¼ì²˜ ë¶„ë¥˜
        self.categorical_features = self.config['MODEL']['FEATURES']['CATEGORICAL']
        self.sequential_feature = self.config['MODEL']['FEATURES']['SEQUENTIAL']
        self.excluded_features = self.config['MODEL']['FEATURES']['EXCLUDED']

        # ë²”ì£¼í˜• í”¼ì²˜ì˜ ì¹´ë””ë„ë¦¬í‹° ë° ì¸ì½”ë”
        self.categorical_cardinalities = {}
        self.categorical_encoders = {}

    def fit(self, df: pd.DataFrame):
        """ë°ì´í„°ì— ë§ì¶° ì¸ì½”ë” í•™ìŠµ (+1 ì¸ë±ì‹±: 0ì€ OOV/ë¯¸ì‹± ì˜ˆì•½)"""
        for feat in self.categorical_features:
            if feat in df.columns:
                unique_vals = df[feat].dropna().unique()
                unique_vals_sorted = sorted(unique_vals)
                # 1..N ë§¤í•‘, 0ì€ OOV/ë¯¸ì‹±
                self.categorical_encoders[feat] = {val: idx + 1 for idx, val in enumerate(unique_vals_sorted)}
                self.categorical_cardinalities[feat] = len(unique_vals_sorted) + 1  # +1 for OOV/ë¯¸ì‹±
            else:
                # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìµœì†Œ ì¹´ë””ë„ë¦¬í‹° 1 (OOVë§Œ)ë¡œ ì„¤ì •
                self.categorical_encoders[feat] = {}
                self.categorical_cardinalities[feat] = 1

        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ ìƒì„± (ë²”ì£¼í˜•, ì‹œí€€ìŠ¤, ID, target, ì œì™¸ í”¼ì²˜ ì œì™¸)
        exclude_cols = set(self.categorical_features + [self.sequential_feature, 'ID', 'clicked'] + self.excluded_features)
        self.numerical_features = [col for col in df.columns if col not in exclude_cols]
        return self

    def _encode_categorical_column(self, s: pd.Series, feat: str) -> np.ndarray:
        enc = self.categorical_encoders.get(feat, {})
        # ë¯¸ë“±ë¡/OOV â†’ 0, ê²°ì¸¡ë„ 0
        return s.map(enc).fillna(0).astype(int).values

    def transform(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, list, torch.Tensor]:
        """ë°ì´í„° ë³€í™˜: (x_categorical, x_numerical, sequences(list[tensor]), nan_mask)"""

        batch_size = len(df)

        # ë²”ì£¼í˜• ì²˜ë¦¬ (+1 ì¸ë±ì‹±)
        categorical_data = []
        for feat in self.categorical_features:
            if feat in df.columns:
                encoded = self._encode_categorical_column(df[feat], feat)
                categorical_data.append(encoded)
            else:
                # ì—†ëŠ” ì»¬ëŸ¼ì€ ì „ë¶€ 0(OOV/ë¯¸ì‹±)
                categorical_data.append(np.zeros(batch_size, dtype=np.int64))
        x_categorical = torch.tensor(np.column_stack(categorical_data), dtype=torch.long) if categorical_data \
            else torch.empty(batch_size, 0, dtype=torch.long)

        # ìˆ˜ì¹˜í˜• ì²˜ë¦¬(í‘œì¤€í™” + eps)
        numerical_data = []
        for feat in self.numerical_features:
            if feat in df.columns:
                if feat not in self.norm_stats:
                    raise ValueError(f"âŒ {feat} í”¼ì²˜ì˜ normalization statsê°€ ì—†ìŠµë‹ˆë‹¤! config.yamlê³¼ normalization_stats.jsonì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                mean = float(self.norm_stats[feat]['mean'])
                std = float(self.norm_stats[feat]['std'])
                std = max(std, EPS_STD)
                feat_data = pd.to_numeric(df[feat], errors='coerce')  # NaN ìœ ì§€
                standardized = ((feat_data - mean) / std).astype(np.float32)
                # NaNì€ ê·¸ëŒ€ë¡œ ë‘ê³  nan_maskì—ì„œ ì²˜ë¦¬ â†’ ëª¨ë¸ì—ì„œ nan_tokenìœ¼ë¡œ ë°”ê¿ˆ
                numerical_data.append(standardized.values)
            else:
                raise ValueError(f"âŒ {feat} í”¼ì²˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        x_numerical = torch.tensor(np.column_stack(numerical_data), dtype=torch.float32) if numerical_data \
            else torch.empty(batch_size, 0, dtype=torch.float32)

        # ì‹œí€€ìŠ¤ ì²˜ë¦¬: íŒŒì‹± ì‹¤íŒ¨/ë¹ˆ ë¬¸ìì—´/ë¬¸ìì—´ 'nan' â†’ ê²°ì¸¡ìœ¼ë¡œ ê°„ì£¼
        seq_is_missing = np.zeros(batch_size, dtype=np.int64)
        sequences = []
        if self.sequential_feature in df.columns:
            seq_strings = df[self.sequential_feature].astype(str).values
            for i, s in enumerate(seq_strings):
                val = s.strip()
                if (val == "") or (val.lower() == "nan"):
                    seq_is_missing[i] = 1
                    arr = np.array([0.0], dtype=np.float32)
                else:
                    try:
                        arr = np.fromstring(val, sep=",", dtype=np.float32)
                        if arr.size == 0:
                            seq_is_missing[i] = 1
                            arr = np.array([0.0], dtype=np.float32)
                    except Exception:
                        seq_is_missing[i] = 1
                        arr = np.array([0.0], dtype=np.float32)
                sequences.append(torch.from_numpy(arr))
        else:
            raise ValueError(f"âŒ ì‹œí€€ìŠ¤ í”¼ì²˜ '{self.sequential_feature}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")

        # NaN ë§ˆìŠ¤í¬ ìƒì„± (ë²”ì£¼í˜•: df.isna OR OOV? â†’ OOVëŠ” ì¸ë±ìŠ¤ 0ì´ë¯€ë¡œ ëª¨ë¸ì—ì„œ ì²˜ë¦¬,
        # ì—¬ê¸°ì„œëŠ” 'ì›ë³¸ NaN'ë§Œ í‘œì‹œ. ìˆ˜ì¹˜í˜•: ì›ë³¸ NaN. ì‹œí€€ìŠ¤: df.isna OR íŒŒì‹± ê²°ì¸¡)
        nan_mask_parts = []

        # ë²”ì£¼í˜• NaN ë§ˆìŠ¤í¬ (ì›ë³¸ NaN ê¸°ì¤€)
        for feat in self.categorical_features:
            if feat in df.columns:
                nan_mask_parts.append(df[feat].isna().astype(int).values)
            else:
                nan_mask_parts.append(np.ones(batch_size, dtype=int))

        # ìˆ˜ì¹˜í˜• NaN ë§ˆìŠ¤í¬
        for feat in self.numerical_features:
            if feat in df.columns:
                nan_mask_parts.append(df[feat].isna().astype(int).values)
            else:
                nan_mask_parts.append(np.ones(batch_size, dtype=int))

        # ì‹œí€€ìŠ¤ NaN ë§ˆìŠ¤í¬ (ì›ë³¸ NaN OR íŒŒì‹±ê²°ì¸¡)
        if self.sequential_feature in df.columns:
            seq_nan = df[self.sequential_feature].isna().astype(int).values
            nan_mask_parts.append(np.maximum(seq_nan, seq_is_missing))
        else:
            nan_mask_parts.append(np.ones(batch_size, dtype=int))

        nan_mask = torch.tensor(np.column_stack(nan_mask_parts), dtype=torch.float32)

        return x_categorical, x_numerical, sequences, nan_mask


class ClickDataset(Dataset):
    def __init__(self, df, feature_processor: FeatureProcessor, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_processor = feature_processor
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        self.x_categorical, self.x_numerical, self.sequences, self.nan_mask = feature_processor.transform(df)

        if self.has_target:
            if self.target_col is None or self.target_col not in self.df.columns:
                raise ValueError("âŒ íƒ€ê¹ƒ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        item = {
            'x_categorical': self.x_categorical[idx],
            'x_numerical': self.x_numerical[idx],
            'seq': self.sequences[idx],
            'nan_mask': self.nan_mask[idx],
        }
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        if self.has_target:
            item['y'] = torch.tensor(self.y[idx], dtype=torch.float32)
        return item


def collate_fn_transformer_train(batch):
    x_categorical = torch.stack([b['x_categorical'] for b in batch])
    x_numerical = torch.stack([b['x_numerical'] for b in batch])
    nan_masks = torch.stack([b['nan_mask'] for b in batch])
    ys = torch.stack([b['y'] for b in batch])

    seqs = [b['seq'] for b in batch]
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long).clamp_min(1)

    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ys': ys,
    }


def collate_fn_transformer_infer(batch):
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    x_categorical = torch.stack([b['x_categorical'] for b in batch])
    x_numerical = torch.stack([b['x_numerical'] for b in batch])
    nan_masks = torch.stack([b['nan_mask'] for b in batch])
    ids = [b['id'] for b in batch]
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    seqs = [b['seq'] for b in batch]
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long).clamp_min(1)

    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ids': ids,
    }


def create_data_loaders(train_df, val_df, test_df, feature_cols, seq_col, target_col, batch_size, CFG):
    feature_processor = FeatureProcessor(CFG)
    if train_df is not None and len(train_df) > 0:
        feature_processor.fit(train_df)
    else:
        # ë”ë¯¸ ë°ì´í„°ë¡œ fit
        dummy = {col: [0.0] for col in feature_cols}
        dummy[seq_col] = ["0.0"]
        dummy[target_col] = [0.0]
        feature_processor.fit(pd.DataFrame(dummy))

    # Train
    if train_df is not None and len(train_df) > 0:
        train_dataset = ClickDataset(train_df, feature_processor, target_col, has_target=True, has_id=False)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,
                                  collate_fn=collate_fn_transformer_train)
    else:
        train_loader = None
        train_dataset = None

    # Val
    if val_df is not None and len(val_df) > 0:
        val_dataset = ClickDataset(val_df, feature_processor, target_col, has_target=True, has_id=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                                collate_fn=collate_fn_transformer_train)
    else:
        val_loader = None
        val_dataset = None

    # Test
    if test_df is not None and len(test_df) > 0:
        test_dataset = ClickDataset(test_df, feature_processor, has_target=False, has_id=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                                 collate_fn=collate_fn_transformer_infer)
    else:
        test_loader = None
        test_dataset = None

    return train_loader, val_loader, test_loader, train_dataset, val_dataset, feature_processor


def load_and_preprocess_data(CFG):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    def safe_load_parquet(file_path):
        """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜ - í•­ìƒ ì „ì²´ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ - {file_path}")
        try:
            return pd.read_parquet(file_path, engine="pyarrow")
        except Exception as e:
            print(f"âš ï¸  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    all_train = safe_load_parquet(CFG['PATHS']['TRAIN_DATA'])
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test = safe_load_parquet(CFG['PATHS']['TEST_DATA'])
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ID ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨ (ì˜ˆì¸¡ ì‹œ í•„ìš”)
    if 'ID' not in test.columns:
        raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test.shape[0]}ê°œ í–‰, ID ì»¬ëŸ¼ í¬í•¨")

    print("Train shape:", all_train.shape)
    print("Test shape:", test.shape)

    # feat_e_3 missing ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œë§
    # 1. feat_e_3ì´ missingì¸ ë°ì´í„°ëŠ” ëª¨ë‘ í¬í•¨
    missing_feat_e_3 = all_train[all_train['feat_e_3'].isna()]
    
    # 2. feat_e_3ì´ ìˆëŠ” ë°ì´í„° ì¤‘ì—ì„œ clicked == 1ì¸ ë°ì´í„°
    available_feat_e_3_clicked_1 = all_train[(all_train['feat_e_3'].notna()) & (all_train['clicked'] == 1)]
    
    # 3. feat_e_3ì´ ìˆëŠ” ë°ì´í„° ì¤‘ì—ì„œ clicked == 0ì¸ ë°ì´í„°
    available_feat_e_3_clicked_0 = all_train[(all_train['feat_e_3'].notna()) & (all_train['clicked'] == 0)]
    
    # 4. missing ë°ì´í„°ì—ì„œ clicked=1 ë°ì´í„°ë¥¼ ëº€ ë§Œí¼ë§Œ clicked=0 ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§
    target_size = len(missing_feat_e_3) - len(available_feat_e_3_clicked_1)
    if target_size > 0 and len(available_feat_e_3_clicked_0) >= target_size:
        sampled_clicked_0 = available_feat_e_3_clicked_0.sample(n=target_size, random_state=42)
    else:
        # target_sizeê°€ 0 ì´í•˜ì´ê±°ë‚˜ available ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë‘ ì‚¬ìš©
        sampled_clicked_0 = available_feat_e_3_clicked_0
    
    # 5. ìµœì¢… í›ˆë ¨ ë°ì´í„° êµ¬ì„±
    train = pd.concat([missing_feat_e_3, available_feat_e_3_clicked_1, sampled_clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

    print("ğŸ“Š ìƒ˜í”Œë§ ê²°ê³¼:")
    print(f"  1. feat_e_3 missing ë°ì´í„°: {len(missing_feat_e_3):,}ê°œ")
    print(f"  2. feat_e_3 available + clicked=1: {len(available_feat_e_3_clicked_1):,}ê°œ")
    print(f"  3. feat_e_3 available + clicked=0 (ìƒ˜í”Œë§): {len(sampled_clicked_0):,}ê°œ")
    print(f"  - ì´ í›ˆë ¨ ë°ì´í„°: {len(train):,}ê°œ")
    print(f"  - ìµœì¢… clicked=0: {len(train[train['clicked']==0]):,}ê°œ")
    print(f"  - ìµœì¢… clicked=1: {len(train[train['clicked']==1]):,}ê°œ")

    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"}
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ)
    if 'ID' in train.columns:
        train = train.drop(columns=['ID'])
        print("âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)

    return train, test, feature_cols, seq_col, target_col
