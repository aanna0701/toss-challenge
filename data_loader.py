import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
import yaml
import os
from typing import Dict, List, Tuple, Optional, Union

class FeatureProcessor:
    """í”¼ì²˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "config.yaml", normalization_stats_path: str = "analysis/results/normalization_stats.json"):
        # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ê²½ë¡œ ì„¤ì •
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Config ë¡œë“œ
        config_full_path = os.path.join(script_dir, config_path)
        with open(config_full_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # Normalization stats ë¡œë“œ
        norm_stats_full_path = os.path.join(script_dir, normalization_stats_path)
        with open(norm_stats_full_path, 'r', encoding='utf-8') as f:
            self.norm_stats = json.load(f)['statistics']
        
        # í”¼ì²˜ ë¶„ë¥˜
        self.categorical_features = self.config['MODEL']['FEATURES']['CATEGORICAL']
        self.sequential_feature = self.config['MODEL']['FEATURES']['SEQUENTIAL']
        self.excluded_features = self.config['MODEL']['FEATURES']['EXCLUDED']
        
        # ë²”ì£¼í˜• í”¼ì²˜ì˜ ì¹´ë””ë„ë¦¬í‹° ê³„ì‚°
        self.categorical_cardinalities = {}
        self.categorical_encoders = {}
        
    def fit(self, df: pd.DataFrame):
        """ë°ì´í„°ì— ë§ì¶° ì¸ì½”ë” í•™ìŠµ"""
        # ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© ì„¤ì •
        for feat in self.categorical_features:
            if feat in df.columns:
                unique_vals = df[feat].dropna().unique()
                # 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ì •ìˆ˜ë¡œ ë§¤í•‘
                unique_vals_sorted = sorted(unique_vals)
                self.categorical_encoders[feat] = {val: idx for idx, val in enumerate(unique_vals_sorted)}
                self.categorical_cardinalities[feat] = len(unique_vals_sorted)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ ìƒì„± (ë²”ì£¼í˜•, ì‹œí€€ìŠ¤, ID, target, ì œì™¸ í”¼ì²˜ ì œì™¸)
        exclude_cols = set(self.categorical_features + [self.sequential_feature, 'ID', 'clicked'] + self.excluded_features)
        self.numerical_features = [col for col in df.columns if col not in exclude_cols]
        
        return self
    
    def transform(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ë°ì´í„° ë³€í™˜"""
        batch_size = len(df)
        
        # ë²”ì£¼í˜• í”¼ì²˜ ì²˜ë¦¬
        categorical_data = []
        for feat in self.categorical_features:
            if feat in df.columns:
                # ë²”ì£¼í˜• ê°’ì„ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì •ìˆ˜ë¡œ ë³€í™˜
                encoded = df[feat].map(self.categorical_encoders[feat]).fillna(0).astype(int)
                categorical_data.append(encoded.values)
            else:
                raise ValueError(f"âŒ ë²”ì£¼í˜• í”¼ì²˜ '{feat}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if categorical_data:
            x_categorical = torch.tensor(np.column_stack(categorical_data), dtype=torch.long)
        else:
            x_categorical = torch.empty(batch_size, 0, dtype=torch.long)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì²˜ë¦¬ (í‘œì¤€í™”) - ë¬´ì¡°ê±´ ì ìš©
        numerical_data = []
        for feat in self.numerical_features:
            if feat in df.columns:
                if feat in self.norm_stats:
                    # í‘œì¤€í™”: (x - mean) / std
                    mean = self.norm_stats[feat]['mean']
                    std = self.norm_stats[feat]['std']
                    # ë°ì´í„°ë¥¼ floatë¡œ ë³€í™˜ í›„ í‘œì¤€í™”
                    feat_data = pd.to_numeric(df[feat], errors='coerce').fillna(0)
                    standardized = (feat_data - mean) / std
                    numerical_data.append(standardized.values.astype(np.float32))
                else:
                    # norm_statsì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ
                    raise ValueError(f"âŒ {feat} í”¼ì²˜ì˜ normalization statsê°€ ì—†ìŠµë‹ˆë‹¤! config.yamlê³¼ normalization_stats.jsonì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                raise ValueError(f"âŒ {feat} í”¼ì²˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if numerical_data:
            x_numerical = torch.tensor(np.column_stack(numerical_data), dtype=torch.float32)
        else:
            x_numerical = torch.empty(batch_size, 0, dtype=torch.float32)
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ ì²˜ë¦¬
        if self.sequential_feature in df.columns:
            seq_strings = df[self.sequential_feature].astype(str).values
            sequences = []
            for s in seq_strings:
                if s and s != 'nan':
                    try:
                        arr = np.fromstring(s, sep=",", dtype=np.float32)
                        if arr.size == 0:
                            arr = np.array([0.0], dtype=np.float32)
                    except:
                        arr = np.array([0.0], dtype=np.float32)
                else:
                    arr = np.array([0.0], dtype=np.float32)
                sequences.append(torch.from_numpy(arr))
        else:
            raise ValueError(f"âŒ ì‹œí€€ìŠ¤ í”¼ì²˜ '{self.sequential_feature}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        # NaN ë§ˆìŠ¤í¬ ìƒì„± (ë²”ì£¼í˜• + ìˆ˜ì¹˜í˜• + ì‹œí€€ìŠ¤ ìˆœì„œ)
        nan_mask = []
        
        # ë²”ì£¼í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.categorical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.numerical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        if self.sequential_feature in df.columns:
            nan_mask.append(df[self.sequential_feature].isna().astype(int).values)
        else:
            nan_mask.append(np.ones(batch_size, dtype=int))
        
        nan_mask = torch.tensor(np.column_stack(nan_mask), dtype=torch.float32)
        
        return x_categorical, x_numerical, sequences, nan_mask


class ClickDataset(Dataset):
    def __init__(self, df, feature_processor: FeatureProcessor, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_processor = feature_processor
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        # í”¼ì²˜ ì²˜ë¦¬
        self.x_categorical, self.x_numerical, self.sequences, self.nan_mask = feature_processor.transform(df)

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        item = {
            'x_categorical': self.x_categorical[idx],
            'x_numerical': self.x_numerical[idx],
            'seq': self.sequences[idx],
            'nan_mask': self.nan_mask[idx]
        }
        
        # IDê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        
        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            item['y'] = y
        
        return item



def collate_fn_transformer_train(batch):
    """Transformer ëª¨ë¸ìš© í›ˆë ¨ collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    ys = [item['y'] for item in batch]  # has_target=Trueì¸ ê²½ìš°ë§Œ
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    ys = torch.stack(ys)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ys': ys
    }

def collate_fn_transformer_infer(batch):
    """Transformer ëª¨ë¸ìš© ì¶”ë¡  collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    
    # ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    ids = [item['id'] for item in batch]
    
    # IDì— Noneì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    result = {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ids': ids
    }
    
    return result


def create_data_loaders(train_df, val_df, test_df, feature_cols, seq_col, target_col, batch_size):
    """ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜"""
    import pandas as pd
    
    # FeatureProcessor ìƒì„± ë° í•™ìŠµ
    feature_processor = FeatureProcessor()
    if train_df is not None and len(train_df) > 0:
        feature_processor.fit(train_df)
    else:
        # ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ
        dummy_data = {col: [0.0] for col in feature_cols}
        dummy_data[seq_col] = ["0.0"]
        dummy_data[target_col] = [0.0]
        dummy_train_df = pd.DataFrame(dummy_data)
        feature_processor.fit(dummy_train_df)
    
    # Train dataset
    if train_df is not None and len(train_df) > 0:
        train_dataset = ClickDataset(train_df, feature_processor, target_col, has_target=True, has_id=False)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_transformer_train)
    else:
        dummy_data = {col: [0.0] for col in feature_cols}
        dummy_data[seq_col] = ["0.0"]
        dummy_data[target_col] = [0.0]
        dummy_train_df = pd.DataFrame(dummy_data)
        train_dataset = ClickDataset(dummy_train_df, feature_processor, target_col, has_target=True, has_id=False)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_transformer_train)
    
    # Val dataset
    if val_df is not None and len(val_df) > 0:
        val_dataset = ClickDataset(val_df, feature_processor, target_col, has_target=True, has_id=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_train)
    else:
        dummy_data = {col: [0.0] for col in feature_cols}
        dummy_data[seq_col] = ["0.0"]
        dummy_data[target_col] = [0.0]
        dummy_val_df = pd.DataFrame(dummy_data)
        val_dataset = ClickDataset(dummy_val_df, feature_processor, target_col, has_target=True, has_id=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_train)
    
    # Test dataset
    if test_df is not None:
        test_dataset = ClickDataset(test_df, feature_processor, has_target=False, has_id=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_infer)
    else:
        dummy_test_df = pd.DataFrame(columns=feature_cols + [seq_col])
        test_dataset = ClickDataset(dummy_test_df, feature_processor, has_target=False, has_id=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_infer)
    
    return train_loader, val_loader, test_loader, train_dataset, val_dataset, feature_processor

def load_and_preprocess_data(CFG):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    def safe_load_parquet(file_path):
        """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜ - í•­ìƒ ì „ì²´ ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ - {file_path}")
        try:
            return pd.read_parquet(file_path, engine="pyarrow")
        except Exception as e:
            print(f"âš ï¸  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    all_train = safe_load_parquet(CFG['PATHS']['TRAIN_DATA'])
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test = safe_load_parquet(CFG['PATHS']['TEST_DATA'])
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ID ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨ (ì˜ˆì¸¡ ì‹œ í•„ìš”)
    if 'ID' not in test.columns:
        raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test.shape[0]}ê°œ í–‰, ID ì»¬ëŸ¼ í¬í•¨")

    print("Train shape:", all_train.shape)
    print("Test shape:", test.shape)

    # feat_e_3 missing ê¸°ì¤€ìœ¼ë¡œ ìƒ˜í”Œë§
    # 1. feat_e_3ì´ missingì¸ ë°ì´í„°ëŠ” ëª¨ë‘ í¬í•¨
    missing_feat_e_3 = all_train[all_train['feat_e_3'].isna()]
    
    # 2. feat_e_3ì´ ìˆëŠ” ë°ì´í„° ì¤‘ì—ì„œ clicked == 1ì¸ ë°ì´í„°
    available_feat_e_3_clicked_1 = all_train[(all_train['feat_e_3'].notna()) & (all_train['clicked'] == 1)]
    
    # 3. feat_e_3ì´ ìˆëŠ” ë°ì´í„° ì¤‘ì—ì„œ clicked == 0ì¸ ë°ì´í„°
    available_feat_e_3_clicked_0 = all_train[(all_train['feat_e_3'].notna()) & (all_train['clicked'] == 0)]
    
    # 4. missing ë°ì´í„°ì—ì„œ clicked=1 ë°ì´í„°ë¥¼ ëº€ ë§Œí¼ë§Œ clicked=0 ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§
    target_size = len(missing_feat_e_3) - len(available_feat_e_3_clicked_1)
    if target_size > 0 and len(available_feat_e_3_clicked_0) >= target_size:
        sampled_clicked_0 = available_feat_e_3_clicked_0.sample(n=target_size, random_state=42)
    else:
        # target_sizeê°€ 0 ì´í•˜ì´ê±°ë‚˜ available ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ëª¨ë‘ ì‚¬ìš©
        sampled_clicked_0 = available_feat_e_3_clicked_0
    
    # 5. ìµœì¢… í›ˆë ¨ ë°ì´í„° êµ¬ì„±
    train = pd.concat([missing_feat_e_3, available_feat_e_3_clicked_1, sampled_clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

    print("ğŸ“Š ìƒ˜í”Œë§ ê²°ê³¼:")
    print(f"  1. feat_e_3 missing ë°ì´í„°: {len(missing_feat_e_3):,}ê°œ")
    print(f"  2. feat_e_3 available + clicked=1: {len(available_feat_e_3_clicked_1):,}ê°œ")
    print(f"  3. feat_e_3 available + clicked=0 (ìƒ˜í”Œë§): {len(sampled_clicked_0):,}ê°œ")
    print(f"  - ì´ í›ˆë ¨ ë°ì´í„°: {len(train):,}ê°œ")
    print(f"  - ìµœì¢… clicked=0: {len(train[train['clicked']==0]):,}ê°œ")
    print(f"  - ìµœì¢… clicked=1: {len(train[train['clicked']==1]):,}ê°œ")

    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"}
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ)
    if 'ID' in train.columns:
        train = train.drop(columns=['ID'])
        print("âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)

    return train, test, feature_cols, seq_col, target_col
