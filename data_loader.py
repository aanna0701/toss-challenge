import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

class ClickDataset(Dataset):
    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True):
        self.df = df.reset_index(drop=True)
        self.feature_cols = feature_cols
        self.seq_col = seq_col
        self.target_col = target_col
        self.has_target = has_target

        # ë¹„-ì‹œí€€ìŠ¤ í”¼ì²˜: ì „ë¶€ ì—°ì†ê°’ìœ¼ë¡œ
        self.X = self.df[self.feature_cols].astype(float).fillna(0).values

        # ì‹œí€€ìŠ¤: ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë³´ê´€ (lazy íŒŒì‹±)
        self.seq_strings = self.df[self.seq_col].astype(str).values

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        x = torch.tensor(self.X[idx], dtype=torch.float)

        # ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš© (ë¹ˆ ì‹œí€€ìŠ¤ë§Œ ë°©ì–´)
        s = self.seq_strings[idx]
        if s:
            arr = np.fromstring(s, sep=",", dtype=np.float32)
        else:
            arr = np.array([], dtype=np.float32)

        if arr.size == 0:
            arr = np.array([0.0], dtype=np.float32)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì–´

        seq = torch.from_numpy(arr)  # shape (seq_len,)

        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            return x, seq, y
        else:
            return x, seq

def collate_fn_train(batch):
    xs, seqs, ys = zip(*batch)
    xs = torch.stack(xs)
    ys = torch.stack(ys)
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    return xs, seqs_padded, seq_lengths, ys

def collate_fn_infer(batch):
    xs, seqs = zip(*batch)
    xs = torch.stack(xs)
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    return xs, seqs_padded, seq_lengths

def create_data_loaders(train_df, val_df, test_df, feature_cols, seq_col, target_col, batch_size):
    """ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜"""
    import pandas as pd
    
    # Train dataset (train_dfê°€ Noneì´ë©´ ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±)
    if train_df is not None and len(train_df) > 0:
        train_dataset = ClickDataset(train_df, feature_cols, seq_col, target_col, has_target=True)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train)
    else:
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (ìµœì†Œ 1ê°œ í–‰)
        dummy_data = {col: [0.0] for col in feature_cols}
        dummy_data[seq_col] = ["0.0"]
        dummy_data[target_col] = [0.0]
        dummy_train_df = pd.DataFrame(dummy_data)
        train_dataset = ClickDataset(dummy_train_df, feature_cols, seq_col, target_col, has_target=True)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_train)
    
    # Val dataset (val_dfê°€ Noneì´ë©´ ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±)
    if val_df is not None and len(val_df) > 0:
        val_dataset = ClickDataset(val_df, feature_cols, seq_col, target_col, has_target=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)
    else:
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (ìµœì†Œ 1ê°œ í–‰)
        dummy_data = {col: [0.0] for col in feature_cols}
        dummy_data[seq_col] = ["0.0"]
        dummy_data[target_col] = [0.0]
        dummy_val_df = pd.DataFrame(dummy_data)
        val_dataset = ClickDataset(dummy_val_df, feature_cols, seq_col, target_col, has_target=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)
    
    # Test dataset (test_dfê°€ Noneì´ë©´ ë”ë¯¸ ë°ì´í„°ì…‹ ìƒì„±)
    if test_df is not None:
        test_dataset = ClickDataset(test_df, feature_cols, seq_col, has_target=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_infer)
    else:
        dummy_test_df = pd.DataFrame(columns=feature_cols + [seq_col])
        test_dataset = ClickDataset(dummy_test_df, feature_cols, seq_col, has_target=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_infer)
    
    return train_loader, val_loader, test_loader, train_dataset, val_dataset

def load_and_preprocess_data(use_sampling=None, sample_size=None):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    from main import CFG
    
    # configì—ì„œ ìƒ˜í”Œë§ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ë§¤ê°œë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ config ì‚¬ìš©)
    if use_sampling is None:
        use_sampling = CFG['DATA']['USE_SAMPLING']
    if sample_size is None:
        sample_size = CFG['DATA']['SAMPLE_SIZE']
    
    def safe_load_parquet(file_path, sample_size=None):
        """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜"""
        try:
            # ì „ì²´ ë°ì´í„° ë¡œë“œ ì‹œë„
            if not use_sampling:
                return pd.read_parquet(file_path, engine="pyarrow")
            else:
                raise Exception("ìƒ˜í”Œë§ ëª¨ë“œë¡œ ì§„í–‰")
        except Exception:
            print(f"âš ï¸  {file_path} ëŒ€ìš©ëŸ‰ ë°ì´í„° - ìƒ˜í”Œë§ ì§„í–‰...")
            
            try:
                import pyarrow.parquet as pq
                parquet_file = pq.ParquetFile(file_path)
                total_rows = parquet_file.metadata.num_rows
                
                if sample_size and total_rows > sample_size:
                    print(f"ğŸ“Š {total_rows:,} í–‰ ì¤‘ {sample_size:,} í–‰ ìƒ˜í”Œë§")
                    sample_ratio = sample_size / total_rows
                    
                    chunks = []
                    for batch in parquet_file.iter_batches(batch_size=50000):
                        chunk_df = batch.to_pandas()
                        chunk_sample = chunk_df.sample(frac=sample_ratio, random_state=42)
                        chunks.append(chunk_sample)
                        
                        if sum(len(chunk) for chunk in chunks) >= sample_size:
                            break
                    
                    return pd.concat(chunks, ignore_index=True).head(sample_size)
                else:
                    return pd.read_parquet(file_path, engine="pyarrow")
                    
            except Exception as e:
                print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    all_train = safe_load_parquet(CFG['PATHS']['TRAIN_DATA'], sample_size)
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test = safe_load_parquet(CFG['PATHS']['TEST_DATA'], sample_size)
    if 'ID' in test.columns:
        test = test.drop(columns=['ID'])

    print("Train shape:", all_train.shape)
    print("Test shape:", test.shape)

    # clicked == 1 ë°ì´í„°
    clicked_1 = all_train[all_train['clicked'] == 1]

    # clicked == 0 ë°ì´í„°ì—ì„œ ë™ì¼ ê°œìˆ˜x2 ë§Œí¼ ë¬´ì‘ìœ„ ì¶”ì¶œ (ë‹¤ìš´ ìƒ˜í”Œë§)
    clicked_0 = all_train[all_train['clicked'] == 0].sample(n=len(clicked_1)*2, random_state=42)

    # ë‘ ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
    train = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

    print("Train shape:", train.shape)
    print("Train clicked:0:", train[train['clicked']==0].shape)
    print("Train clicked:1:", train[train['clicked']==1].shape)

    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"}
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)

    return train, test, feature_cols, seq_col, target_col
