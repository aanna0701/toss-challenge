import json
import os
import pickle
from typing import Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset

class FeatureProcessor:
    """í”¼ì²˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (LabelEncoder ì‚¬ìš©)"""
    
    def __init__(self, config, normalization_stats_path):
        self.config = config
        
        # Normalization stats ë¡œë“œ
        script_dir = os.path.dirname(os.path.abspath(__file__))
        norm_stats_full_path = os.path.join(script_dir, normalization_stats_path)
        with open(norm_stats_full_path, 'r', encoding='utf-8') as f:
            self.norm_stats = json.load(f)['statistics']
        
        # í”¼ì²˜ ë¶„ë¥˜
        self.categorical_features = self.config['MODEL']['FEATURES']['CATEGORICAL']
        self.sequential_feature = self.config['MODEL']['FEATURES']['SEQUENTIAL']
        self.excluded_features = self.config['MODEL']['FEATURES']['EXCLUDED']
        
        # numerical_featuresëŠ” fit ì‹œì ì— ë°ì´í„°ë¥¼ ë³´ê³  ê²°ì •
        self.numerical_features = []
        
        # LabelEncoder ì‚¬ìš©
        self.label_encoders = {}  # {feat: LabelEncoder()}
        self.categorical_cardinalities = {}
        
    def fit(self, train_df: pd.DataFrame, test_df: pd.DataFrame = None):
        """
        LabelEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© í•™ìŠµ
        train_dfì™€ test_dfë¥¼ ëª¨ë‘ ë°›ì•„ì„œ ì „ì²´ ë²”ì£¼ íŒŒì•…
        """
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ ìƒì„± (ë²”ì£¼í˜•, ì‹œí€€ìŠ¤, ID, target, ì œì™¸ í”¼ì²˜ ì œì™¸)
        exclude_cols = set(self.categorical_features + [self.sequential_feature, 'ID', 'clicked'] + self.excluded_features)
        self.numerical_features = [col for col in train_df.columns if col not in exclude_cols]
        
        print("ğŸ”§ ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© í•™ìŠµ ì¤‘...")
        # ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© ì„¤ì • (LabelEncoder ì‚¬ìš©)
        for feat in self.categorical_features:
            if feat not in train_df.columns:
                continue
            
            # trainê³¼ testì˜ ëª¨ë“  ê°’ì„ í•©ì³ì„œ fit
            if test_df is not None and feat in test_df.columns:
                all_values = pd.concat([
                    train_df[feat].astype(str).fillna("UNK"),
                    test_df[feat].astype(str).fillna("UNK")
                ], axis=0)
            else:
                all_values = train_df[feat].astype(str).fillna("UNK")
            
            le = LabelEncoder()
            le.fit(all_values)
            
            self.label_encoders[feat] = le
            self.categorical_cardinalities[feat] = len(le.classes_)
            
            print(f"   â€¢ {feat}: {len(le.classes_)} unique categories")
        
        print("âœ… ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© í•™ìŠµ ì™„ë£Œ")
        return self
    
    def transform(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ë°ì´í„° ë³€í™˜ (LabelEncoder ì‚¬ìš©)"""
        batch_size = len(df)
        
        # ë²”ì£¼í˜• í”¼ì²˜ ì²˜ë¦¬ (LabelEncoder ì‚¬ìš©)
        categorical_data = []
        for feat in self.categorical_features:
            if feat in df.columns:
                # LabelEncoderë¡œ ë³€í™˜
                encoded = self.label_encoders[feat].transform(
                    df[feat].astype(str).fillna("UNK")
                )
                categorical_data.append(encoded)
            else:
                raise ValueError(f"âŒ ë²”ì£¼í˜• í”¼ì²˜ '{feat}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if categorical_data:
            x_categorical = torch.tensor(np.column_stack(categorical_data), dtype=torch.long)
        else:
            x_categorical = torch.empty(batch_size, 0, dtype=torch.long)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì²˜ë¦¬ (í‘œì¤€í™”) - ë¬´ì¡°ê±´ ì ìš©
        numerical_data = []
        for feat in self.numerical_features:
            if feat in df.columns:
                if feat in self.norm_stats:
                    # í‘œì¤€í™”: (x - mean) / std
                    mean = self.norm_stats[feat]['mean']
                    std = self.norm_stats[feat]['std']
                    # ë°ì´í„°ë¥¼ floatë¡œ ë³€í™˜
                    feat_data = pd.to_numeric(df[feat], errors='coerce')
                    # ê²°ì¸¡ì¹˜ë¥¼ ì œì™¸í•˜ê³  í‘œì¤€í™”í•œ í›„, ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì±„ì›€
                    standardized = (feat_data - mean) / std
                    standardized = standardized.fillna(0)
                    numerical_data.append(standardized.values.astype(np.float32))
                else:
                    # norm_statsì— ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë°œìƒ
                    raise ValueError(f"âŒ {feat} í”¼ì²˜ì˜ normalization statsê°€ ì—†ìŠµë‹ˆë‹¤! config.yamlê³¼ normalization_stats.jsonì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                raise ValueError(f"âŒ {feat} í”¼ì²˜ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        if numerical_data:
            x_numerical = torch.tensor(np.column_stack(numerical_data), dtype=torch.float32)
        else:
            x_numerical = torch.empty(batch_size, 0, dtype=torch.float32)
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ ì²˜ë¦¬
        if self.sequential_feature in df.columns:
            seq_strings = df[self.sequential_feature].astype(str).values
            sequences = []
            for s in seq_strings:
                if s and s != 'nan':
                    try:
                        arr = np.fromstring(s, sep=",", dtype=np.float32)
                        if arr.size == 0:
                            arr = np.array([0.0], dtype=np.float32)
                    except:
                        arr = np.array([0.0], dtype=np.float32)
                else:
                    arr = np.array([0.0], dtype=np.float32)
                sequences.append(torch.from_numpy(arr))
        else:
            raise ValueError(f"âŒ ì‹œí€€ìŠ¤ í”¼ì²˜ '{self.sequential_feature}'ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤!")
        
        return x_categorical, x_numerical, sequences


class ClickDataset(Dataset):
    def __init__(self, df, feature_processor: FeatureProcessor, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_processor = feature_processor
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        # í”¼ì²˜ ì²˜ë¦¬
        self.x_categorical, self.x_numerical, self.sequences = feature_processor.transform(df)

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        item = {
            'x_categorical': self.x_categorical[idx],
            'x_numerical': self.x_numerical[idx],
            'seq': self.sequences[idx]
        }
        
        # IDê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        
        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            item['y'] = y
        
        return item



def collate_fn_transformer_train(batch):
    """Transformer ëª¨ë¸ìš© í›ˆë ¨ collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    ys = [item['y'] for item in batch]  # has_target=Trueì¸ ê²½ìš°ë§Œ
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    ys = torch.stack(ys)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'ys': ys
    }

def collate_fn_transformer_infer(batch):
    """Transformer ëª¨ë¸ìš© ì¶”ë¡  collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    
    # ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    ids = [item['id'] for item in batch]
    
    # IDì— Noneì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    result = {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'ids': ids
    }
    
    return result


def safe_load_parquet(file_path):
    """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜ - í•­ìƒ ì „ì²´ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ - {file_path}")
    try:
        return pd.read_parquet(file_path, engine="pyarrow")
    except Exception as e:
        print(f"âš ï¸  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def load_train_data(config):
    """í›ˆë ¨ ë°ì´í„°ë§Œ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    train = safe_load_parquet(config['PATHS']['TRAIN_DATA'])
    
    print("Train shape:", train.shape)
    
    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target/ì œì™¸ í”¼ì²˜ ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    excluded_features = config['MODEL']['FEATURES']['EXCLUDED']
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"} | set(excluded_features)
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ)
    if 'ID' in train.columns:
        train = train.drop(columns=['ID'])
        print("âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)
    print("Excluded features:", excluded_features)

    return train, feature_cols, seq_col, target_col


def load_test_data(config):
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ë¡œë“œ í•¨ìˆ˜"""
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    test = safe_load_parquet(config['PATHS']['TEST_DATA'])
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ID ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨ (ì˜ˆì¸¡ ì‹œ í•„ìš”)
    if 'ID' not in test.columns:
        raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test.shape[0]}ê°œ í–‰, ID ì»¬ëŸ¼ í¬í•¨")
    print("Test shape:", test.shape)
    
    return test


def save_feature_processor(feature_processor: FeatureProcessor, save_path: str):
    """FeatureProcessorë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    print(f"ğŸ’¾ FeatureProcessor ì €ì¥ ì¤‘...")
    print(f"   â€¢ ê²½ë¡œ: {save_path}")
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    save_dir = os.path.dirname(save_path)
    if save_dir and not os.path.exists(save_dir):
        os.makedirs(save_dir, exist_ok=True)
    
    # FeatureProcessor ì €ì¥
    with open(save_path, 'wb') as f:
        pickle.dump(feature_processor, f)
    
    print(f"âœ… FeatureProcessor ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"   â€¢ ë²”ì£¼í˜• í”¼ì²˜: {len(feature_processor.categorical_features)}ê°œ")
    print(f"   â€¢ ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(feature_processor.numerical_features)}ê°œ")
    print(f"   â€¢ ì‹œí€€ìŠ¤ ì»¬ëŸ¼: {feature_processor.sequential_feature}")


def load_feature_processor(load_path: str) -> FeatureProcessor:
    """ì €ì¥ëœ FeatureProcessorë¥¼ ë¡œë“œ"""
    print(f"ğŸ“‚ FeatureProcessor ë¡œë“œ ì¤‘...")
    print(f"   â€¢ ê²½ë¡œ: {load_path}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"âŒ FeatureProcessor íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {load_path}")
    
    # FeatureProcessor ë¡œë“œ
    with open(load_path, 'rb') as f:
        feature_processor = pickle.load(f)
    
    print(f"âœ… FeatureProcessor ë¡œë“œ ì™„ë£Œ")
    print(f"   â€¢ ë²”ì£¼í˜• í”¼ì²˜: {len(feature_processor.categorical_features)}ê°œ")
    print(f"   â€¢ ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(feature_processor.numerical_features)}ê°œ")
    print(f"   â€¢ ì‹œí€€ìŠ¤ ì»¬ëŸ¼: {feature_processor.sequential_feature}")
    
    return feature_processor
