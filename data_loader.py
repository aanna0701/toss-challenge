import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
import yaml
from typing import Dict, List, Tuple, Optional, Union

class FeatureProcessor:
    """í”¼ì²˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "config.yaml", normalization_stats_path: str = "analysis/results/normalization_stats.json"):
        # Config ë¡œë“œ
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # Normalization stats ë¡œë“œ
        with open(normalization_stats_path, 'r', encoding='utf-8') as f:
            self.norm_stats = json.load(f)['statistics']
        
        # í”¼ì²˜ ë¶„ë¥˜
        self.categorical_features = self.config['MODEL']['FEATURES']['CATEGORICAL']
        self.sequential_feature = self.config['MODEL']['FEATURES']['SEQUENTIAL']
        self.excluded_features = self.config['MODEL']['FEATURES']['EXCLUDED']
        
        # ë²”ì£¼í˜• í”¼ì²˜ì˜ ì¹´ë””ë„ë¦¬í‹° ê³„ì‚°
        self.categorical_cardinalities = {}
        self.categorical_encoders = {}
        
    def fit(self, df: pd.DataFrame):
        """ë°ì´í„°ì— ë§ì¶° ì¸ì½”ë” í•™ìŠµ"""
        # ë²”ì£¼í˜• í”¼ì²˜ ì¸ì½”ë”© ì„¤ì •
        for feat in self.categorical_features:
            if feat in df.columns:
                unique_vals = df[feat].dropna().unique()
                # 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì†ëœ ì •ìˆ˜ë¡œ ë§¤í•‘
                unique_vals_sorted = sorted(unique_vals)
                self.categorical_encoders[feat] = {val: idx for idx, val in enumerate(unique_vals_sorted)}
                self.categorical_cardinalities[feat] = len(unique_vals_sorted)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ëª©ë¡ ìƒì„± (ë²”ì£¼í˜•, ì‹œí€€ìŠ¤, ID, target, ì œì™¸ í”¼ì²˜ ì œì™¸)
        exclude_cols = set(self.categorical_features + [self.sequential_feature, 'ID', 'clicked'] + self.excluded_features)
        self.numerical_features = [col for col in df.columns if col not in exclude_cols]
        
        return self
    
    def transform(self, df: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ë°ì´í„° ë³€í™˜"""
        batch_size = len(df)
        
        # ë²”ì£¼í˜• í”¼ì²˜ ì²˜ë¦¬
        categorical_data = []
        for feat in self.categorical_features:
            if feat in df.columns:
                # ë²”ì£¼í˜• ê°’ì„ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì •ìˆ˜ë¡œ ë³€í™˜
                encoded = df[feat].map(self.categorical_encoders[feat]).fillna(0).astype(int)
                categorical_data.append(encoded.values)
            else:
                # í”¼ì²˜ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                categorical_data.append(np.zeros(batch_size, dtype=int))
        
        if categorical_data:
            x_categorical = torch.tensor(np.column_stack(categorical_data), dtype=torch.long)
        else:
            x_categorical = torch.empty(batch_size, 0, dtype=torch.long)
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì²˜ë¦¬ (í‘œì¤€í™”)
        numerical_data = []
        for feat in self.numerical_features:
            if feat in df.columns and feat in self.norm_stats:
                # í‘œì¤€í™”: (x - mean) / std
                mean = self.norm_stats[feat]['mean']
                std = self.norm_stats[feat]['std']
                # ë°ì´í„°ë¥¼ floatë¡œ ë³€í™˜ í›„ í‘œì¤€í™”
                feat_data = pd.to_numeric(df[feat], errors='coerce').fillna(0)
                standardized = (feat_data - mean) / std
                numerical_data.append(standardized.values.astype(np.float32))
            else:
                # í†µê³„ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
                numerical_data.append(np.zeros(batch_size, dtype=np.float32))
        
        if numerical_data:
            x_numerical = torch.tensor(np.column_stack(numerical_data), dtype=torch.float32)
        else:
            x_numerical = torch.empty(batch_size, 0, dtype=torch.float32)
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ ì²˜ë¦¬
        if self.sequential_feature in df.columns:
            seq_strings = df[self.sequential_feature].astype(str).values
            sequences = []
            for s in seq_strings:
                if s and s != 'nan':
                    try:
                        arr = np.fromstring(s, sep=",", dtype=np.float32)
                        if arr.size == 0:
                            arr = np.array([0.0], dtype=np.float32)
                    except:
                        arr = np.array([0.0], dtype=np.float32)
                else:
                    arr = np.array([0.0], dtype=np.float32)
                sequences.append(torch.from_numpy(arr))
        else:
            sequences = [torch.tensor([0.0], dtype=torch.float32) for _ in range(batch_size)]
        
        # NaN ë§ˆìŠ¤í¬ ìƒì„± (ë²”ì£¼í˜• + ìˆ˜ì¹˜í˜• + ì‹œí€€ìŠ¤ ìˆœì„œ)
        nan_mask = []
        
        # ë²”ì£¼í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.categorical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        for feat in self.numerical_features:
            if feat in df.columns:
                nan_mask.append(df[feat].isna().astype(int).values)
            else:
                nan_mask.append(np.ones(batch_size, dtype=int))
        
        # ì‹œí€€ìŠ¤ í”¼ì²˜ NaN ë§ˆìŠ¤í¬
        if self.sequential_feature in df.columns:
            nan_mask.append(df[self.sequential_feature].isna().astype(int).values)
        else:
            nan_mask.append(np.ones(batch_size, dtype=int))
        
        nan_mask = torch.tensor(np.column_stack(nan_mask), dtype=torch.float32)
        
        return x_categorical, x_numerical, sequences, nan_mask


class ClickDataset(Dataset):
    def __init__(self, df, feature_processor: FeatureProcessor, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_processor = feature_processor
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        # í”¼ì²˜ ì²˜ë¦¬
        self.x_categorical, self.x_numerical, self.sequences, self.nan_mask = feature_processor.transform(df)

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        item = {
            'x_categorical': self.x_categorical[idx],
            'x_numerical': self.x_numerical[idx],
            'seq': self.sequences[idx],
            'nan_mask': self.nan_mask[idx]
        }
        
        # IDê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        
        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            item['y'] = y
        
        return item


class TabularSeqDataset(Dataset):
    """ê¸°ì¡´ TabularSeq ëª¨ë¸ìš© ë°ì´í„°ì…‹ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    def __init__(self, df, feature_cols, seq_col, target_col=None, has_target=True, has_id=False):
        self.df = df.reset_index(drop=True)
        self.feature_cols = feature_cols
        self.seq_col = seq_col
        self.target_col = target_col
        self.has_target = has_target
        self.has_id = has_id

        # ë¹„-ì‹œí€€ìŠ¤ í”¼ì²˜: ì „ë¶€ ì—°ì†ê°’ìœ¼ë¡œ
        self.X = self.df[self.feature_cols].astype(float).fillna(0).values

        # ì‹œí€€ìŠ¤: ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë³´ê´€ (lazy íŒŒì‹±)
        self.seq_strings = self.df[self.seq_col].astype(str).values

        if self.has_target:
            self.y = self.df[self.target_col].astype(np.float32).values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        x = torch.tensor(self.X[idx], dtype=torch.float)

        # ì „ì²´ ì‹œí€€ìŠ¤ ì‚¬ìš© (ë¹ˆ ì‹œí€€ìŠ¤ë§Œ ë°©ì–´)
        s = self.seq_strings[idx]
        if s:
            arr = np.fromstring(s, sep=",", dtype=np.float32)
        else:
            arr = np.array([], dtype=np.float32)

        if arr.size == 0:
            arr = np.array([0.0], dtype=np.float32)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì–´

        seq = torch.from_numpy(arr)  # shape (seq_len,)

        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        item = {
            'x': x,
            'seq': seq
        }
        
        # IDê°€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
        if self.has_id:
            if 'ID' not in self.df.columns:
                raise ValueError("âŒ IDê°€ í•„ìš”í•œë° ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            item['id'] = self.df.iloc[idx]['ID']
        
        if self.has_target:
            y = torch.tensor(self.y[idx], dtype=torch.float)
            item['y'] = y
        
        return item

def collate_fn_transformer_train(batch):
    """Transformer ëª¨ë¸ìš© í›ˆë ¨ collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    ys = [item['y'] for item in batch]  # has_target=Trueì¸ ê²½ìš°ë§Œ
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    ys = torch.stack(ys)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    return {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ys': ys
    }

def collate_fn_transformer_infer(batch):
    """Transformer ëª¨ë¸ìš© ì¶”ë¡  collate í•¨ìˆ˜"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    x_categorical = [item['x_categorical'] for item in batch]
    x_numerical = [item['x_numerical'] for item in batch]
    seqs = [item['seq'] for item in batch]
    nan_masks = [item['nan_mask'] for item in batch]
    
    # ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    ids = [item['id'] for item in batch]
    
    # IDì— Noneì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ìŠ¤íƒìœ¼ë¡œ ë³€í™˜
    x_categorical = torch.stack(x_categorical)
    x_numerical = torch.stack(x_numerical)
    nan_masks = torch.stack(nan_masks)
    
    # ì‹œí€€ìŠ¤ íŒ¨ë”©
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    result = {
        'x_categorical': x_categorical,
        'x_numerical': x_numerical,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'nan_mask': nan_masks,
        'ids': ids
    }
    
    return result

def collate_fn_seq_train(batch):
    """TabularSeq ëª¨ë¸ìš© í›ˆë ¨ collate í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    xs = [item['x'] for item in batch]
    seqs = [item['seq'] for item in batch]
    ys = [item['y'] for item in batch]  # has_target=Trueì¸ ê²½ìš°ë§Œ
    
    xs = torch.stack(xs)
    ys = torch.stack(ys)
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)  # ë¹ˆ ì‹œí€€ìŠ¤ ë°©ì§€
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜ (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ ì—†ìŒ)
    return {
        'xs': xs,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'ys': ys
    }

def collate_fn_seq_infer(batch):
    """TabularSeq ëª¨ë¸ìš© ì¶”ë¡  collate í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    # ë”•ì…”ë„ˆë¦¬ ë°°ì¹˜ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
    xs = [item['x'] for item in batch]
    seqs = [item['seq'] for item in batch]
    
    # ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”
    if 'id' not in batch[0]:
        raise ValueError("âŒ ì˜ˆì¸¡ ì‹œì—ëŠ” IDê°€ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    ids = [item['id'] for item in batch]
    
    # IDì— Noneì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if any(id_val is None for id_val in ids):
        raise ValueError("âŒ ID ê°’ì— Noneì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ 'ID' ì»¬ëŸ¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    xs = torch.stack(xs)
    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)
    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)
    seq_lengths = torch.clamp(seq_lengths, min=1)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°°ì¹˜ ë°˜í™˜
    result = {
        'xs': xs,
        'seqs': seqs_padded,
        'seq_lengths': seq_lengths,
        'ids': ids
    }
    
    return result

def create_data_loaders(train_df, val_df, test_df, feature_cols, seq_col, target_col, batch_size, model_type="tabular_transformer"):
    """ë°ì´í„°ë¡œë” ìƒì„± í•¨ìˆ˜"""
    import pandas as pd
    
    if model_type == "tabular_transformer":
        # Transformer ëª¨ë¸ìš© ë°ì´í„° ë¡œë”
        # FeatureProcessor ìƒì„± ë° í•™ìŠµ
        feature_processor = FeatureProcessor()
        if train_df is not None and len(train_df) > 0:
            feature_processor.fit(train_df)
        else:
            # ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ
            dummy_data = {col: [0.0] for col in feature_cols}
            dummy_data[seq_col] = ["0.0"]
            dummy_data[target_col] = [0.0]
            dummy_train_df = pd.DataFrame(dummy_data)
            feature_processor.fit(dummy_train_df)
        
        # Train dataset
        if train_df is not None and len(train_df) > 0:
            train_dataset = ClickDataset(train_df, feature_processor, target_col, has_target=True, has_id=False)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_transformer_train)
        else:
            dummy_data = {col: [0.0] for col in feature_cols}
            dummy_data[seq_col] = ["0.0"]
            dummy_data[target_col] = [0.0]
            dummy_train_df = pd.DataFrame(dummy_data)
            train_dataset = ClickDataset(dummy_train_df, feature_processor, target_col, has_target=True, has_id=False)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_transformer_train)
        
        # Val dataset
        if val_df is not None and len(val_df) > 0:
            val_dataset = ClickDataset(val_df, feature_processor, target_col, has_target=True, has_id=False)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_train)
        else:
            dummy_data = {col: [0.0] for col in feature_cols}
            dummy_data[seq_col] = ["0.0"]
            dummy_data[target_col] = [0.0]
            dummy_val_df = pd.DataFrame(dummy_data)
            val_dataset = ClickDataset(dummy_val_df, feature_processor, target_col, has_target=True, has_id=False)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_train)
        
        # Test dataset
        if test_df is not None:
            test_dataset = ClickDataset(test_df, feature_processor, has_target=False, has_id=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_infer)
        else:
            dummy_test_df = pd.DataFrame(columns=feature_cols + [seq_col])
            test_dataset = ClickDataset(dummy_test_df, feature_processor, has_target=False, has_id=False)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_transformer_infer)
        
        return train_loader, val_loader, test_loader, train_dataset, val_dataset, feature_processor
    
    else:
        # TabularSeq ëª¨ë¸ìš© ë°ì´í„° ë¡œë” (í•˜ìœ„ í˜¸í™˜ì„±)
        # Train dataset
        if train_df is not None and len(train_df) > 0:
            train_dataset = TabularSeqDataset(train_df, feature_cols, seq_col, target_col, has_target=True, has_id=False)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_seq_train)
        else:
            dummy_data = {col: [0.0] for col in feature_cols}
            dummy_data[seq_col] = ["0.0"]
            dummy_data[target_col] = [0.0]
            dummy_train_df = pd.DataFrame(dummy_data)
            train_dataset = TabularSeqDataset(dummy_train_df, feature_cols, seq_col, target_col, has_target=True, has_id=False)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_seq_train)
        
        # Val dataset
        if val_df is not None and len(val_df) > 0:
            val_dataset = TabularSeqDataset(val_df, feature_cols, seq_col, target_col, has_target=True, has_id=False)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_seq_train)
        else:
            dummy_data = {col: [0.0] for col in feature_cols}
            dummy_data[seq_col] = ["0.0"]
            dummy_data[target_col] = [0.0]
            dummy_val_df = pd.DataFrame(dummy_data)
            val_dataset = TabularSeqDataset(dummy_val_df, feature_cols, seq_col, target_col, has_target=True, has_id=False)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_seq_train)
        
        # Test dataset
        if test_df is not None:
            test_dataset = TabularSeqDataset(test_df, feature_cols, seq_col, has_target=False, has_id=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_seq_infer)
        else:
            dummy_test_df = pd.DataFrame(columns=feature_cols + [seq_col])
            test_dataset = TabularSeqDataset(dummy_test_df, feature_cols, seq_col, has_target=False, has_id=False)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_seq_infer)
        
        return train_loader, val_loader, test_loader, train_dataset, val_dataset

def load_and_preprocess_data(use_sampling=None, sample_size=None):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    from main import CFG
    
    # configì—ì„œ ìƒ˜í”Œë§ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ë§¤ê°œë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ config ì‚¬ìš©)
    if use_sampling is None:
        use_sampling = CFG['DATA']['USE_SAMPLING']
    if sample_size is None:
        sample_size = CFG['DATA']['SAMPLE_SIZE']
    
    def safe_load_parquet(file_path, sample_size=None, force_full_load=False):
        """ì•ˆì „í•œ parquet ë¡œë“œ í•¨ìˆ˜"""
        # force_full_loadê°€ Trueì´ê±°ë‚˜ use_samplingì´ Falseì´ë©´ ì „ì²´ ë°ì´í„° ë¡œë“œ
        if force_full_load or not use_sampling:
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ëª¨ë“œ - {file_path}")
            try:
                return pd.read_parquet(file_path, engine="pyarrow")
            except Exception as e:
                print(f"âš ï¸  ì „ì²´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒ˜í”Œë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                raise
        else:
            print(f"ğŸ“Š ìƒ˜í”Œë§ ëª¨ë“œ í™œì„±í™” - {file_path}")
            try:
                import pyarrow.parquet as pq
                parquet_file = pq.ParquetFile(file_path)
                total_rows = parquet_file.metadata.num_rows
                
                if sample_size and total_rows > sample_size:
                    print(f"ğŸ“Š {total_rows:,} í–‰ ì¤‘ {sample_size:,} í–‰ ìƒ˜í”Œë§")
                    sample_ratio = sample_size / total_rows
                    
                    chunks = []
                    for batch in parquet_file.iter_batches(batch_size=50000):
                        chunk_df = batch.to_pandas()
                        chunk_sample = chunk_df.sample(frac=sample_ratio, random_state=42)
                        chunks.append(chunk_sample)
                        
                        if sum(len(chunk) for chunk in chunks) >= sample_size:
                            break
                    
                    return pd.concat(chunks, ignore_index=True).head(sample_size)
                else:
                    return pd.read_parquet(file_path, engine="pyarrow")
            except Exception as e:
                print(f"âš ï¸  ìƒ˜í”Œë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("ì „ì²´ ë°ì´í„° ë¡œë“œë¡œ ëŒ€ì²´...")
                return pd.read_parquet(file_path, engine="pyarrow")
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘...")
    all_train = safe_load_parquet(CFG['PATHS']['TRAIN_DATA'], sample_size)
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¬´ì¡°ê±´ ì „ì²´ ë¡œë“œ
    test = safe_load_parquet(CFG['PATHS']['TEST_DATA'], sample_size, force_full_load=True)
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ID ì»¬ëŸ¼ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨ (ì˜ˆì¸¡ ì‹œ í•„ìš”)
    if 'ID' not in test.columns:
        raise ValueError("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— 'ID' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤! ì˜ˆì¸¡ì„ ìœ„í•´ì„œëŠ” IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {test.shape[0]}ê°œ í–‰, ID ì»¬ëŸ¼ í¬í•¨")

    print("Train shape:", all_train.shape)
    print("Test shape:", test.shape)

    # clicked == 1 ë°ì´í„°
    clicked_1 = all_train[all_train['clicked'] == 1]

    # clicked == 0 ë°ì´í„°ì—ì„œ ë™ì¼ ê°œìˆ˜x2 ë§Œí¼ ë¬´ì‘ìœ„ ì¶”ì¶œ (ë‹¤ìš´ ìƒ˜í”Œë§)
    clicked_0 = all_train[all_train['clicked'] == 0].sample(n=len(clicked_1)*2, random_state=42)

    # ë‘ ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
    train = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)

    print("Train shape:", train.shape)
    print("Train clicked:0:", train[train['clicked']==0].shape)
    print("Train clicked:1:", train[train['clicked']==1].shape)

    # Target / Sequence
    target_col = "clicked"
    seq_col = "seq"

    # í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜: ID/seq/target ì œì™¸, ë‚˜ë¨¸ì§€ ì „ë¶€
    FEATURE_EXCLUDE = {target_col, seq_col, "ID"}
    feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]

    # í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° (í›ˆë ¨ ì‹œì—ëŠ” IDê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ)
    if 'ID' in train.columns:
        train = train.drop(columns=['ID'])
        print("âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ID ì»¬ëŸ¼ ì œê±° ì™„ë£Œ")

    print("Num features:", len(feature_cols))
    print("Sequence:", seq_col)
    print("Target:", target_col)

    return train, test, feature_cols, seq_col, target_col
