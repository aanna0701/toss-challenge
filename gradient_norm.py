"""
Gradient Norm ì¸¡ì • ë° ë¡œê¹… ê¸°ëŠ¥
"""

import torch
import pandas as pd
import numpy as np
from typing import Dict, List, Optional


def calculate_gradient_norms(model, components: List[str]) -> Dict[str, float]:
    """
    ëª¨ë¸ êµ¬ì„± ìš”ì†Œë³„ gradient norm ê³„ì‚°
    
    Args:
        model: PyTorch ëª¨ë¸
        components: ì¸¡ì •í•  êµ¬ì„± ìš”ì†Œ ë¦¬ìŠ¤íŠ¸ ['lstm', 'mlp', 'total']
    
    Returns:
        Dict[str, float]: êµ¬ì„± ìš”ì†Œë³„ gradient norm
    """
    gradient_norms = {}
    
    # ì „ì²´ ëª¨ë¸ gradient norm
    if 'total' in components:
        total_norm = 0.0
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        gradient_norms['total'] = total_norm ** 0.5
    
    # LSTM ë¶€ë¶„ gradient norm
    if 'lstm' in components:
        lstm_norm = 0.0
        lstm_params = 0
        for name, param in model.named_parameters():
            if 'lstm' in name.lower() and param.grad is not None:
                param_norm = param.grad.data.norm(2)
                lstm_norm += param_norm.item() ** 2
                lstm_params += 1
        gradient_norms['lstm'] = lstm_norm ** 0.5 if lstm_params > 0 else 0.0
    
    # ëª¨ë¸ ë¶€ë¶„ gradient norm (MLP/Transformer ë“±)
    if 'model' in components:
        model_norm = 0.0
        model_params = 0
        for name, param in model.named_parameters():
            if ('mlp' in name.lower() or 
                'transformer' in name.lower() or 
                ('linear' in name.lower() and 'lstm' not in name.lower()) or
                'attention' in name.lower() or
                'feedforward' in name.lower()) and param.grad is not None:
                param_norm = param.grad.data.norm(2)
                model_norm += param_norm.item() ** 2
                model_params += 1
        gradient_norms['model'] = model_norm ** 0.5 if model_params > 0 else 0.0
    
    return gradient_norms


def print_gradient_norms(gradient_norms: Dict[str, float], prefix: str = ""):
    """
    Gradient norm ê²°ê³¼ ì¶œë ¥
    
    Args:
        gradient_norms: gradient norm ë”•ì…”ë„ˆë¦¬
        prefix: ì¶œë ¥ ì ‘ë‘ì‚¬
    """
    print(f"{prefix}Gradient Norms:")
    for component, norm in gradient_norms.items():
        print(f"   â€¢ {component.upper()}: {norm:.6f}")


def save_gradient_norm_logs(logs: List[Dict], filepath: str):
    """
    Gradient norm ë¡œê·¸ë¥¼ CSV íŒŒì¼ë¡œ ì €ìž¥
    
    Args:
        logs: gradient norm ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
        filepath: ì €ìž¥í•  íŒŒì¼ ê²½ë¡œ
    """
    if not logs:
        print("âš ï¸ ì €ìž¥í•  gradient norm ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(logs)
    df.to_csv(filepath, index=False)
    print(f"ðŸ“Š Gradient norm ë¡œê·¸ ì €ìž¥: {filepath}")
    print(f"   â€¢ ì´ {len(logs)}ê°œ ê¸°ë¡")
    
    # ê° êµ¬ì„± ìš”ì†Œë³„ í†µê³„ ì¶œë ¥
    for component in ['total', 'lstm', 'mlp']:
        if component in df.columns:
            col_name = f'{component}_grad_norm'
            if col_name in df.columns:
                print(f"   â€¢ {component.upper()} í‰ê· : {df[col_name].mean():.6f}")
                print(f"   â€¢ {component.upper()} ìµœëŒ€: {df[col_name].max():.6f}")
                print(f"   â€¢ {component.upper()} ìµœì†Œ: {df[col_name].min():.6f}")


def analyze_gradient_behavior(logs: List[Dict]) -> Dict[str, any]:
    """
    Gradient norm í–‰ë™ ë¶„ì„
    
    Args:
        logs: gradient norm ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
    """
    if not logs:
        return {}
    
    df = pd.DataFrame(logs)
    analysis = {}
    
    for component in ['total', 'lstm', 'mlp']:
        col_name = f'{component}_grad_norm'
        if col_name in df.columns:
            norms = df[col_name]
            analysis[component] = {
                'mean': float(norms.mean()),
                'std': float(norms.std()),
                'max': float(norms.max()),
                'min': float(norms.min()),
                'trend': 'increasing' if norms.iloc[-1] > norms.iloc[0] else 'decreasing',
                'stability': float(norms.std() / norms.mean()) if norms.mean() > 0 else 0.0
            }
    
    return analysis


def print_gradient_analysis(analysis: Dict[str, any]):
    """
    Gradient ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    
    Args:
        analysis: analyze_gradient_behavior ê²°ê³¼
    """
    print("ðŸ” Gradient Norm ë¶„ì„:")
    for component, stats in analysis.items():
        print(f"   â€¢ {component.upper()}:")
        print(f"     - í‰ê· : {stats['mean']:.6f}")
        print(f"     - í‘œì¤€íŽ¸ì°¨: {stats['std']:.6f}")
        print(f"     - ìµœëŒ€: {stats['max']:.6f}")
        print(f"     - ìµœì†Œ: {stats['min']:.6f}")
        print(f"     - íŠ¸ë Œë“œ: {stats['trend']}")
        print(f"     - ì•ˆì •ì„±: {stats['stability']:.6f}")


def check_gradient_issues(gradient_norms: Dict[str, float], 
                         max_norm: float = 10.0, 
                         min_norm: float = 1e-6) -> List[str]:
    """
    Gradient ë¬¸ì œ ì²´í¬
    
    Args:
        gradient_norms: gradient norm ë”•ì…”ë„ˆë¦¬
        max_norm: ìµœëŒ€ í—ˆìš© norm (gradient explosion ì²´í¬)
        min_norm: ìµœì†Œ í—ˆìš© norm (gradient vanishing ì²´í¬)
    
    Returns:
        List[str]: ë°œê²¬ëœ ë¬¸ì œë“¤
    """
    issues = []
    
    for component, norm in gradient_norms.items():
        if norm > max_norm:
            issues.append(f"{component.upper()} gradient explosion (norm: {norm:.6f})")
        elif norm < min_norm:
            issues.append(f"{component.upper()} gradient vanishing (norm: {norm:.6f})")
    
    return issues


def print_gradient_issues(issues: List[str]):
    """
    Gradient ë¬¸ì œ ì¶œë ¥
    
    Args:
        issues: check_gradient_issues ê²°ê³¼
    """
    if issues:
        print("âš ï¸ Gradient ë¬¸ì œ ë°œê²¬:")
        for issue in issues:
            print(f"   â€¢ {issue}")
    else:
        print("âœ… Gradient ìƒíƒœ ì •ìƒ")
