#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸

Pipeline:
1. train.parquetë¥¼ train_t(80%), train_v(10%), train_c(10%)ë¡œ ë¶„í• 
2. train_tì—ì„œ 10% stratified samplingí•˜ì—¬ train_hpo ìƒì„±
3. âš ï¸  train_të¡œë§Œ NVTabular workflow fit (Data Leakage ë°©ì§€!)
4. ë‚˜ë¨¸ì§€(val/cal/hpo/test)ëŠ” train_t í†µê³„ë¡œ transformë§Œ ìˆ˜í–‰
5. l_feat_20, l_feat_23 ì œê±° (ìƒìˆ˜ í”¼ì²˜), seqëŠ” ìœ ì§€
6. ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥:
   - data/proc_train_t, data/proc_train_v, data/proc_train_c, 
   - data/proc_train_hpo, data/proc_test
7. ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬ (data/tmp ì‚­ì œ)

ì œì™¸ ì»¬ëŸ¼:
- l_feat_20, l_feat_23: ìƒìˆ˜ í”¼ì²˜ (ì •ë³´ ì—†ìŒ)

ìœ ì§€ ì»¬ëŸ¼:
- seq: ì‹œí€€ìŠ¤ ë°ì´í„° (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)

Features:
- 3 categorical features (gender, age_group, inventory_id)
  â†’ Categorify ì ìš© (ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, ì¸ë±ìŠ¤ ì¸ì½”ë”©)
- 112 continuous features (l_feat_14 í¬í•¨)
  â†’ Normalize(mean=0, std=1) + FillMissing(0) ì ìš© (ìˆœì„œ ì¤‘ìš”!)
- seq: string (LSTM ì…ë ¥)
- Total: 3 categorical + 112 continuous + seq + clicked (target)

ì „ì²˜ë¦¬ ìƒì„¸:
- Categorical: Categorify (ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, 0-based ì¸ë±ìŠ¤ ì¸ì½”ë”©)
- Continuous: 
  1. Normalize: Standardization (mean=0, std=1) ë¨¼ì € ìˆ˜í–‰
     - ê²°ì¸¡ì¹˜ ì—†ëŠ” ì‹¤ì œ ë°ì´í„°ë¡œ mean/std ê³„ì‚°
     - âš ï¸  train_të¡œë§Œ í†µê³„ ê³„ì‚° (Data Leakage ë°©ì§€!)
     - ëª¨ë“  splitì— train_t í†µê³„ ì ìš©
  2. FillMissing(0): í‘œì¤€í™” í›„ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ëŒ€ì²´
     - í‘œì¤€í™” ê³µê°„ì—ì„œ 0 = ì›ë˜ í‰ê· ê°’
     - ì‹¤ì§ˆì ìœ¼ë¡œ í‰ê· ê°’ imputation íš¨ê³¼
- seq: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë§Œ ì ìš© (ë¹ˆ ë¬¸ìì—´/NaN â†’ '0.0')

ìƒì„± ë””ë ‰í† ë¦¬:
- data/proc_train_t/    (80%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_v/    (10%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_c/    (10%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_hpo/  (~10%, HPOìš© ìƒ˜í”Œ, seq í¬í•¨)
- data/proc_test/       (test, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/tmp/             (ì„ì‹œ, ìë™ ì‚­ì œ)

ì‚¬ìš© ë°©ë²•:
- GBDT: ë¡œë”ì—ì„œ seq ì œê±° í›„ ì‚¬ìš©
- DNN: seq í¬í•¨ ê·¸ëŒ€ë¡œ ì‚¬ìš©

ì¥ì :
- âœ… ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° 1ë²Œë¡œ GBDT/DNN ëª¨ë‘ ì‚¬ìš©
- âœ… Data Leakage ë°©ì§€ (train_të¡œë§Œ í†µê³„ ê³„ì‚°)
- âœ… ëª¨ë“  splitì´ train_t í†µê³„ë¡œ ì¼ê´€ì„± ìœ ì§€
- âœ… Testë„ ë¯¸ë¦¬ ì „ì²˜ë¦¬ë˜ì–´ prediction ë¹ ë¦„
- âœ… HPOìš© ì‘ì€ datasetìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜
- âœ… Stratified samplingìœ¼ë¡œ ë¶„í¬ ìœ ì§€
- âœ… ìƒìˆ˜ í”¼ì²˜ ìë™ ì œê±°
- âœ… ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬
- âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° Standardization ìë™ ì ìš©
- âœ… ë””ìŠ¤í¬ ì ˆì•½ (ì¤‘ë³µ ë°ì´í„° ì—†ìŒ)
"""

import os
import gc
import shutil
import argparse
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split

def fill_missing_seq(df, seq_col='seq', fill_value='0.0'):
    """
    seq ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    
    seqëŠ” "1.0,2.0,3.0" í˜•íƒœì˜ ë¬¸ìì—´
    ë¹ˆ ë¬¸ìì—´, NaN, None ë“±ì„ fill_valueë¡œ ëŒ€ì²´
    """
    print(f"\n  ğŸ”§ seq ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    def clean_seq(seq_str):
        if seq_str is None or str(seq_str).strip() == '' or str(seq_str) == 'nan':
            return fill_value
        return str(seq_str)
    
    missing_count = df[seq_col].isna().sum() + (df[seq_col] == '').sum()
    df[seq_col] = df[seq_col].apply(clean_seq)
    
    print(f"  âœ… seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ê²°ì¸¡ì¹˜ {missing_count}ê°œ â†’ '{fill_value}'ë¡œ ëŒ€ì²´)")
    
    return df


def create_workflow_common():
    """Create NVTabular workflow (seq ì œì™¸ - cudf string limit íšŒí”¼)"""
    import nvtabular as nvt
    from nvtabular import ops
    
    print("\nğŸ”§ Creating common workflow (seq ì œì™¸)...")

    # CONTINUOUS COLUMNS (112 total, l_feat_20, l_feat_23 ì œì™¸)
    # l_feat_14ëŠ” continuous (float ê°’ ì¡´ì¬ í™•ì¸ë¨)
    all_continuous = (
        [f'feat_a_{i}' for i in range(1, 19)] +   # 18
        [f'feat_b_{i}' for i in range(1, 7)] +    # 6
        [f'feat_c_{i}' for i in range(1, 9)] +    # 8
        [f'feat_d_{i}' for i in range(1, 7)] +    # 6
        [f'feat_e_{i}' for i in range(1, 11)] +   # 10
        [f'history_a_{i}' for i in range(1, 8)] +   # 7
        [f'history_b_{i}' for i in range(1, 31)] +  # 30
        [f'l_feat_{i}' for i in range(1, 28) if i not in [20, 23]]  # 25 (l_feat_14 í¬í•¨!)
    )
    
    # CATEGORICAL COLUMNS (Categorify ì ìš© - ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, ì¸ë±ìŠ¤ ì¸ì½”ë”©)
    # l_feat_14ëŠ” ì œì™¸ (float ê°’ ì¡´ì¬í•˜ì—¬ continuousë¡œ ì²˜ë¦¬)
    categorical_cols = ['gender', 'age_group', 'inventory_id']
    
    print(f"   Categorical: {len(categorical_cols)} columns")
    print(f"   Continuous: {len(all_continuous)} columns")
    print(f"   âš ï¸  seq: workflow ì œì™¸ (cudf string limit íšŒí”¼, ë‚˜ì¤‘ì— ë³‘í•©)")

    # Preprocessing pipeline:
    # 1. Categorical: Categorify ì ìš© (ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, ì¸ë±ìŠ¤ ì¸ì½”ë”©)
    cat_features = categorical_cols >> ops.Categorify(
        freq_threshold=0,
        max_size=50000
    )
    
    # 2. Normalize + FillMissing for continuous features
    cont_features = all_continuous >> ops.Normalize() >> ops.FillMissing(fill_val=0)

    # 3. _row_idëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ìˆœì„œ ë³´ì¥ìš©)
    workflow = nvt.Workflow(cat_features + cont_features + ['clicked', '_row_id'])

    print("   âœ… Workflow created (seq ì œì™¸):")
    print("      - Categorical: Categorify (ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, ì¸ë±ìŠ¤ ì¸ì½”ë”©)")
    print("      - Continuous: Normalize + FillMissing(0)")
    print("      - _row_id: passthrough (ìˆœì„œ ë³´ì¥)")
    print("      - seq: transform í›„ ì›ë³¸ ë³‘í•©")
    return workflow


def process_all_data(train_ratio=0.8, val_ratio=0.1, cal_ratio=0.1, 
                     hpo_ratio=0.1, random_state=42):
    """
    ì „ì²´ ë°ì´í„° ë¶„í•  ë° ì „ì²˜ë¦¬
    
    Args:
        train_ratio: Training ë°ì´í„° ë¹„ìœ¨ (0.8 = 80%)
        val_ratio: Validation ë°ì´í„° ë¹„ìœ¨ (0.1 = 10%)
        cal_ratio: Calibration ë°ì´í„° ë¹„ìœ¨ (0.1 = 10%)
        hpo_ratio: HPOìš© ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = trainì˜ 10%)
        random_state: ëœë¤ ì‹œë“œ
    """
    print("=" * 80)
    print("ğŸš€ ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“… ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ë¶„í•  ë¹„ìœ¨: train={train_ratio}, val={val_ratio}, cal={cal_ratio}")
    print(f"ğŸ”¬ HPO ìƒ˜í”Œë§ ë¹„ìœ¨: {hpo_ratio} (from train)")
    
    # ë¹„ìœ¨ ê²€ì¦
    assert abs(train_ratio + val_ratio + cal_ratio - 1.0) < 1e-6, "ë¹„ìœ¨ì˜ í•©ì´ 1ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤"
    
    # ì œì™¸í•  ì»¬ëŸ¼ ëª©ë¡ (ìƒìˆ˜ í”¼ì²˜ë§Œ ì œì™¸, seqëŠ” ìœ ì§€)
    EXCLUDE_COLS = ['l_feat_20', 'l_feat_23', '']
    print(f"ğŸ—‘ï¸  ì œì™¸ ì»¬ëŸ¼: {', '.join([c for c in EXCLUDE_COLS if c])}")
    print(f"âœ… seq ìœ ì§€ (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„°, seq í¬í•¨, continuous standardization)
    output_dirs = {
        # ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° (seq í¬í•¨, continuous standardization ì ìš©)
        # GBDT: ë¡œë”ì—ì„œ seq ì œê±° í›„ ì‚¬ìš©
        # DNN: seq í¬í•¨ ê·¸ëŒ€ë¡œ ì‚¬ìš© (categoricalì€ ìì²´ LabelEncoder)
        'train_t': 'data/proc_train_t',
        'train_v': 'data/proc_train_v',
        'train_c': 'data/proc_train_c',
        'train_hpo': 'data/proc_train_hpo',
        'test': 'data/proc_test',
        'temp': 'data/tmp'
    }
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ í™•ì¸
    existing = [d for d in output_dirs.values() if os.path.exists(d)]
    if existing:
        print("\nâš ï¸  ê¸°ì¡´ ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤:")
        for d in existing:
            print(f"  - {d}")
        response = input("\nğŸ”„ ê¸°ì¡´ ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            print("âŒ ì‘ì—…ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return
        print("\nğŸ—‘ï¸  ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘...")
        for d in existing:
            if os.path.exists(d):
                shutil.rmtree(d)
                print(f"  âœ… ì‚­ì œ: {d}")
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dirs['temp'], exist_ok=True)
    print(f"\nğŸ“ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dirs['temp']}")
    
    # =================================================================
    # Step 1: ì „ì²´ train.parquet ë¡œë“œ ë° ë¶„í• 
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ“‚ Step 1: ì „ì²´ train.parquet ë¡œë“œ ë° ë¶„í• ")
    print("=" * 80)
    
    train_path = 'data/train.parquet'
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"{train_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    print(f"\nğŸ“– ë¡œë”©: {train_path}")
    df_full = pd.read_parquet(train_path)
    total_rows = len(df_full)
    print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {total_rows:,} rows")
    print(f"  ğŸ“Š Positive ratio: {df_full['clicked'].mean():.6f}")
    
    # ë°ì´í„° ì„ê¸°
    print("\nğŸ”€ ë°ì´í„° ì…”í”Œë§...")
    df_full = df_full.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    # ë¶„í•  í¬ê¸° ê³„ì‚°
    train_size = int(total_rows * train_ratio)
    val_size = int(total_rows * val_ratio)
    cal_size = total_rows - train_size - val_size
    
    print(f"\nâœ‚ï¸  ë¶„í•  í¬ê¸°:")
    print(f"  - train_t: {train_size:,} ({train_size/total_rows:.1%})")
    print(f"  - train_v: {val_size:,} ({val_size/total_rows:.1%})")
    print(f"  - train_c: {cal_size:,} ({cal_size/total_rows:.1%})")
    
    # ë¶„í• 
    df_train_t = df_full.iloc[:train_size].copy()
    df_train_v = df_full.iloc[train_size:train_size+val_size].copy()
    df_train_c = df_full.iloc[train_size+val_size:].copy()
    
    # ë¶„í¬ í™•ì¸
    print(f"\n  ğŸ“Š ë¶„í•  í›„ ë¶„í¬:")
    print(f"    train_t positive ratio: {df_train_t['clicked'].mean():.6f}")
    print(f"    train_v positive ratio: {df_train_v['clicked'].mean():.6f}")
    print(f"    train_c positive ratio: {df_train_c['clicked'].mean():.6f}")
    
    # train_hpo ìƒì„± (train_tì—ì„œ stratified sampling)
    print(f"\nğŸ”¬ train_hpo ìƒì„± (train_tì˜ {hpo_ratio:.1%} stratified sampling)...")
    _, df_train_hpo = train_test_split(
        df_train_t,
        test_size=hpo_ratio,
        random_state=random_state,
        stratify=df_train_t['clicked']
    )
    df_train_hpo = df_train_hpo.reset_index(drop=True)
    print(f"  âœ… train_hpo: {len(df_train_hpo):,} rows")
    print(f"  ğŸ“Š train_hpo positive ratio: {df_train_hpo['clicked'].mean():.6f}")
    
    # =================================================================
    # Step 2: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° ì„ì‹œ ì €ì¥ (seqëŠ” ìœ ì§€!)
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ—‘ï¸  Step 2: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° ì„ì‹œ ì €ì¥")
    print("=" * 80)
    print("   ì œì™¸: l_feat_20, l_feat_23 (ìƒìˆ˜ í”¼ì²˜)")
    print("   ìœ ì§€: seq (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    
    splits = {
        'train_t': df_train_t,  # workflow fitìš© (í†µê³„ ê³„ì‚°)
        'train_v': df_train_v,
        'train_c': df_train_c,
        'train_hpo': df_train_hpo
    }
    
    temp_files = {}
    seq_files = {}  # seqë¥¼ ë³„ë„ë¡œ ì €ì¥
    
    for name, df in splits.items():
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        cols_to_keep = [c for c in df.columns if c not in EXCLUDE_COLS]
        df_clean = df[cols_to_keep].copy()
        
        # âœ… ìˆœì„œ ë³´ì¥ì„ ìœ„í•´ ì„ì‹œ row_id ì¶”ê°€
        df_clean['_row_id'] = range(len(df_clean))
        
        # seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if 'seq' in df_clean.columns:
            df_clean = fill_missing_seq(df_clean, seq_col='seq', fill_value='0.0')
            
            # seqë¥¼ row_idì™€ í•¨ê»˜ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
            seq_path = os.path.join(output_dirs['temp'], f'{name}_seq.parquet')
            df_clean[['_row_id', 'seq']].to_parquet(seq_path, index=False)
            seq_files[name] = seq_path
            
            # workflowìš© ë°ì´í„°ì—ì„œ seq ì œê±° (row_idëŠ” ìœ ì§€)
            df_no_seq = df_clean.drop('seq', axis=1)
        else:
            df_no_seq = df_clean
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (seq ì œì™¸, row_id í¬í•¨)
        temp_path = os.path.join(output_dirs['temp'], f'{name}.parquet')
        df_no_seq.to_parquet(temp_path, index=False)
        temp_files[name] = temp_path
        print(f"  âœ… {name}: {len(df_no_seq):,} rows, {len(df_no_seq.columns)} cols (seq ë³„ë„ ì €ì¥, row_id ì¶”ê°€)")
        
        del df_clean, df_no_seq
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ (df_fullì€ ì´ë¯¸ ì‚¬ìš© ì™„ë£Œ)
    del df_full, df_train_t, df_train_v, df_train_c, df_train_hpo, splits
    gc.collect()
    print(f"\n  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    # =================================================================
    # Step 3: NVTabular Workflow Fit (train_të§Œ ì‚¬ìš©)
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ”§ Step 3: NVTabular Workflow Fit (train_t ONLY - Data Leakage ë°©ì§€)")
    print("=" * 80)
    print("   âœ… train_të¡œë§Œ í†µê³„ ê³„ì‚° (val/calì€ transformë§Œ)")
    print("   âœ… seq í¬í•¨, continuousë§Œ standardization")
    print("   âœ… categoricalì€ raw ìœ ì§€ (DNN ìì²´ LabelEncoder ì‚¬ìš©)")
    
    # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì´ˆê¸°í™”
    try:
        import cupy as cp
        import rmm
        
        # RMM ë©”ëª¨ë¦¬ í’€ ì„¤ì • (ë” í° í’€)
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=8 * 1024**3,  # 8GB
            managed_memory=True,
        )
        cp.cuda.Device(0).use()
        print("âœ… GPU í™œì„±í™” (RMM pool: 8GB)")
    except Exception as e:
        print(f"âš ï¸  GPU ì´ˆê¸°í™” ê²½ê³ : {e}")
    
    from merlin.io import Dataset
    
    # train_të¡œë§Œ workflow fit (data leakage ë°©ì§€)
    print(f"\nğŸ“Š Workflow fitting on train_t ONLY ({train_size:,} rows)...")
    print("   âš¡ Part size: 128MB (ë©”ëª¨ë¦¬ íš¨ìœ¨)")
    print("   âš ï¸  ì¤‘ìš”: train_të¡œë§Œ í†µê³„ ê³„ì‚° (val/cal/test ì •ë³´ ëˆ„ì¶œ ë°©ì§€)")
    
    train_dataset = Dataset(temp_files['train_t'], engine='parquet', part_size='128MB')
    
    workflow = create_workflow_common()
    
    # Fit with memory cleanup
    print("   ğŸ”§ Fitting workflow on train_t (seq ì œì™¸)...")
    workflow.fit(train_dataset)
    print("  âœ… Workflow fitted on train_t only (val/cal/testëŠ” transformë§Œ)")
    
    # WorkflowëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ìœ ì§€ (ì €ì¥ ì•ˆ í•¨)
    
    del train_dataset
    gc.collect()
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        cp.get_default_memory_pool().free_all_blocks()
        cp.get_default_pinned_memory_pool().free_all_blocks()
        print("  âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    # =================================================================
    # Step 4: Transform ê° split + seq ë³‘í•©
    # =================================================================
    print("\n" + "=" * 80)
    print("âš™ï¸  Step 4: Transform ê° split + seq ë³‘í•©")
    print("=" * 80)
    
    # Transform ìˆœì„œ: train_t, train_v, train_c, train_hpo
    splits_to_transform = ['train_t', 'train_v', 'train_c', 'train_hpo']
    
    for split_name in splits_to_transform:
        print(f"\nğŸ”„ Processing {split_name}...")
        
        # Dataset ìƒì„± (í° íŒŒí‹°ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ )
        dataset = Dataset(temp_files[split_name], engine='parquet', part_size='128MB')
        
        # Transform (seq ì œì™¸)
        temp_output_dir = os.path.join(output_dirs['temp'], f'{split_name}_transformed')
        os.makedirs(temp_output_dir, exist_ok=True)
        
        workflow.transform(dataset).to_parquet(
            output_path=temp_output_dir,
            shuffle=None,
            out_files_per_proc=4
        )
        print(f"  âœ… {split_name} transformed (seq ì œì™¸)")
        
        del dataset
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
        except:
            pass
        
        # seq ë³‘í•© (pandasë¡œ ì²˜ë¦¬ - row_idë¡œ ìˆœì„œ ë³´ì¥)
        print(f"  ğŸ”— Merging seq back (using row_id for order preservation)...")
        from merlin.io import Dataset as MerlinDataset
        
        # Transformëœ ë°ì´í„° ë¡œë“œ
        dataset_transformed = MerlinDataset(temp_output_dir, engine='parquet')
        df_transformed = dataset_transformed.to_ddf().compute().to_pandas()
        
        # ì›ë³¸ seq ë¡œë“œ ë° ë³‘í•©
        if split_name in seq_files:
            df_seq = pd.read_parquet(seq_files[split_name])
            
            # row_idë¡œ ì •ë ¬ (ìˆœì„œ ë³´ì¥)
            df_transformed = df_transformed.sort_values('_row_id').reset_index(drop=True)
            df_seq = df_seq.sort_values('_row_id').reset_index(drop=True)
            
            # ìˆœì„œ ê²€ì¦
            assert len(df_transformed) == len(df_seq), f"Length mismatch: {len(df_transformed)} vs {len(df_seq)}"
            assert (df_transformed['_row_id'].values == df_seq['_row_id'].values).all(), "row_id mismatch!"
            
            # seq ë³‘í•© (row_id ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì•ˆì „)
            df_final = df_transformed.copy()
            df_final['seq'] = df_seq['seq'].values  # numpy arrayë¡œ ì§ì ‘ í• ë‹¹ (ìˆœì„œ ë³´ì¥)
            
            # row_id ì œê±°
            df_final = df_final.drop('_row_id', axis=1)
            
            print(f"  âœ… seq merged: {len(df_final)} rows, {len(df_final.columns)} cols (row_id removed)")
        else:
            df_final = df_transformed.drop('_row_id', axis=1) if '_row_id' in df_transformed.columns else df_transformed
            print(f"  âš ï¸  No seq found for {split_name}")
        
        # ìµœì¢… ì €ì¥
        output_dir = output_dirs[split_name]
        os.makedirs(output_dir, exist_ok=True)
        df_final.to_parquet(
            os.path.join(output_dir, '0.parquet'),
            index=False,
            engine='pyarrow'
        )
        print(f"  âœ… Saved to {output_dir}")
        
        # ì •ë¦¬
        del df_transformed, df_final
        if split_name in seq_files:
            del df_seq
        gc.collect()
        
        # ì„ì‹œ ë³€í™˜ ë””ë ‰í† ë¦¬ ì‚­ì œ
        shutil.rmtree(temp_output_dir)
        print(f"  ğŸ§¹ Cleaned temp: {temp_output_dir}")
    
    # =================================================================
    # Step 5: Test ë°ì´í„° ì „ì²˜ë¦¬
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ§ª Step 5: Test ë°ì´í„° ì „ì²˜ë¦¬")
    print("=" * 80)
    
    test_path = 'data/test.parquet'
    if os.path.exists(test_path):
        print(f"\nğŸ“– ë¡œë”©: {test_path}")
        df_test = pd.read_parquet(test_path)
        print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(df_test):,} rows")
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (l_feat_20, l_feat_23ë§Œ ì œê±°, seqëŠ” ìœ ì§€)
        cols_to_keep = [c for c in df_test.columns if c not in EXCLUDE_COLS]
        df_test_clean = df_test[cols_to_keep].copy()
        print(f"  ğŸ—‘ï¸  ì œì™¸ ì»¬ëŸ¼: {[c for c in EXCLUDE_COLS if c in df_test.columns]}")
        
        # âœ… ìˆœì„œ ë³´ì¥ì„ ìœ„í•´ ì„ì‹œ row_id ì¶”ê°€
        df_test_clean['_row_id'] = range(len(df_test_clean))
        
        # seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if 'seq' in df_test_clean.columns:
            df_test_clean = fill_missing_seq(df_test_clean, seq_col='seq', fill_value='0.0')
            
            # seqë¥¼ row_idì™€ í•¨ê»˜ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
            test_seq_path = os.path.join(output_dirs['temp'], 'test_seq.parquet')
            df_test_clean[['_row_id', 'seq']].to_parquet(test_seq_path, index=False)
            
            # workflowìš© ë°ì´í„°ì—ì„œ seq ì œê±° (row_idëŠ” ìœ ì§€)
            df_test_no_seq = df_test_clean.drop('seq', axis=1)
        else:
            df_test_no_seq = df_test_clean
            test_seq_path = None
        
        # clicked ì»¬ëŸ¼ ì¶”ê°€ (dummy, NVTabular workflow í˜¸í™˜ìš©)
        if 'clicked' not in df_test_no_seq.columns:
            df_test_no_seq['clicked'] = 0
            print("  âš ï¸  Testì— 'clicked' ì»¬ëŸ¼ ì¶”ê°€ (dummy)")
        
        # ì„ì‹œ ì €ì¥ (seq ì œì™¸, row_id í¬í•¨)
        test_temp_path = os.path.join(output_dirs['temp'], 'test.parquet')
        df_test_no_seq.to_parquet(test_temp_path, index=False)
        print(f"  âœ… Test ì¤€ë¹„ ì™„ë£Œ (seq ë³„ë„ ì €ì¥, row_id ì¶”ê°€)")
        
        del df_test, df_test_clean, df_test_no_seq
        gc.collect()
        
        # Transform (seq ì œì™¸)
        print(f"\nğŸ”„ Processing test (seq ì œì™¸)...")
        test_dataset = Dataset(test_temp_path, engine='parquet', part_size='128MB')
        
        temp_test_output_dir = os.path.join(output_dirs['temp'], 'test_transformed')
        os.makedirs(temp_test_output_dir, exist_ok=True)
        
        workflow.transform(test_dataset).to_parquet(
            output_path=temp_test_output_dir,
            shuffle=None,
            out_files_per_proc=4
        )
        print(f"  âœ… test transformed (seq ì œì™¸)")
        
        del test_dataset
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
        except:
            pass
        
        # seq ë³‘í•© (pandasë¡œ ì²˜ë¦¬ - row_idë¡œ ìˆœì„œ ë³´ì¥)
        if test_seq_path:
            print(f"  ğŸ”— Merging seq back to test...")
            from merlin.io import Dataset as MerlinDataset
            
            # Transformëœ ë°ì´í„° ë¡œë“œ
            dataset_transformed = MerlinDataset(temp_test_output_dir, engine='parquet')
            df_test_transformed = dataset_transformed.to_ddf().compute().to_pandas()
            
            # ì›ë³¸ seq ë¡œë“œ
            df_test_seq = pd.read_parquet(test_seq_path)
            
            # row_idë¡œ ì •ë ¬ (ìˆœì„œ ë³´ì¥)
            df_test_transformed = df_test_transformed.sort_values('_row_id').reset_index(drop=True)
            df_test_seq = df_test_seq.sort_values('_row_id').reset_index(drop=True)
            
            # ìˆœì„œ ê²€ì¦
            assert len(df_test_transformed) == len(df_test_seq), f"Length mismatch: {len(df_test_transformed)} vs {len(df_test_seq)}"
            assert (df_test_transformed['_row_id'].values == df_test_seq['_row_id'].values).all(), "row_id mismatch!"
            
            # seq ë³‘í•©
            df_test_final = df_test_transformed.copy()
            df_test_final['seq'] = df_test_seq['seq'].values
            
            # row_id ì œê±°
            df_test_final = df_test_final.drop('_row_id', axis=1)
            
            print(f"  âœ… seq merged: {len(df_test_final)} rows, {len(df_test_final.columns)} cols")
        else:
            # seqê°€ ì—†ëŠ” ê²½ìš°
            from merlin.io import Dataset as MerlinDataset
            dataset_transformed = MerlinDataset(temp_test_output_dir, engine='parquet')
            df_test_final = dataset_transformed.to_ddf().compute().to_pandas()
            df_test_final = df_test_final.drop('_row_id', axis=1) if '_row_id' in df_test_final.columns else df_test_final
        
        # ìµœì¢… ì €ì¥
        output_dir = output_dirs['test']
        os.makedirs(output_dir, exist_ok=True)
        df_test_final.to_parquet(
            os.path.join(output_dir, '0.parquet'),
            index=False,
            engine='pyarrow'
        )
        print(f"  âœ… Test saved to {output_dir}")
        print(f"     (GBDT: seq ì œê±°í•´ì„œ ì‚¬ìš©, DNN: seq í¬í•¨ ì‚¬ìš©)")
        
        # ì •ë¦¬
        del df_test_final
        if test_seq_path:
            del df_test_transformed, df_test_seq
        gc.collect()
        
        # ì„ì‹œ ë³€í™˜ ë””ë ‰í† ë¦¬ ì‚­ì œ
        shutil.rmtree(temp_test_output_dir)
        print(f"  ğŸ§¹ Cleaned temp: {temp_test_output_dir}")
    else:
        print(f"\nâš ï¸  {test_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Test ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # =================================================================
    # Step 6: ì„ì‹œ íŒŒì¼ ì •ë¦¬
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ§¹ Step 6: ì„ì‹œ íŒŒì¼ ì •ë¦¬")
    print("=" * 80)
    
    if os.path.exists(output_dirs['temp']):
        # ì„ì‹œ íŒŒì¼ ê°œìˆ˜ í™•ì¸
        temp_files_count = len([f for f in os.listdir(output_dirs['temp']) if os.path.isfile(os.path.join(output_dirs['temp'], f))])
        print(f"  ğŸ—‘ï¸  ì‚­ì œí•  ì„ì‹œ íŒŒì¼: {temp_files_count}ê°œ")
        
        shutil.rmtree(output_dirs['temp'])
        print(f"  âœ… ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {output_dirs['temp']}")
    
    # =================================================================
    # ìµœì¢… ìš”ì•½
    # =================================================================
    print("\n" + "=" * 80)
    print("âœ… ì „ì²´ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“… ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nğŸ“ ìƒì„±ëœ ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ (ê³µí†µ):")
    print("  ğŸ”· ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° (seq í¬í•¨, continuous standardization):")
    for key in ['train_t', 'train_v', 'train_c', 'train_hpo', 'test']:
        path = output_dirs.get(key)
        if path and os.path.exists(path):
            print(f"    âœ… {key:12} â†’ {path}")
    
    print("\nğŸ“Š ì „ì²˜ë¦¬ ì •ë³´:")
    print(f"  - seq í¬í•¨ (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    print(f"    * seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬: ë¹ˆ ë¬¸ìì—´/NaN â†’ '0.0'")
    print(f"  - l_feat_20, l_feat_23 ì œê±° (ìƒìˆ˜ í”¼ì²˜)")
    print(f"  âš ï¸  ì¤‘ìš”: train_të¡œë§Œ workflow fit (Data Leakage ë°©ì§€!)")
    print(f"    * val/cal/testëŠ” train_t í†µê³„ë¡œ transformë§Œ ìˆ˜í–‰")
    print(f"  - Categorical: 3ê°œ (gender, age_group, inventory_id)")
    print(f"    * Categorify: ê²°ì¸¡ì¹˜ ìë™ ì²˜ë¦¬, 0-based ì¸ë±ìŠ¤")
    print(f"  - Continuous: 112ê°œ (l_feat_14 í¬í•¨)")
    print(f"    * Normalize â†’ FillMissing(0)")
    print(f"    * Normalize ë¨¼ì €: train_tì˜ ì‹¤ì œ ë¶„í¬ë¡œ mean/std ê³„ì‚°")
    print(f"    * FillMissing(0) ë‚˜ì¤‘: í‘œì¤€í™” ê³µê°„ì—ì„œ í‰ê· ê°’ìœ¼ë¡œ imputation")
    print(f"  - Normalization: Standardization (mean=0, std=1)")
    print(f"  - ëª¨ë“  splitì— train_t í†µê³„ ì ìš© (ì¼ê´€ì„± ë³´ì¥)")
    
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("\n  [GBDT ëª¨ë¸ - train_gbdt.py, hpo_xgboost.py]")
    print("  from data_loader import load_processed_data_gbdt")
    print("  X, y = load_processed_data_gbdt('data/proc_train_t', drop_seq=True)")
    print("  # âœ… seq ì œê±°")
    print("  # âœ… categoricalì€ ì´ë¯¸ ì¸ë±ìŠ¤ ì¸ì½”ë”©ë¨ (0-based)")
    print("  # âœ… continuousëŠ” ì´ë¯¸ standardization ì ìš©ë¨ (mean=0, std=1)")
    print()
    print("  [DNN ëª¨ë¸ - train_dnn_ddp.py, hpo_dnn.py]")
    print("  from merlin.io import Dataset")
    print("  dataset = Dataset('data/proc_train_t', engine='parquet')")
    print("  gdf = dataset.to_ddf().compute()")
    print("  df = gdf.to_pandas()  # seq í¬í•¨")
    print("  # âœ… l_feat_20, l_feat_23 ì´ë¯¸ ì œê±°ë¨")
    print("  # âœ… seq ê²°ì¸¡ì¹˜ ì´ë¯¸ ì²˜ë¦¬ë¨ (ë¹ˆê°’ â†’ '0.0')")
    print("  # âœ… categoricalì€ ì´ë¯¸ ì¸ë±ìŠ¤ ì¸ì½”ë”©ë¨ (0-based)")
    print("  # âœ… continuousëŠ” ì´ë¯¸ standardization ì ìš©ë¨")
    
    print("\n" + "=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ì‹¤í–‰ (0.8/0.1/0.1 ë¶„í• , HPO 10%)
  python dataset_split_and_preprocess.py
  
  # ì»¤ìŠ¤í…€ ë¹„ìœ¨
  python dataset_split_and_preprocess.py --train-ratio 0.85 --val-ratio 0.075 --cal-ratio 0.075
  
  # HPO ìƒ˜í”Œë§ ë¹„ìœ¨ ë³€ê²½
  python dataset_split_and_preprocess.py --hpo-ratio 0.15
        """
    )
    
    parser.add_argument('--train-ratio', type=float, default=0.8,
                        help='Training ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1,
                        help='Validation ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.1)')
    parser.add_argument('--cal-ratio', type=float, default=0.1,
                        help='Calibration ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.1)')
    parser.add_argument('--hpo-ratio', type=float, default=0.1,
                        help='HPO ìƒ˜í”Œë§ ë¹„ìœ¨ (trainì˜ %, ê¸°ë³¸: 0.1)')
    parser.add_argument('--random-state', type=int, default=42,
                        help='ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42)')
    
    args = parser.parse_args()
    
    try:
        process_all_data(
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            cal_ratio=args.cal_ratio,
            hpo_ratio=args.hpo_ratio,
            random_state=args.random_state
        )
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

