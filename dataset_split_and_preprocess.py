#!/usr/bin/env python3
"""
ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬ í†µí•© ìŠ¤í¬ë¦½íŠ¸

Pipeline:
1. train.parquetë¥¼ train_t(80%), train_v(10%), train_c(10%)ë¡œ ë¶„í• 
2. train_tì—ì„œ 10% stratified samplingí•˜ì—¬ train_hpo ìƒì„±
3. âš ï¸  train_të¡œë§Œ NVTabular workflow fit (Data Leakage ë°©ì§€!)
4. ë‚˜ë¨¸ì§€(val/cal/hpo/test)ëŠ” train_t í†µê³„ë¡œ transformë§Œ ìˆ˜í–‰
5. l_feat_20, l_feat_23 ì œê±° (ìƒìˆ˜ í”¼ì²˜), seqëŠ” ìœ ì§€
6. ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥:
   - data/proc_train_t, data/proc_train_v, data/proc_train_c, 
   - data/proc_train_hpo, data/proc_test
7. ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬ (data/tmp ì‚­ì œ)

ì œì™¸ ì»¬ëŸ¼:
- l_feat_20, l_feat_23: ìƒìˆ˜ í”¼ì²˜ (ì •ë³´ ì—†ìŒ)

ìœ ì§€ ì»¬ëŸ¼:
- seq: ì‹œí€€ìŠ¤ ë°ì´í„° (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)

Features:
- 4 categorical features (gender, age_group, inventory_id, l_feat_14)
  â†’ raw ìœ ì§€ (DNN ìì²´ LabelEncoder ì‚¬ìš©)
- 110 continuous features
  â†’ Normalize(mean=0, std=1) + FillMissing(0) ì ìš© (ìˆœì„œ ì¤‘ìš”!)
- seq: string (LSTM ì…ë ¥)
- Total: 4 categorical + 110 continuous + seq + clicked (target)

ì „ì²˜ë¦¬ ìƒì„¸:
- Categorical: raw ìœ ì§€ (DNNì—ì„œ LabelEncoder ì‚¬ìš©)
- Continuous: 
  1. Normalize: Standardization (mean=0, std=1) ë¨¼ì € ìˆ˜í–‰
     - ê²°ì¸¡ì¹˜ ì—†ëŠ” ì‹¤ì œ ë°ì´í„°ë¡œ mean/std ê³„ì‚°
     - âš ï¸  train_të¡œë§Œ í†µê³„ ê³„ì‚° (Data Leakage ë°©ì§€!)
     - ëª¨ë“  splitì— train_t í†µê³„ ì ìš©
  2. FillMissing(0): í‘œì¤€í™” í›„ ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ëŒ€ì²´
     - í‘œì¤€í™” ê³µê°„ì—ì„œ 0 = ì›ë˜ í‰ê· ê°’
     - ì‹¤ì§ˆì ìœ¼ë¡œ í‰ê· ê°’ imputation íš¨ê³¼
- seq: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë§Œ ì ìš© (ë¹ˆ ë¬¸ìì—´/NaN â†’ '0.0')

ìƒì„± ë””ë ‰í† ë¦¬:
- data/proc_train_t/    (80%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_v/    (10%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_c/    (10%, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/proc_train_hpo/  (~10%, HPOìš© ìƒ˜í”Œ, seq í¬í•¨)
- data/proc_test/       (test, ì „ì²˜ë¦¬ ì™„ë£Œ, seq í¬í•¨)
- data/tmp/             (ì„ì‹œ, ìë™ ì‚­ì œ)

ì‚¬ìš© ë°©ë²•:
- GBDT: ë¡œë”ì—ì„œ seq ì œê±° í›„ ì‚¬ìš©
- DNN: seq í¬í•¨ ê·¸ëŒ€ë¡œ ì‚¬ìš©

ì¥ì :
- âœ… ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° 1ë²Œë¡œ GBDT/DNN ëª¨ë‘ ì‚¬ìš©
- âœ… Data Leakage ë°©ì§€ (train_të¡œë§Œ í†µê³„ ê³„ì‚°)
- âœ… ëª¨ë“  splitì´ train_t í†µê³„ë¡œ ì¼ê´€ì„± ìœ ì§€
- âœ… Testë„ ë¯¸ë¦¬ ì „ì²˜ë¦¬ë˜ì–´ prediction ë¹ ë¦„
- âœ… HPOìš© ì‘ì€ datasetìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜
- âœ… Stratified samplingìœ¼ë¡œ ë¶„í¬ ìœ ì§€
- âœ… ìƒìˆ˜ í”¼ì²˜ ìë™ ì œê±°
- âœ… ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬
- âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° Standardization ìë™ ì ìš©
- âœ… ë””ìŠ¤í¬ ì ˆì•½ (ì¤‘ë³µ ë°ì´í„° ì—†ìŒ)
"""

import os
import gc
import shutil
import argparse
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split

def create_workflow_gbdt():
    """Create NVTabular workflow for GBDT (seq ì œê±°, standardization ì ìš©)"""
    import nvtabular as nvt
    from nvtabular import ops
    
    print("\nğŸ”§ Creating GBDT workflow with standardization...")

    # TRUE CATEGORICAL COLUMNS (only 5)
    true_categorical = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']

    # CONTINUOUS COLUMNS (110 total, l_feat_20, l_feat_23 ì œì™¸)
    all_continuous = (
        [f'feat_a_{i}' for i in range(1, 19)] +   # 18
        [f'feat_b_{i}' for i in range(1, 7)] +    # 6
        [f'feat_c_{i}' for i in range(1, 9)] +    # 8
        [f'feat_d_{i}' for i in range(1, 7)] +    # 6
        [f'feat_e_{i}' for i in range(1, 11)] +   # 10
        [f'history_a_{i}' for i in range(1, 8)] +   # 7
        [f'history_b_{i}' for i in range(1, 31)] +  # 30
        [f'l_feat_{i}' for i in range(1, 28) if i not in [20, 23]]  # 25 (l_feat_20, l_feat_23 ì œì™¸)
    )

    print(f"   Categorical: {len(true_categorical)} columns")
    print(f"   Continuous: {len(all_continuous)} columns")
    print(f"   Total features: {len(true_categorical) + len(all_continuous)}")

    # Preprocessing pipeline:
    # 1. Categorify for categorical features
    cat_features = true_categorical >> ops.Categorify(
        freq_threshold=0,
        max_size=50000
    )
    
    # 2. Normalize + FillMissing for continuous features
    # - Normalize ë¨¼ì €: ê²°ì¸¡ì¹˜ ì—†ëŠ” ë°ì´í„°ë¡œ mean/std ê³„ì‚° (ì‹¤ì œ ë¶„í¬ ë°˜ì˜)
    # - FillMissing(0) ë‚˜ì¤‘: í‘œì¤€í™” ê³µê°„ì—ì„œ 0 = í‰ê· ê°’ìœ¼ë¡œ imputation
    cont_features = all_continuous >> ops.Normalize() >> ops.FillMissing(fill_val=0)

    workflow = nvt.Workflow(cat_features + cont_features + ['clicked'])

    print("   âœ… Workflow created with standardization:")
    print("      - Categorical: Categorify")
    print("      - Continuous: Normalize(mean=0, std=1) + FillMissing(0)")
    print("      - ìˆœì„œ: Normalize ë¨¼ì € (ì‹¤ì œ ë¶„í¬ë¡œ í†µê³„), ê·¸ í›„ ê²°ì¸¡ì¹˜=0 (í‰ê· )")
    return workflow


def fill_missing_seq(df, seq_col='seq', fill_value='0.0'):
    """
    seq ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    
    seqëŠ” "1.0,2.0,3.0" í˜•íƒœì˜ ë¬¸ìì—´
    ë¹ˆ ë¬¸ìì—´, NaN, None ë“±ì„ fill_valueë¡œ ëŒ€ì²´
    """
    print(f"\n  ğŸ”§ seq ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    def clean_seq(seq_str):
        if seq_str is None or str(seq_str).strip() == '' or str(seq_str) == 'nan':
            return fill_value
        return str(seq_str)
    
    missing_count = df[seq_col].isna().sum() + (df[seq_col] == '').sum()
    df[seq_col] = df[seq_col].apply(clean_seq)
    
    print(f"  âœ… seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ (ê²°ì¸¡ì¹˜ {missing_count}ê°œ â†’ '{fill_value}'ë¡œ ëŒ€ì²´)")
    
    return df


def create_workflow_dnn():
    """Create NVTabular workflow for DNN (seq í¬í•¨, continuousë§Œ standardization)"""
    import nvtabular as nvt
    from nvtabular import ops
    
    print("\nğŸ”§ Creating DNN workflow with standardization (seq í¬í•¨)...")

    # DNNì€ categoricalì„ ìì²´ LabelEncoderë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ rawë¡œ ìœ ì§€
    # seqëŠ” ë³„ë„ë¡œ standardization ì ìš© (workflow ì™¸ë¶€)
    
    # CONTINUOUS COLUMNS (110 total, l_feat_20, l_feat_23 ì œì™¸)
    all_continuous = (
        [f'feat_a_{i}' for i in range(1, 19)] +   # 18
        [f'feat_b_{i}' for i in range(1, 7)] +    # 6
        [f'feat_c_{i}' for i in range(1, 9)] +    # 8
        [f'feat_d_{i}' for i in range(1, 7)] +    # 6
        [f'feat_e_{i}' for i in range(1, 11)] +   # 10
        [f'history_a_{i}' for i in range(1, 8)] +   # 7
        [f'history_b_{i}' for i in range(1, 31)] +  # 30
        [f'l_feat_{i}' for i in range(1, 28) if i not in [20, 23]]  # 25 (l_feat_20, l_feat_23 ì œì™¸)
    )
    
    # CATEGORICAL COLUMNS (DNNì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒë“¤ - rawë¡œ ìœ ì§€)
    categorical_raw = ['gender', 'age_group', 'inventory_id', 'l_feat_14']
    
    print(f"   Categorical (raw): {len(categorical_raw)} columns (DNN ì½”ë“œì—ì„œ LabelEncoder)")
    print(f"   Continuous: {len(all_continuous)} columns (standardization ì ìš©)")
    print(f"   seq: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë§Œ ì ìš© (workflow ì™¸ë¶€)")

    # Preprocessing pipeline:
    # 1. Categorical: rawë¡œ ìœ ì§€ (DNN ì½”ë“œì—ì„œ LabelEncoder ì‚¬ìš©)
    cat_features = categorical_raw
    
    # 2. Normalize + FillMissing for continuous features
    cont_features = all_continuous >> ops.Normalize() >> ops.FillMissing(fill_val=0)
    
    # 3. seqëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ë³„ë„ ì²˜ë¦¬)
    seq_feature = ['seq']

    workflow = nvt.Workflow(cat_features + cont_features + seq_feature + ['clicked'])

    print("   âœ… Workflow created:")
    print("      - Categorical: raw ìœ ì§€ (DNN ì½”ë“œì—ì„œ LabelEncoder)")
    print("      - Continuous: Normalize(mean=0, std=1) + FillMissing(0)")
    print("      - seq: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë§Œ ì ìš© (workflow ì™¸ë¶€)")
    return workflow


def process_all_data(train_ratio=0.8, val_ratio=0.1, cal_ratio=0.1, 
                     hpo_ratio=0.1, random_state=42):
    """
    ì „ì²´ ë°ì´í„° ë¶„í•  ë° ì „ì²˜ë¦¬
    
    Args:
        train_ratio: Training ë°ì´í„° ë¹„ìœ¨ (0.8 = 80%)
        val_ratio: Validation ë°ì´í„° ë¹„ìœ¨ (0.1 = 10%)
        cal_ratio: Calibration ë°ì´í„° ë¹„ìœ¨ (0.1 = 10%)
        hpo_ratio: HPOìš© ìƒ˜í”Œë§ ë¹„ìœ¨ (0.1 = trainì˜ 10%)
        random_state: ëœë¤ ì‹œë“œ
    """
    print("=" * 80)
    print("ğŸš€ ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“… ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ë¶„í•  ë¹„ìœ¨: train={train_ratio}, val={val_ratio}, cal={cal_ratio}")
    print(f"ğŸ”¬ HPO ìƒ˜í”Œë§ ë¹„ìœ¨: {hpo_ratio} (from train)")
    
    # ë¹„ìœ¨ ê²€ì¦
    assert abs(train_ratio + val_ratio + cal_ratio - 1.0) < 1e-6, "ë¹„ìœ¨ì˜ í•©ì´ 1ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤"
    
    # ì œì™¸í•  ì»¬ëŸ¼ ëª©ë¡ (ìƒìˆ˜ í”¼ì²˜ë§Œ ì œì™¸, seqëŠ” ìœ ì§€)
    EXCLUDE_COLS = ['l_feat_20', 'l_feat_23', '']
    print(f"ğŸ—‘ï¸  ì œì™¸ ì»¬ëŸ¼: {', '.join([c for c in EXCLUDE_COLS if c])}")
    print(f"âœ… seq ìœ ì§€ (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„°, seq í¬í•¨, continuous standardization)
    output_dirs = {
        # ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° (seq í¬í•¨, continuous standardization ì ìš©)
        # GBDT: ë¡œë”ì—ì„œ seq ì œê±° í›„ ì‚¬ìš©
        # DNN: seq í¬í•¨ ê·¸ëŒ€ë¡œ ì‚¬ìš© (categoricalì€ ìì²´ LabelEncoder)
        'train_t': 'data/proc_train_t',
        'train_v': 'data/proc_train_v',
        'train_c': 'data/proc_train_c',
        'train_hpo': 'data/proc_train_hpo',
        'test': 'data/proc_test',
        'temp': 'data/tmp'
    }
    
    # ê¸°ì¡´ ë””ë ‰í† ë¦¬ í™•ì¸
    existing = [d for d in output_dirs.values() if os.path.exists(d)]
    if existing:
        print("\nâš ï¸  ê¸°ì¡´ ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤:")
        for d in existing:
            print(f"  - {d}")
        response = input("\nğŸ”„ ê¸°ì¡´ ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•˜ê³  ë‹¤ì‹œ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if response.lower() != 'y':
            print("âŒ ì‘ì—…ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            return
        print("\nğŸ—‘ï¸  ê¸°ì¡´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘...")
        for d in existing:
            if os.path.exists(d):
                shutil.rmtree(d)
                print(f"  âœ… ì‚­ì œ: {d}")
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dirs['temp'], exist_ok=True)
    print(f"\nğŸ“ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dirs['temp']}")
    
    # =================================================================
    # Step 1: ì „ì²´ train.parquet ë¡œë“œ ë° ë¶„í• 
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ“‚ Step 1: ì „ì²´ train.parquet ë¡œë“œ ë° ë¶„í• ")
    print("=" * 80)
    
    train_path = 'data/train.parquet'
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"{train_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    print(f"\nğŸ“– ë¡œë”©: {train_path}")
    df_full = pd.read_parquet(train_path)
    total_rows = len(df_full)
    print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {total_rows:,} rows")
    print(f"  ğŸ“Š Positive ratio: {df_full['clicked'].mean():.6f}")
    
    # ë°ì´í„° ì„ê¸°
    print("\nğŸ”€ ë°ì´í„° ì…”í”Œë§...")
    df_full = df_full.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    # ë¶„í•  í¬ê¸° ê³„ì‚°
    train_size = int(total_rows * train_ratio)
    val_size = int(total_rows * val_ratio)
    cal_size = total_rows - train_size - val_size
    
    print(f"\nâœ‚ï¸  ë¶„í•  í¬ê¸°:")
    print(f"  - train_t: {train_size:,} ({train_size/total_rows:.1%})")
    print(f"  - train_v: {val_size:,} ({val_size/total_rows:.1%})")
    print(f"  - train_c: {cal_size:,} ({cal_size/total_rows:.1%})")
    
    # ë¶„í• 
    df_train_t = df_full.iloc[:train_size].copy()
    df_train_v = df_full.iloc[train_size:train_size+val_size].copy()
    df_train_c = df_full.iloc[train_size+val_size:].copy()
    
    # ë¶„í¬ í™•ì¸
    print(f"\n  ğŸ“Š ë¶„í•  í›„ ë¶„í¬:")
    print(f"    train_t positive ratio: {df_train_t['clicked'].mean():.6f}")
    print(f"    train_v positive ratio: {df_train_v['clicked'].mean():.6f}")
    print(f"    train_c positive ratio: {df_train_c['clicked'].mean():.6f}")
    
    # train_hpo ìƒì„± (train_tì—ì„œ stratified sampling)
    print(f"\nğŸ”¬ train_hpo ìƒì„± (train_tì˜ {hpo_ratio:.1%} stratified sampling)...")
    _, df_train_hpo = train_test_split(
        df_train_t,
        test_size=hpo_ratio,
        random_state=random_state,
        stratify=df_train_t['clicked']
    )
    df_train_hpo = df_train_hpo.reset_index(drop=True)
    print(f"  âœ… train_hpo: {len(df_train_hpo):,} rows")
    print(f"  ğŸ“Š train_hpo positive ratio: {df_train_hpo['clicked'].mean():.6f}")
    
    # =================================================================
    # Step 2: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° ì„ì‹œ ì €ì¥ (seqëŠ” ìœ ì§€!)
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ—‘ï¸  Step 2: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° ì„ì‹œ ì €ì¥")
    print("=" * 80)
    print("   ì œì™¸: l_feat_20, l_feat_23 (ìƒìˆ˜ í”¼ì²˜)")
    print("   ìœ ì§€: seq (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    
    splits = {
        'train_t': df_train_t,  # workflow fitìš© (í†µê³„ ê³„ì‚°)
        'train_v': df_train_v,
        'train_c': df_train_c,
        'train_hpo': df_train_hpo
    }
    
    temp_files = {}
    for name, df in splits.items():
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
        cols_to_keep = [c for c in df.columns if c not in EXCLUDE_COLS]
        df_clean = df[cols_to_keep].copy()
        
        # seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if 'seq' in df_clean.columns:
            df_clean = fill_missing_seq(df_clean, seq_col='seq', fill_value='0.0')
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_path = os.path.join(output_dirs['temp'], f'{name}.parquet')
        df_clean.to_parquet(temp_path, index=False)
        temp_files[name] = temp_path
        print(f"  âœ… {name}: {len(df_clean):,} rows, {len(df_clean.columns)} cols â†’ {temp_path}")
        
        del df_clean
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ (df_fullì€ ì´ë¯¸ ì‚¬ìš© ì™„ë£Œ)
    del df_full, df_train_t, df_train_v, df_train_c, df_train_hpo, splits
    gc.collect()
    print(f"\n  ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    # =================================================================
    # Step 3: NVTabular Workflow Fit (train_të§Œ ì‚¬ìš©)
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ”§ Step 3: NVTabular Workflow Fit (train_t ONLY - Data Leakage ë°©ì§€)")
    print("=" * 80)
    print("   âœ… train_të¡œë§Œ í†µê³„ ê³„ì‚° (val/calì€ transformë§Œ)")
    print("   âœ… seq í¬í•¨, continuousë§Œ standardization")
    print("   âœ… categoricalì€ raw ìœ ì§€ (DNN ìì²´ LabelEncoder ì‚¬ìš©)")
    
    # GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ì´ˆê¸°í™”
    try:
        import cupy as cp
        import rmm
        
        # RMM ë©”ëª¨ë¦¬ í’€ ì„¤ì • (ë” í° í’€)
        rmm.reinitialize(
            pool_allocator=True,
            initial_pool_size=8 * 1024**3,  # 8GB
            managed_memory=True,
        )
        cp.cuda.Device(0).use()
        print("âœ… GPU í™œì„±í™” (RMM pool: 8GB)")
    except Exception as e:
        print(f"âš ï¸  GPU ì´ˆê¸°í™” ê²½ê³ : {e}")
    
    from merlin.io import Dataset
    
    # train_të¡œë§Œ workflow fit (data leakage ë°©ì§€)
    print(f"\nğŸ“Š Workflow fitting on train_t ONLY ({train_size:,} rows)...")
    print("   âš¡ Part size: 128MB (ë©”ëª¨ë¦¬ íš¨ìœ¨)")
    print("   âš ï¸  ì¤‘ìš”: train_të¡œë§Œ í†µê³„ ê³„ì‚° (val/cal/test ì •ë³´ ëˆ„ì¶œ ë°©ì§€)")
    
    train_dataset = Dataset(temp_files['train_t'], engine='parquet', part_size='128MB')
    
    workflow = create_workflow_dnn()
    
    # Fit with memory cleanup
    print("   ğŸ”§ Fitting workflow on train_t...")
    workflow.fit(train_dataset)
    print("  âœ… Workflow fitted on train_t only (val/cal/testëŠ” transformë§Œ)")
    
    # WorkflowëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ìœ ì§€ (ì €ì¥ ì•ˆ í•¨)
    
    del train_dataset
    gc.collect()
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    try:
        cp.get_default_memory_pool().free_all_blocks()
        cp.get_default_pinned_memory_pool().free_all_blocks()
        print("  âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    # =================================================================
    # Step 4: Transform ê° split (ë™ì¼í•œ workflow ì‚¬ìš©)
    # =================================================================
    print("\n" + "=" * 80)
    print("âš™ï¸  Step 4: Transform ê° split (ë™ì¼í•œ í†µê³„ ì ìš©)")
    print("=" * 80)
    
    # Transform ìˆœì„œ: train_t, train_v, train_c, train_hpo
    splits_to_transform = ['train_t', 'train_v', 'train_c', 'train_hpo']
    
    for split_name in splits_to_transform:
        print(f"\nğŸ”„ Processing {split_name}...")
        
        # Dataset ìƒì„± (í° íŒŒí‹°ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê°œì„ )
        dataset = Dataset(temp_files[split_name], engine='parquet', part_size='128MB')
        
        # Transform
        output_dir = output_dirs[split_name]
        workflow.transform(dataset).to_parquet(
            output_path=output_dir,
            shuffle=None,
            out_files_per_proc=4  # íŒŒì¼ ìˆ˜ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
        )
        print(f"  âœ… {split_name} transformed â†’ {output_dir}")
        
        del dataset
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
        except:
            pass
    
    # =================================================================
    # Step 5: Test ë°ì´í„° ì „ì²˜ë¦¬
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ§ª Step 5: Test ë°ì´í„° ì „ì²˜ë¦¬")
    print("=" * 80)
    
    test_path = 'data/test.parquet'
    if os.path.exists(test_path):
        print(f"\nğŸ“– ë¡œë”©: {test_path}")
        df_test = pd.read_parquet(test_path)
        print(f"  âœ… ë¡œë“œ ì™„ë£Œ: {len(df_test):,} rows")
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (l_feat_20, l_feat_23ë§Œ ì œê±°, seqëŠ” ìœ ì§€)
        cols_to_keep = [c for c in df_test.columns if c not in EXCLUDE_COLS]
        df_test_clean = df_test[cols_to_keep].copy()
        print(f"  ğŸ—‘ï¸  ì œì™¸ ì»¬ëŸ¼: {[c for c in EXCLUDE_COLS if c in df_test.columns]}")
        print(f"  âœ… seq ìœ ì§€")
        print(f"  ğŸ“Š ë‚¨ì€ ì»¬ëŸ¼: {len(df_test_clean.columns)} columns")
        
        # seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if 'seq' in df_test_clean.columns:
            df_test_clean = fill_missing_seq(df_test_clean, seq_col='seq', fill_value='0.0')
        
        # clicked ì»¬ëŸ¼ ì¶”ê°€ (dummy, NVTabular workflow í˜¸í™˜ìš©)
        if 'clicked' not in df_test_clean.columns:
            df_test_clean['clicked'] = 0
            print("  âš ï¸  Testì— 'clicked' ì»¬ëŸ¼ ì¶”ê°€ (dummy)")
        
        # ì„ì‹œ ì €ì¥
        test_temp_path = os.path.join(output_dirs['temp'], 'test.parquet')
        df_test_clean.to_parquet(test_temp_path, index=False)
        
        del df_test, df_test_clean
        gc.collect()
        
        # Transform (ê³µí†µ ì „ì²˜ë¦¬)
        print(f"\nğŸ”„ Processing proc_test...")
        test_dataset = Dataset(test_temp_path, engine='parquet', part_size='128MB')
        workflow.transform(test_dataset).to_parquet(
            output_path=output_dirs['test'],
            shuffle=None,
            out_files_per_proc=4  # íŒŒì¼ ìˆ˜ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
        )
        print(f"  âœ… test transformed â†’ {output_dirs['test']}")
        print(f"     (GBDT: seq ì œê±°í•´ì„œ ì‚¬ìš©, DNN: seq í¬í•¨ ì‚¬ìš©)")
        
        del test_dataset
        gc.collect()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
        except:
            pass
    else:
        print(f"\nâš ï¸  {test_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Test ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # =================================================================
    # Step 6: ì„ì‹œ íŒŒì¼ ì •ë¦¬
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ§¹ Step 6: ì„ì‹œ íŒŒì¼ ì •ë¦¬")
    print("=" * 80)
    
    if os.path.exists(output_dirs['temp']):
        # ì„ì‹œ íŒŒì¼ ê°œìˆ˜ í™•ì¸
        temp_files_count = len([f for f in os.listdir(output_dirs['temp']) if os.path.isfile(os.path.join(output_dirs['temp'], f))])
        print(f"  ğŸ—‘ï¸  ì‚­ì œí•  ì„ì‹œ íŒŒì¼: {temp_files_count}ê°œ")
        
        shutil.rmtree(output_dirs['temp'])
        print(f"  âœ… ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {output_dirs['temp']}")
    
    # =================================================================
    # ìµœì¢… ìš”ì•½
    # =================================================================
    print("\n" + "=" * 80)
    print("âœ… ì „ì²´ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"ğŸ“… ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\nğŸ“ ìƒì„±ëœ ì „ì²˜ë¦¬ ë””ë ‰í† ë¦¬ (ê³µí†µ):")
    print("  ğŸ”· ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„° (seq í¬í•¨, continuous standardization):")
    for key in ['train_t', 'train_v', 'train_c', 'train_hpo', 'test']:
        path = output_dirs.get(key)
        if path and os.path.exists(path):
            print(f"    âœ… {key:12} â†’ {path}")
    
    print("\nğŸ“Š ì „ì²˜ë¦¬ ì •ë³´:")
    print(f"  - seq í¬í•¨ (DNNìš©, GBDTëŠ” ë¡œë”ì—ì„œ ì œê±°)")
    print(f"    * seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬: ë¹ˆ ë¬¸ìì—´/NaN â†’ '0.0'")
    print(f"  - l_feat_20, l_feat_23 ì œê±° (ìƒìˆ˜ í”¼ì²˜)")
    print(f"  âš ï¸  ì¤‘ìš”: train_të¡œë§Œ workflow fit (Data Leakage ë°©ì§€!)")
    print(f"    * val/cal/testëŠ” train_t í†µê³„ë¡œ transformë§Œ ìˆ˜í–‰")
    print(f"  - Categorical: raw ìœ ì§€ (DNN ìì²´ LabelEncoder ì‚¬ìš©)")
    print(f"  - Continuous: Normalize â†’ FillMissing(0) (110ê°œ í”¼ì²˜)")
    print(f"    * Normalize ë¨¼ì €: train_tì˜ ì‹¤ì œ ë¶„í¬ë¡œ mean/std ê³„ì‚°")
    print(f"    * FillMissing(0) ë‚˜ì¤‘: í‘œì¤€í™” ê³µê°„ì—ì„œ í‰ê· ê°’ìœ¼ë¡œ imputation")
    print(f"  - Normalization: Standardization (mean=0, std=1)")
    print(f"  - ëª¨ë“  splitì— train_t í†µê³„ ì ìš© (ì¼ê´€ì„± ë³´ì¥)")
    
    print("\nğŸ’¡ ì‚¬ìš© ë°©ë²•:")
    print("\n  [GBDT ëª¨ë¸ - train_gbdt.py, hpo_xgboost.py]")
    print("  from data_loader import load_processed_data_gbdt")
    print("  X, y = load_processed_data_gbdt('data/proc_train_t', drop_seq=True)")
    print("  # âœ… seq ì œê±°")
    print("  # âœ… continuousëŠ” ì´ë¯¸ standardization ì ìš©ë¨ (mean=0, std=1)")
    print()
    print("  [DNN ëª¨ë¸ - train_dnn_ddp.py, hpo_dnn.py]")
    print("  from merlin.io import Dataset")
    print("  dataset = Dataset('data/proc_train_t', engine='parquet')")
    print("  gdf = dataset.to_ddf().compute()")
    print("  df = gdf.to_pandas()  # seq í¬í•¨")
    print("  # âœ… l_feat_20, l_feat_23 ì´ë¯¸ ì œê±°ë¨")
    print("  # âœ… seq ê²°ì¸¡ì¹˜ ì´ë¯¸ ì²˜ë¦¬ë¨ (ë¹ˆê°’ â†’ '0.0')")
    print("  # âœ… continuousëŠ” ì´ë¯¸ standardization ì ìš©ë¨")
    print("  # âš ï¸  Categorical encodingì€ DNN ì½”ë“œì—ì„œ LabelEncoder ì‚¬ìš©")
    
    print("\n" + "=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ë°ì´í„°ì…‹ ë¶„í•  ë° NVTabular ì „ì²˜ë¦¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ì‹¤í–‰ (0.8/0.1/0.1 ë¶„í• , HPO 10%)
  python dataset_split_and_preprocess.py
  
  # ì»¤ìŠ¤í…€ ë¹„ìœ¨
  python dataset_split_and_preprocess.py --train-ratio 0.85 --val-ratio 0.075 --cal-ratio 0.075
  
  # HPO ìƒ˜í”Œë§ ë¹„ìœ¨ ë³€ê²½
  python dataset_split_and_preprocess.py --hpo-ratio 0.15
        """
    )
    
    parser.add_argument('--train-ratio', type=float, default=0.8,
                        help='Training ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.8)')
    parser.add_argument('--val-ratio', type=float, default=0.1,
                        help='Validation ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.1)')
    parser.add_argument('--cal-ratio', type=float, default=0.1,
                        help='Calibration ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸: 0.1)')
    parser.add_argument('--hpo-ratio', type=float, default=0.1,
                        help='HPO ìƒ˜í”Œë§ ë¹„ìœ¨ (trainì˜ %, ê¸°ë³¸: 0.1)')
    parser.add_argument('--random-state', type=int, default=42,
                        help='ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42)')
    
    args = parser.parse_args()
    
    try:
        process_all_data(
            train_ratio=args.train_ratio,
            val_ratio=args.val_ratio,
            cal_ratio=args.cal_ratio,
            hpo_ratio=args.hpo_ratio,
            random_state=args.random_state
        )
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

