# Toss Click Prediction Challenge

í´ë¦­ ì˜ˆì¸¡ ëŒ€íšŒë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸. GBDT (XGBoost/CatBoost) ë° DNN (WideDeepCTR) ëª¨ë¸ ì§€ì›.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
conda env create -f environment.yaml
conda activate toss-env
```

### 2. ë°ì´í„° ì „ì²˜ë¦¬ (í•„ìˆ˜)
```bash
python dataset_split_and_preprocess.py
```

**ìƒì„± ê²°ê³¼:**
```
data/proc_train_t/     # Training (80%)
data/proc_train_v/     # Validation (10%)
data/proc_train_c/     # Calibration (10%)
data/proc_train_hpo/   # HPO subset (~10% of train_t)
data/proc_test/        # Test data
```

**ì „ì²˜ë¦¬ ë‚´ìš©:**
- âœ… l_feat_20, l_feat_23 ì œê±° (ìƒìˆ˜ í”¼ì²˜)
- âœ… Continuous features standardization (mean=0, std=1)
- âœ… seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ('0.0')
- âœ… ê³µí†µ ë°ì´í„°ë¡œ GBDT/DNN ëª¨ë‘ ì§€ì›

### 3. ëª¨ë¸ í•™ìŠµ

**GBDT (XGBoost):**
```bash
python train_gbdt.py
```

**DNN (Multi-GPU):**
```bash
python train_dnn_ddp.py
```

### 4. ì˜ˆì¸¡
```bash
# GBDT
python pred_gbdt.py --model-dir result_GBDT_xgboost/20231201_120000

# DNN
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000
```

## ğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸

```
dataset_split_and_preprocess.py ì‹¤í–‰
â†“
data/proc_train_t/    (ê³µí†µ ì „ì²˜ë¦¬ ë°ì´í„°)
data/proc_train_v/    - l_feat_20, l_feat_23 ì œê±°
data/proc_train_c/    - continuous standardized
data/proc_train_hpo/  - seq ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ('0.0')
data/proc_test/       - seq í¬í•¨
â†“
â”œâ”€ GBDT (train_gbdt.py, hpo_xgboost.py)
â”‚  â””â”€ load_processed_data_gbdt(path)
â”‚     â†’ seq ìë™ ì œê±°ë¨, categorical encoded, continuous standardized
â”‚
â””â”€ DNN (train_dnn_ddp.py, hpo_dnn.py)
   â””â”€ load_processed_dnn_data(path)
      â†’ seq í¬í•¨, continuous standardized, categorical raw
```

**ì¥ì :**
- âœ… 1íšŒ ì „ì²˜ë¦¬ë¡œ ëª¨ë“  ëª¨ë¸ ì§€ì›
- âœ… í†µê³„ ì¼ê´€ì„± ë³´ì¥ (ì „ì²´ trainìœ¼ë¡œ fit)
- âœ… ë””ìŠ¤í¬ ì ˆì•½ (ì¤‘ë³µ ë°ì´í„° ì—†ìŒ)
- âœ… ë¹ ë¥¸ ë¡œë”© (pre-processed)

## ğŸ”§ Hyperparameter Optimization

### XGBoost HPO
```bash
python hpo_xgboost.py \
  --n-trials 100 \
  --use-hpo-subset  # proc_train_hpo ì‚¬ìš© (ë¹ ë¦„)
```

### DNN HPO
```bash
python hpo_dnn.py \
  --n-trials 50 \
  --use-mixup
```

**ê²°ê³¼:**
- `config_optimized.yaml` (GBDT)
- `config_widedeep_optimized_best_params.yaml` (DNN)

## ğŸ“ ì£¼ìš” íŒŒì¼

```
toss-challenge/
â”œâ”€â”€ dataset_split_and_preprocess.py  # ë°ì´í„° ì „ì²˜ë¦¬ (í•„ìˆ˜)
â”œâ”€â”€ train_gbdt.py                    # GBDT í•™ìŠµ
â”œâ”€â”€ train_dnn_ddp.py                 # DNN í•™ìŠµ (Multi-GPU)
â”œâ”€â”€ pred_gbdt.py                     # GBDT ì˜ˆì¸¡ + calibration
â”œâ”€â”€ pred_dnn_ddp.py                  # DNN ì˜ˆì¸¡ + calibration
â”œâ”€â”€ hpo_xgboost.py                   # XGBoost HPO
â”œâ”€â”€ hpo_dnn.py                       # DNN HPO
â”œâ”€â”€ data_loader.py                   # ë°ì´í„° ë¡œë”
â”œâ”€â”€ utils.py                         # ê³µí†µ í•¨ìˆ˜
â”œâ”€â”€ mixup.py                         # MixUp ì¦ê°•
â”œâ”€â”€ config_GBDT.yaml                 # GBDT ì„¤ì •
â””â”€â”€ data/
    â”œâ”€â”€ train.parquet                # ì›ë³¸ (10.7M rows)
    â”œâ”€â”€ test.parquet                 # í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ proc_train_t/                # ì „ì²˜ë¦¬ ì™„ë£Œ
    â”œâ”€â”€ proc_train_v/
    â”œâ”€â”€ proc_train_c/
    â”œâ”€â”€ proc_train_hpo/
    â””â”€â”€ proc_test/
```

## âš™ï¸ ì„¤ì • íŒŒì¼

### GBDT (config_GBDT.yaml)
```yaml
data:
  train_t_path: "data/proc_train_t"
  train_v_path: "data/proc_train_v"
  train_c_path: "data/proc_train_c"

model:
  name: "xgboost"  # or "catboost"

xgboost:
  n_estimators: 200
  learning_rate: 0.1
  max_depth: 8
  use_mixup: true
  mixup_alpha: 0.6
  mixup_ratio: 0.3
```

### DNN
ì»¤ë§¨ë“œ ë¼ì¸ ë˜ëŠ” HPO ê²°ê³¼ YAML ì‚¬ìš©:
```bash
python train_dnn_ddp.py --config hpo_results.yaml --epochs 5
```

## ğŸ“ˆ í”¼ì²˜ ë° í‰ê°€

### í”¼ì²˜
- **Categorical** (4): gender, age_group, inventory_id, l_feat_14
- **Continuous** (110): feat_a_*, feat_b_*, feat_c_*, history_*, l_feat_*
- **Sequence** (DNN only): seq
- **ì œì™¸**: l_feat_20, l_feat_23 (ìƒìˆ˜ í”¼ì²˜)

### í‰ê°€ ë©”íŠ¸ë¦­
```
Score = 0.5 Ã— AP + 0.5 Ã— (1 / (1 + WLL))
```
- AP: Average Precision
- WLL: Weighted LogLoss (50:50 class balance)

## ğŸ¯ ê³ ê¸‰ ê¸°ëŠ¥

### Calibration (ìë™)
Prediction ì‹œ 4ê°€ì§€ ë°©ë²• ìë™ í…ŒìŠ¤íŠ¸í•˜ì—¬ best ì„ íƒ:
- none, isotonic, sigmoid, temperature
- ì„±ëŠ¥ ê°œì„  ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©

```bash
# ìë™ (ê¸°ë³¸)
python pred_gbdt.py --model-dir result_GBDT_xgboost/xxx

# ë¹„í™œì„±í™”
python pred_gbdt.py --model-dir result_GBDT_xgboost/xxx --no-calibration
```

### MixUp ì¦ê°•
- **GBDT**: ì˜¤í”„ë¼ì¸ (í•™ìŠµ ì „)
- **DNN**: ì˜¨ë¼ì¸ (ë°°ì¹˜ë§ˆë‹¤)

## ğŸ’¾ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### GBDT
- GPU: 10-14GB
- RAM: 32GB+
- í•™ìŠµ: 10-30ë¶„

### DNN
- GPU: 24GB Ã— 4ê°œ ê¶Œì¥
- RAM: 64GB+
- í•™ìŠµ: 30ë¶„-1ì‹œê°„ (5 epochs)

## ğŸ› ë¬¸ì œ í•´ê²°

### ì „ì²˜ë¦¬ ì¤‘ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# 1. GPU ì„ íƒ (ê°€ì¥ í° ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ GPU ì‚¬ìš©)
CUDA_VISIBLE_DEVICES=0 python dataset_split_and_preprocess.py

# 2. ìŠ¤í¬ë¦½íŠ¸ëŠ” ìë™ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ìµœì í™”ë¨:
#    - RMM pool: 8GB
#    - Part size: 128MB (ë©”ëª¨ë¦¬ íš¨ìœ¨)
#    - Out files: 4 (íŒŒì¼ ìˆ˜ ê°ì†Œ)
#    - ê° ë‹¨ê³„ë§ˆë‹¤ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
```

### í•™ìŠµ ì¤‘ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# DNN: GPU ê°œìˆ˜ ì¡°ì •
CUDA_VISIBLE_DEVICES=0,1 python train_dnn_ddp.py

# DNN: Batch size ê°ì†Œ
python train_dnn_ddp.py --batch-size 256
```

### í™˜ê²½ ì¬ì„¤ì¹˜
```bash
conda env remove -n toss-env
conda env create -f environment.yaml
```

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

- **ë°ì´í„° ë¶„ì„**: `analysis/README.md`
- **í™˜ê²½ ì„¤ì •**: `environment.yaml`
- **ì„¤ì • ì˜ˆì‹œ**: `config_GBDT.yaml`

## ğŸ“ ì£¼ìš” íŠ¹ì§•

- âœ… í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (1íšŒ ì‹¤í–‰, ëª¨ë“  ëª¨ë¸ ì§€ì›)
- âœ… GPU ê°€ì† (GBDT/DNN ëª¨ë‘)
- âœ… ìë™ calibration (prediction ì‹œ)
- âœ… HPO ì§€ì› (Optuna)
- âœ… MixUp ë°ì´í„° ì¦ê°•
- âœ… Multi-GPU DNN í•™ìŠµ (DDP)
- âœ… ë©”ëª¨ë¦¬ ìµœì í™”
