# Toss Click Prediction Challenge

í´ë¦­ ì˜ˆì¸¡ ëŒ€íšŒë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. GBDT (XGBoost/CatBoost) ë° DNN (Deep Neural Network) ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
conda env create -f environment.yaml
conda activate toss-env
```

### 2. ë°ì´í„° ë¶„í• 
```bash
# train.parquet â†’ train_t (80%) / train_v (10%) / train_c (10%)
python dataset_split.py
```

### 3. ëª¨ë¸ í•™ìŠµ
```bash
# GBDT (XGBoost)
python train_gbdt.py

# DNN (ë©€í‹° GPU)
python train_dnn_ddp.py
```

### 4. ì˜ˆì¸¡
```bash
# GBDT (ìë™ calibration í¬í•¨)
python pred_gbdt.py --model-dir result_GBDT_xgboost/20231201_120000

# DNN (ìë™ calibration í¬í•¨)
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
toss-challenge/
â”œâ”€â”€ train_gbdt.py                # GBDT ëª¨ë¸ í›ˆë ¨
â”œâ”€â”€ pred_gbdt.py                 # GBDT ëª¨ë¸ ì˜ˆì¸¡ + ìë™ calibration
â”œâ”€â”€ train_dnn_ddp.py             # DNN ë©€í‹° GPU í›ˆë ¨
â”œâ”€â”€ pred_dnn_ddp.py              # DNN ëª¨ë¸ ì˜ˆì¸¡ + ìë™ calibration
â”œâ”€â”€ hpo_xgboost.py               # XGBoost HPO
â”œâ”€â”€ hpo_catboost.py              # CatBoost HPO
â”œâ”€â”€ hpo_dnn.py                   # DNN HPO
â”œâ”€â”€ dataset_split.py             # ë°ì´í„° ë¶„í•  (train/val/cal)
â”œâ”€â”€ utils.py                     # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ data_loader.py               # ë°ì´í„° ë¡œë”
â”œâ”€â”€ mixup.py                     # MixUp ë°ì´í„° ì¦ê°•
â”œâ”€â”€ config_GBDT.yaml             # GBDT ì„¤ì •
â”œâ”€â”€ config_dnn_example.yaml      # DNN ì„¤ì • ì˜ˆì‹œ
â”œâ”€â”€ environment.yaml             # Conda í™˜ê²½
â””â”€â”€ data/
    â”œâ”€â”€ train.parquet            # ì›ë³¸ (10.7M rows)
    â”œâ”€â”€ train_t.parquet          # í›ˆë ¨ (80%)
    â”œâ”€â”€ train_v.parquet          # ê²€ì¦ (10%)
    â”œâ”€â”€ train_c.parquet          # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (10%)
    â””â”€â”€ test.parquet             # í…ŒìŠ¤íŠ¸
```

## ğŸ› ï¸ GBDT ëª¨ë¸

### í•™ìŠµ
```bash
# XGBoost ê¸°ë³¸ ì‹¤í–‰
python train_gbdt.py

# CatBoost ì‚¬ìš©
python train_gbdt.py --preset catboost_deep

# ë°ì´í„° ì¬ì²˜ë¦¬
python train_gbdt.py --force-reprocess
```

### ì˜ˆì¸¡
```bash
# ìë™ calibration (ê¸°ë³¸ê°’ - ê¶Œì¥)
python pred_gbdt.py --model-dir result_GBDT_xgboost/20231201_120000

# Calibration ë¹„í™œì„±í™”
python pred_gbdt.py --model-dir result_GBDT_xgboost/20231201_120000 --no-calibration
```

### ì„¤ì • íŒŒì¼ (config_GBDT.yaml)
```yaml
model:
  name: "xgboost"  # "xgboost" or "catboost"

training:
  use_mixup: false
  mixup_alpha: 0.3
  mixup_ratio: 0.5

xgboost:
  n_estimators: 200
  learning_rate: 0.1
  max_depth: 8
  tree_method: "gpu_hist"

catboost:
  n_estimators: 200
  learning_rate: 0.1
  max_depth: 8
  task_type: "GPU"
```

## ğŸ§  DNN ëª¨ë¸

### í•™ìŠµ

**ê¸°ë³¸ ì‹¤í–‰:**
```bash
python train_dnn_ddp.py
```

**HPO ê²°ê³¼ í™œìš© (ê¶Œì¥):**
```bash
# 1. HPO ì‹¤í–‰
python hpo_dnn.py --train-path data/train_t.parquet --n-trials 50

# 2. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ
python train_dnn_ddp.py --config config_dnn_optimized_best_params.yaml --epochs 20
```

**ì»¤ë§¨ë“œ ë¼ì¸ ì˜µì…˜:**
```bash
# ì„¤ì • override
python train_dnn_ddp.py --epochs 10 --learning-rate 0.0005 --no-mixup
```

### ì˜ˆì¸¡
```bash
# ìë™ calibration (ê¸°ë³¸ê°’ - ê¶Œì¥)
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000

# Calibration ë¹„í™œì„±í™”
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000 --no-calibration
```

### ì»¤ë§¨ë“œ ë¼ì¸ ì˜µì…˜

**Training:**

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--config` | YAML config íŒŒì¼ (HPO ê²°ê³¼) | None |
| `--epochs` | í•™ìŠµ epochs | 5 |
| `--batch-size` | Batch size | 1024 |
| `--learning-rate` | Learning rate | 0.001 |
| `--no-mixup` | MixUp ë¹„í™œì„±í™” | False |

**Prediction:**

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--model-dir` | ëª¨ë¸ ë””ë ‰í† ë¦¬ | í•„ìˆ˜ |
| `--test-path` | í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ | data/test.parquet |
| `--no-calibration` | Calibration ë¹„í™œì„±í™” | False |
| `--batch-size` | ë°°ì¹˜ ì‚¬ì´ì¦ˆ | 2048 |

### Config íŒŒì¼ í˜•ì‹

**HPO ê²°ê³¼ (ìë™ ìƒì„±):**
```yaml
best_score: 0.875432
best_params:
  learning_rate: 0.001234
  weight_decay: 0.000056
  emb_dim: 32
  lstm_hidden: 64
  n_layers: 3
  mixup_alpha: 0.3
```

**ìˆ˜ë™ ì‘ì„±:**
```yaml
LEARNING_RATE: 0.002
WEIGHT_DECAY: 0.0001
USE_MIXUP: true
MODEL:
  EMB_DIM: 64
  LSTM_HIDDEN: 128
  HIDDEN_UNITS: [1024, 512, 256]
  CROSS_LAYERS: 3
```

**Config ìš°ì„ ìˆœìœ„:** CLI args > YAML config > Default config

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (HPO)

Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

### ì‚¬ìš©ë²•

```bash
# XGBoost
python hpo_xgboost.py --data-path data/train_fold1.parquet --n-trials 1000

# CatBoost
python hpo_catboost.py --data-path data/train_fold1.parquet --n-trials 1000 --task-type GPU

# DNN
python hpo_dnn.py --train-path data/train_fold1.parquet --n-trials 100
```

### HPO í›„ ì‚¬ìš©

```bash
# GBDT
python train_gbdt.py --config config_GBDT_optimized.yaml

# DNN
python train_dnn_ddp.py --config config_dnn_optimized_best_params.yaml --epochs 30
```

### ìµœì í™”ë˜ëŠ” íŒŒë¼ë¯¸í„°

**XGBoost/CatBoost:**
- `n_estimators`, `learning_rate`, `max_depth`, `subsample`, `colsample_bytree`
- `min_child_weight`, `gamma`, `reg_alpha`, `reg_lambda`

**DNN:**
- `learning_rate`, `weight_decay`, `batch_size`
- `emb_dim`, `lstm_hidden`, `cross_layers`, `hidden_units`, `dropout`
- `mixup_alpha`, `mixup_prob`

## ğŸ“Š ë°ì´í„° ì¤€ë¹„

### ë°ì´í„° ë¶„í•  (í•„ìˆ˜)
```bash
python dataset_split.py
```

**ìƒì„± íŒŒì¼:**
- `data/train_t.parquet`: í›ˆë ¨ (80%) - ëª¨ë¸ í•™ìŠµ
- `data/train_v.parquet`: ê²€ì¦ (10%) - Early stopping
- `data/train_c.parquet`: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (10%) - Prediction ì‹œ calibration

**íŠ¹ì§•:**
- Stratified split (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
- ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
- ì¬í˜„ì„± ë³´ì¥

## ğŸ¯ Calibration (ìë™ í™•ë¥  ë³´ì •)

**Prediction ì‹œì ì— ìë™ìœ¼ë¡œ ìµœì  calibration ë°©ë²• ì„ íƒ**

### ì‘ë™ ë°©ì‹

1. **train_c ë¶„í• **: Balanced set (50:50) + Test set (imbalanced)
2. **4ê°€ì§€ ë°©ë²• í…ŒìŠ¤íŠ¸**: none, isotonic, sigmoid, temperature
3. **ìë™ ì„ íƒ**: Test setì—ì„œ ê°€ì¥ ë†’ì€ scoreë¥¼ ê°€ì§„ ë°©ë²• ì‚¬ìš©
4. **ì•ˆì „ì¥ì¹˜**: ì„±ëŠ¥ ë–¨ì–´ì§€ë©´ ì›ë³¸ ì‚¬ìš© (none ì„ íƒ)

```
train_c â†’ Balanced fit (50:50) + Imbalanced test (~1% pos)
           â†“
  Test 4 methods: none, isotonic, sigmoid, temperature
           â†“
  Select best â†’ Apply to test.parquet
```

### ì‚¬ìš© ì˜ˆì‹œ

```bash
# ìë™ calibration (ê¸°ë³¸ê°’ - ê¶Œì¥)
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000

# Calibration ë¹„í™œì„±í™”
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000 --no-calibration
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ¯ Finding Best Calibration Method
...
   [NONE]       Score: 0.850000
   [ISOTONIC]   Score: 0.865000
   [SIGMOID]    Score: 0.868000
   [TEMPERATURE] Score: 0.872000 â† Optimal temperature: 1.2345

ğŸ† Best Method: TEMPERATURE (Improvement: +0.022000)
```

**ì¥ì :**
- âœ… ìë™ ìµœì í™”
- âœ… í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
- âœ… ì„±ëŠ¥ ë–¨ì–´ì§€ë©´ ì›ë³¸ ì‚¬ìš©
- âœ… Balanced fittingìœ¼ë¡œ robust

## ğŸ’¡ MixUp ë°ì´í„° ì¦ê°•

### GBDT MixUp
- ì˜¤í”„ë¼ì¸ ì¦ê°•: í•™ìŠµ ì „ MixUp ìƒ˜í”Œ ìƒì„±
- Configì—ì„œ ì„¤ì •: `use_mixup: true`, `mixup_ratio: 0.5`

### DNN MixUp
- ì˜¨ë¼ì¸ ì¦ê°•: ë°°ì¹˜ë§ˆë‹¤ ì‹¤ì‹œê°„ MixUp
- í™•ë¥ ì  ì ìš©: `MIXUP_PROB`ë¡œ ë°°ì¹˜ë³„ ì ìš© í™•ë¥  ì¡°ì ˆ
- ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ ì ìš© (ë²”ì£¼í˜•/ì‹œí€€ìŠ¤ëŠ” ì›ë³¸ ìœ ì§€)

**ê¶Œì¥ ì„¤ì •:**
- `MIXUP_ALPHA`: 0.3
- `MIXUP_PROB` (DNN): 0.5
- `mixup_ratio` (GBDT): 0.5

## ğŸ“ˆ í”¼ì²˜ ë° í‰ê°€

### í”¼ì²˜
- **ë²”ì£¼í˜•** (5ê°œ): gender, age_group, inventory_id, day_of_week, hour
- **ì—°ì†í˜•** (110ê°œ): feat_a_*, feat_b_*, feat_c_*, history_*, l_feat_*
- **ì‹œí€€ìŠ¤** (DNN only): seq
- **ì œì™¸**: l_feat_20, l_feat_23 (ìƒìˆ˜)

### í‰ê°€ ë©”íŠ¸ë¦­
```
Score = 0.5 * AP + 0.5 * (1 / (1 + WLL))
```
- **AP**: Average Precision
- **WLL**: Weighted LogLoss (50:50 class balance)

## ğŸ’¾ ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥

### GBDT
- GPU ë©”ëª¨ë¦¬: 10-14GB
- System RAM: 32GB+
- í•™ìŠµ ì‹œê°„: 10-30ë¶„

### DNN
- GPU ë©”ëª¨ë¦¬: 24GB per GPU (4 GPUs ê¶Œì¥)
- System RAM: 64GB+
- í•™ìŠµ ì‹œê°„: 5 epochs â†’ 30ë¶„-1ì‹œê°„

## ğŸ“ ì£¼ìš” ì—…ë°ì´íŠ¸

- **2025-10-08**: **Calibration ìë™ ì„ íƒ ì‹œìŠ¤í…œ**
  - Prediction ì‹œ 4ê°€ì§€ ë°©ë²• ìë™ í…ŒìŠ¤íŠ¸ ë° best ì„ íƒ
  - Training ì‹œê°„ ë‹¨ì¶• (calibration ì œê±°)
  - ì„±ëŠ¥ ë–¨ì–´ì§€ë©´ ì›ë³¸ ì‚¬ìš©
  
- **2025-10-08**: **DNN HPO ê²°ê³¼ í™œìš©**
  - YAML configë¡œ HPO ê²°ê³¼ ì €ì¥/ë¡œë“œ
  - CLI args > YAML > Default ìš°ì„ ìˆœìœ„
  
- **2025-10-08**: **í•™ìŠµ/ì˜ˆì¸¡ ë¶„ë¦¬**
  - Training: ëª¨ë¸ë§Œ ì €ì¥
  - Prediction: ìë™ calibration + submission ìƒì„±
  
- **2025-10-08**: **ë°ì´í„° ë¶„í•  ë³€ê²½**
  - `dataset_split.py`ë¡œ train_t/train_v/train_c ìƒì„±
  - ì¬í˜„ì„± í–¥ìƒ ë° ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
  
- **2025-10-08**: **MixUp ì¦ê°• ì¶”ê°€**
  - GBDT: ì˜¤í”„ë¼ì¸ ì¦ê°•
  - DNN: ì˜¨ë¼ì¸ í™•ë¥ ì  ì¦ê°•

## ğŸ¯ ê³ ê¸‰ ì‚¬ìš©ë²•

### HPO ì›Œí¬í”Œë¡œìš°

```bash
# 1. ë¹ ë¥¸ HPO (ì‘ì€ ë°ì´í„°)
python hpo_dnn.py \
    --train-path data/train_t.parquet \
    --n-trials 50 \
    --subsample-ratio 0.3 \
    --max-epochs 5

# 2. ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
python train_dnn_ddp.py \
    --config config_dnn_optimized_best_params.yaml \
    --epochs 30

# 3. ì˜ˆì¸¡ (ìë™ calibration)
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000
```

### MixUp ì„¤ì •

**GBDT (config_GBDT.yaml):**
```yaml
training:
  use_mixup: true
  mixup_alpha: 0.3
  mixup_ratio: 0.5  # 50% ìƒ˜í”Œ ì¶”ê°€
```

**DNN (config or CLI):**
```bash
python train_dnn_ddp.py --config hpo_results.yaml
# Configì— mixup_alpha, mixup_prob í¬í•¨
```

### Calibration ìƒì„¸

**ìë™ ì„ íƒ í”„ë¡œì„¸ìŠ¤:**
1. train_cë¥¼ balanced fit set + imbalanced test setìœ¼ë¡œ ë¶„í• 
2. 4ê°€ì§€ ë°©ë²•(none, isotonic, sigmoid, temperature) ëª¨ë‘ fitting
3. Test setì—ì„œ í‰ê°€í•˜ì—¬ best ì„ íƒ
4. Best methodë¡œ test.parquet ì˜ˆì¸¡

**ë¹„í™œì„±í™”:**
```bash
python pred_dnn_ddp.py --model-dir result_dnn_ddp/20231201_120000 --no-calibration
```

## ğŸ” ë°ì´í„° ë¶„ì„

`analysis/` ë””ë ‰í† ë¦¬ì— EDA ë° í”¼ì²˜ ë¶„ì„ ë„êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `analysis/README.md`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

**ì£¼ìš” ë„êµ¬:**
- `chunk_eda.py`: ëŒ€ìš©ëŸ‰ ë°ì´í„° EDA
- `feature_quality_analysis.py`: í”¼ì²˜ í’ˆì§ˆ ë¶„ì„
- `compute_normalization_stats.py`: í‘œì¤€í™” í†µê³„ ê³„ì‚°

## ğŸ› ë¬¸ì œ í•´ê²°

### í™˜ê²½ ë¬¸ì œ
```bash
# í™˜ê²½ ì¬ì„¤ì¹˜
conda env remove -n toss-env
conda env create -f environment.yaml
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GBDT: ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬ (ì½”ë“œ ë‚´ë¶€)
python train_gbdt.py

# DNN: GPU ê°œìˆ˜ ë˜ëŠ” batch size ì¡°ì •
CUDA_VISIBLE_DEVICES=0,1 python train_dnn_ddp.py
python train_dnn_ddp.py --batch-size 512
```

### cuDF string limit
ìë™ìœ¼ë¡œ `seq` ì»¬ëŸ¼ì´ ì œì™¸ë©ë‹ˆë‹¤ (NVTabular ì²˜ë¦¬ ì‹œ).

## ğŸ¤ ì°¸ê³ ì‚¬í•­

- **í†µí•© í™˜ê²½**: GBDTì™€ DNN ëª¨ë‘ `toss-env` ì‚¬ìš©
- **GPU í•„ìˆ˜**: CUDA 11.8+ ê¶Œì¥
- **ë°ì´í„° ë¶„í•  í•„ìˆ˜**: `dataset_split.py` ë¨¼ì € ì‹¤í–‰
- **Calibration**: Prediction ì‹œ ìë™ ìˆ˜í–‰ (--no-calibrationìœ¼ë¡œ ë¹„í™œì„±í™” ê°€ëŠ¥)
- **HPO**: ì‘ì€ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ ì‹¤í–‰ í›„ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ ê¶Œì¥

## ğŸ“š ìƒì„¸ ë¬¸ì„œ

- **ë°ì´í„° ë¶„ì„**: `analysis/README.md`
- **í™˜ê²½ ì„¤ì •**: `environment.yaml` ì£¼ì„ ì°¸ì¡°
- **ì„¤ì • ì˜ˆì‹œ**: `config_GBDT.yaml`, `config_dnn_example.yaml`
