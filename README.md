# Toss Click Prediction Challenge

í´ë¦­ ì˜ˆì¸¡ ëŒ€íšŒë¥¼ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. GBDT (XGBoost/CatBoost) ë° DNN (Deep Neural Network) ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì • (í†µí•© í™˜ê²½)
```bash
# ë‹¨ì¼ í†µí•© í™˜ê²½ìœ¼ë¡œ GBDTì™€ DNN ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
conda env create -f environment.yaml
conda activate toss-env
```

### 2. GBDT ëª¨ë¸ ì‹¤í–‰
```bash
# XGBoost ê¸°ë³¸ ì‹¤í–‰
python train_and_predict_GBDT.py

# CatBoost ì‚¬ìš©
python train_and_predict_GBDT.py --config config_GBDT.yaml --preset catboost_deep

# Validation ratio ë³€ê²½
python train_and_predict_GBDT.py --val-ratio 0.2
```

### 3. DNN ëª¨ë¸ ì‹¤í–‰
```bash
# ë©€í‹° GPU í›ˆë ¨ (DDP)
python train_and_predict_dnn_ddp.py
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
toss-challenge/
â”œâ”€â”€ train_and_predict_GBDT.py    # GBDT ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
â”œâ”€â”€ train_and_predict_dnn_ddp.py # DNN ë©€í‹° GPU í›ˆë ¨
â”œâ”€â”€ hpo_xgboost.py               # XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
â”œâ”€â”€ hpo_catboost.py              # CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
â”œâ”€â”€ hpo_dnn.py                   # DNN í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
â”œâ”€â”€ dataset_split.py             # ë°ì´í„°ì…‹ 10-fold ë¶„í• 
â”œâ”€â”€ utils.py                     # ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ data_loader.py               # ë°ì´í„° ë¡œë” (DNN + GBDT)
â”œâ”€â”€ mixup.py                     # MixUp ë°ì´í„° ì¦ê°• í•¨ìˆ˜
â”œâ”€â”€ config_GBDT.yaml            # GBDT ì„¤ì • íŒŒì¼
â”œâ”€â”€ environment.yaml            # í†µí•© conda í™˜ê²½ ì„¤ì •
â”œâ”€â”€ analysis/                   # ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (ë³„ë„ README ì°¸ì¡°)
â””â”€â”€ data/                       # ë°ì´í„° ë””ë ‰í† ë¦¬
    â”œâ”€â”€ train.parquet           # í›ˆë ¨ ë°ì´í„° (í•„ìˆ˜)
    â””â”€â”€ test.parquet            # í…ŒìŠ¤íŠ¸ ë°ì´í„° (í•„ìˆ˜)
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í†µí•© í™˜ê²½ (environment.yaml)
ë‹¨ì¼ conda í™˜ê²½ìœ¼ë¡œ GBDTì™€ DNN ëª¨ë¸ì„ ëª¨ë‘ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬:**
- Python 3.10
- **RAPIDS ìŠ¤íƒ**: cudf, nvtabular, cupy (GPU ê°€ì† ë°ì´í„° ì²˜ë¦¬)
- **GBDT ëª¨ë¸**: XGBoost, CatBoost
- **ë”¥ëŸ¬ë‹**: PyTorch, Lightning (pip ì„¤ì¹˜ë¡œ GPU í˜¸í™˜ì„± ìµœì í™”)
- **ë°ì´í„° ì²˜ë¦¬**: pandas, numpy, scikit-learn, dask

**ì„¤ì¹˜:**
```bash
conda env create -f environment.yaml
conda activate toss-env
```

**ê¸°ì¡´ í™˜ê²½ ì—…ë°ì´íŠ¸:**
```bash
conda activate toss-env
conda env update -f environment.yaml --prune
```

## ğŸ› ï¸ GBDT ëª¨ë¸ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‹¤í–‰
```bash
# XGBoost (ê¸°ë³¸, 10% validation)
python train_and_predict_GBDT.py

# CatBoost ì‚¬ìš©
python train_and_predict_GBDT.py --config config_GBDT.yaml

# Validation ratio ë³€ê²½
python train_and_predict_GBDT.py --val-ratio 0.2

# ë°ì´í„° ì¬ì²˜ë¦¬ ê°•ì œ
python train_and_predict_GBDT.py --force-reprocess
```

### MixUp Data Augmentation
MixUpì€ ë‘ ìƒ˜í”Œì„ ì„ í˜• ë³´ê°„í•˜ì—¬ ìƒˆë¡œìš´ í•™ìŠµ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë°ì´í„° ì¦ê°• ê¸°ë²•ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ íš¨ê³¼ì ì…ë‹ˆë‹¤.

```bash
# config_GBDT.yamlì—ì„œ ì„¤ì •
training:
  use_mixup: true      # MixUp í™œì„±í™”
  mixup_alpha: 0.3     # Beta ë¶„í¬ íŒŒë¼ë¯¸í„° (0.3 ê¶Œì¥)
  mixup_ratio: 0.5     # ì¶”ê°€í•  MixUp ìƒ˜í”Œ ë¹„ìœ¨ (0.5 = 50% ì¦ê°€)
```

**MixUp íŒŒë¼ë¯¸í„°:**
- `alpha`: Beta(Î±, Î±) ë¶„í¬ì—ì„œ mixing coefficient Î»ë¥¼ ìƒ˜í”Œë§
  - Î±=1.0: ê· ë“±í•œ mixing
  - Î±<1.0: ì›ë³¸ ìƒ˜í”Œ ì„ í˜¸ (0.3 ê¶Œì¥)
  - Î±>1.0: ê· í˜•ì¡íŒ mixing ì„ í˜¸
- `ratio`: ì¶”ê°€í•  MixUp ìƒ˜í”Œì˜ ë¹„ìœ¨
  - 0.5: ì›ë³¸ì˜ 50% ì¶”ê°€ (1.5ë°° ë°ì´í„°)
  - 1.0: ì›ë³¸ê³¼ ë™ì¼ ê°œìˆ˜ ì¶”ê°€ (2ë°° ë°ì´í„°)

### ì„¤ì • íŒŒì¼ (config_GBDT.yaml)
```yaml
# ëª¨ë¸ ì„ íƒ
model:
  name: "xgboost"  # "xgboost" ë˜ëŠ” "catboost"

# Training ì„¤ì •
training:
  val_ratio: 0.1
  force_reprocess: false

# XGBoost ì„¤ì •
xgboost:
  n_estimators: 200
  learning_rate: 0.1
  max_depth: 8
  subsample: 0.8
  colsample_bytree: 0.8
  tree_method: "gpu_hist"
  gpu_id: 0
  early_stopping_rounds: 20

# CatBoost ì„¤ì •
catboost:
  n_estimators: 200
  learning_rate: 0.1
  max_depth: 8
  task_type: "GPU"
  devices: "0"
  early_stopping_rounds: 20
```

### ì¶œë ¥ íŒŒì¼
```
result_GBDT_{model}/
â”œâ”€â”€ workflow/             # NVTabular ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ *.parquet            # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â””â”€â”€ submission.csv       # ì œì¶œ íŒŒì¼
```

## ğŸ§  DNN ëª¨ë¸ ì‚¬ìš©ë²•

### ë©€í‹° GPU í›ˆë ¨
```bash
# 4ê°œ GPU ì‚¬ìš© (DDP)
python train_and_predict_dnn_ddp.py

# íŠ¹ì • GPU ì„ íƒ
CUDA_VISIBLE_DEVICES=0,1 python train_and_predict_dnn_ddp.py
```

### ì£¼ìš” ì„¤ì • (ì½”ë“œ ë‚´ë¶€)
```python
CFG = {
    'BATCH_SIZE': 1024,
    'EPOCHS': 5,
    'LEARNING_RATE': 1e-3,
    'NUM_DEVICES': 4,    # GPU ê°œìˆ˜
    'STRATEGY': 'ddp',   # ë¶„ì‚° ì „ëµ
    'VAL_RATIO': 0.1,
    'USE_MIXUP': True,   # MixUp í™œì„±í™”
    'MIXUP_ALPHA': 0.3,  # Beta ë¶„í¬ íŒŒë¼ë¯¸í„°
    'MIXUP_PROB': 0.5    # ë°°ì¹˜ë³„ MixUp ì ìš© í™•ë¥ 
}
```

### MixUp for DNN
DNN ëª¨ë¸ì€ ì˜¨ë¼ì¸ MixUpì„ ì‚¬ìš©í•˜ì—¬ ê° ë°°ì¹˜ë§ˆë‹¤ í™•ë¥ ì ìœ¼ë¡œ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**DNN MixUp íŠ¹ì§•:**
- **ì˜¨ë¼ì¸ ì¦ê°•**: í•™ìŠµ ì¤‘ ê° ë°°ì¹˜ë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ MixUp ì ìš©
- **í™•ë¥ ì  ì ìš©**: `MIXUP_PROB`ë¡œ ë°°ì¹˜ë³„ ì ìš© í™•ë¥  ì¡°ì ˆ
- **ìˆ˜ì¹˜í˜• í”¼ì²˜ ì „ìš©**: í˜„ì¬ êµ¬í˜„ì€ ì—°ì†í˜• í”¼ì²˜ì—ë§Œ MixUp ì ìš© (ë²”ì£¼í˜•/ì‹œí€€ìŠ¤ëŠ” ì›ë³¸ ìœ ì§€)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì **: ì›ë³¸ ë°ì´í„° í¬ê¸° ìœ ì§€í•˜ë©´ì„œ ì¦ê°• íš¨ê³¼

**ê¶Œì¥ ì„¤ì •:**
- `MIXUP_ALPHA`: 0.2~0.4 (0.3 ê¶Œì¥)
- `MIXUP_PROB`: 0.3~0.7 (0.5 ê¶Œì¥)

## ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (HPO)

Optunaë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.

### XGBoost HPO
```bash
# ê¸°ë³¸ ì‹¤í–‰
python hpo_xgboost.py --data-path data/train.parquet --n-trials 100 --val-ratio 0.2

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ë°ì´í„° ì„œë¸Œìƒ˜í”Œë§)
python hpo_xgboost.py --data-path data/train.parquet --n-trials 30 --subsample-ratio 0.1

# ì‹œê°„ ì œí•œ ì„¤ì •
python hpo_xgboost.py --data-path data/train.parquet --n-trials 200 --timeout 28800  # 8ì‹œê°„
```

### CatBoost HPO
```bash
# GPU ì‚¬ìš©
python hpo_catboost.py --data-path data/train.parquet --n-trials 100 --task-type GPU

# CPU ì‚¬ìš© (colsample_bylevel í¬í•¨)
python hpo_catboost.py --data-path data/train.parquet --n-trials 100 --task-type CPU

# NVTabular ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©
python hpo_catboost.py --data-path result_GBDT_catboost --n-trials 100
```

### DNN HPO
```bash
# ê¸°ë³¸ ì‹¤í–‰
python hpo_dnn.py --train-path data/train.parquet --n-trials 50 --val-ratio 0.2

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì„œë¸Œìƒ˜í”Œë§)
python hpo_dnn.py --train-path data/train.parquet --n-trials 20 --subsample-ratio 0.1 --max-epochs 5

# ì „ì²´ ìµœì í™”
python hpo_dnn.py --train-path data/train.parquet --n-trials 100 --max-epochs 15 --timeout 14400
```

### HPO ëª…ë ¹ì¤„ ì¸ì

| ì¸ì | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--data-path` / `--train-path` | ë°ì´í„° ê²½ë¡œ | í•„ìˆ˜ |
| `--n-trials` | Optuna ì‹œë„ íšŸìˆ˜ | 100 (GBDT), 50 (DNN) |
| `--val-ratio` | Validation ë¹„ìœ¨ | 0.2 |
| `--subsample-ratio` | ì‚¬ìš©í•  ë°ì´í„° ë¹„ìœ¨ | 1.0 |
| `--timeout` | ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (ì´ˆ) | None |
| `--max-epochs` | Trialë‹¹ ìµœëŒ€ ì—í¬í¬ (DNN) | 10 |
| `--patience` | Early stopping patience (DNN) | 3 |

### ìµœì í™”ë˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°

**XGBoost:**
- `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`
- `min_child_weight`, `gamma`, `reg_alpha`, `reg_lambda`, `max_bin`

**CatBoost:**
- `iterations`, `depth`, `learning_rate`, `subsample`, `l2_leaf_reg`
- `bootstrap_type`, `bagging_temperature`, `colsample_bylevel` (CPU), `border_count`

**DNN:**
- `batch_size`, `learning_rate`, `weight_decay`
- `emb_dim`, `lstm_hidden`, `cross_layers`, `n_layers`, `hidden_size`, `dropout`

### ìµœì í™” í›„ ì‚¬ìš©

ìµœì í™”ê°€ ì™„ë£Œë˜ë©´ `config_*_optimized.yaml` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

```bash
# GBDT ìµœì í™” íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í•™ìŠµ
python train_and_predict_GBDT.py --config config_GBDT_optimized.yaml

# DNNì€ ìƒì„±ëœ config íŒŒì¼ì„ ì½”ë“œì— ë°˜ì˜í•˜ì—¬ ì‚¬ìš©
```

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„

**XGBoost/CatBoost:**
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10% ë°ì´í„°, 30 trials): 5-15ë¶„
- ì¤‘ê°„ ìµœì í™” (30% ë°ì´í„°, 100 trials): 30ë¶„-1ì‹œê°„
- ìµœì¢… ìµœì í™” (100% ë°ì´í„°, 200 trials): 2-4ì‹œê°„

**DNN:**
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10% ë°ì´í„°, 20 trials, 5 epochs): 10-20ë¶„
- ì¤‘ê°„ ìµœì í™” (30% ë°ì´í„°, 50 trials, 10 epochs): 1-2ì‹œê°„
- ìµœì¢… ìµœì í™” (100% ë°ì´í„°, 100 trials, 15 epochs): 3-6ì‹œê°„

## ğŸ“Š ë°ì´í„° ì¤€ë¹„

### í•„ìˆ˜ ë°ì´í„°
```
data/
â”œâ”€â”€ train.parquet    # í›ˆë ¨ ë°ì´í„° (10.7M rows)
â””â”€â”€ test.parquet     # í…ŒìŠ¤íŠ¸ ë°ì´í„°
```

### ë°ì´í„° ë¶„í•  (Optional)
```bash
# 10-fold ë°ì´í„° ë¶„í• 
python dataset_split.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” `clicked` ê°’ì— ë”°ë¼ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤:
- `clicked=1`: ëª¨ë“  foldì— í¬í•¨ (positive ìƒ˜í”Œ ë³´ì¡´)
- `clicked=0`: 10ê°œ foldë¡œ ë¶„í• 

## ğŸ¯ í”¼ì²˜ ì„¤ëª…

### ë²”ì£¼í˜• í”¼ì²˜ (5ê°œ)
- `gender`, `age_group`, `inventory_id`, `day_of_week`, `hour`

### ì—°ì†í˜• í”¼ì²˜ (110ê°œ)
- `feat_a_*` (18ê°œ)
- `feat_b_*` (6ê°œ)
- `feat_c_*` (8ê°œ)
- `feat_d_*` (6ê°œ)
- `feat_e_*` (10ê°œ)
- `history_a_*` (7ê°œ)
- `history_b_*` (30ê°œ)
- `l_feat_*` (25ê°œ, `l_feat_20`, `l_feat_23` ì œì™¸)

### ì‹œí€€ìŠ¤ í”¼ì²˜ (DNNë§Œ ì‚¬ìš©)
- `seq`: ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ë°ì´í„°

### ì œì™¸ í”¼ì²˜
- `l_feat_20`, `l_feat_23` (ìƒìˆ˜ ê°’)

## ğŸ“ˆ í‰ê°€ ë©”íŠ¸ë¦­

ëŒ€íšŒ ë©”íŠ¸ë¦­: `Score = 0.5 * AP + 0.5 * (1 / (1 + WLL))`

- **AP (Average Precision)**: 50% ê°€ì¤‘ì¹˜
- **WLL (Weighted LogLoss)**: 50% ê°€ì¤‘ì¹˜ (í´ë˜ìŠ¤ 50:50 ê· í˜•)

## ğŸ’¾ ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥

### GBDT ëª¨ë¸
- **GPU ë©”ëª¨ë¦¬**: 10-14GB (RTX 3090 ê¸°ì¤€)
- **System RAM**: 32GB+ ê¶Œì¥
- **ì²˜ë¦¬ ì†ë„**: ì „ì²´ ë°ì´í„° ë‹¨ì¼ í•™ìŠµ ì•½ 10-30ë¶„

### DNN ëª¨ë¸
- **GPU ë©”ëª¨ë¦¬**: 24GB per GPU (4 GPUs ê¶Œì¥)
- **System RAM**: 64GB+ ê¶Œì¥
- **ì²˜ë¦¬ ì†ë„**: 5 epochs ì•½ 30ë¶„-1ì‹œê°„

## ğŸ” ë°ì´í„° ë¶„ì„

`analysis/` ë””ë ‰í† ë¦¬ì— ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `analysis/README.md`ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

**ì£¼ìš” ë¶„ì„ ë„êµ¬:**
- `chunk_eda.py`: ëŒ€ìš©ëŸ‰ ë°ì´í„° EDA (ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬)
- `feature_quality_analysis.py`: í”¼ì²˜ í’ˆì§ˆ ë¶„ì„
- `compute_normalization_stats.py`: í‘œì¤€í™” í†µê³„ ê³„ì‚°
- Missing pattern ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

## ğŸ“ ì£¼ìš” ì—…ë°ì´íŠ¸

- **2025-10-08**: MixUp ë°ì´í„° ì¦ê°• ê¸°ë²• ì¶”ê°€ (GBDT/DNN ëª¨ë‘ ì§€ì›)
- **2025-10-08**: í†µí•© í™˜ê²½ (environment.yaml) êµ¬ì„± - RAPIDS + PyTorch ë‹¨ì¼ í™˜ê²½
- **2025-01-08**: Cross-validation â†’ Train/Val split ë³€ê²½ (validation ratio 0.1)
- **2025-01-08**: ì½”ë“œ ì •ë¦¬ ë° êµ¬ì¡°í™”, ê³µí†µ í•¨ìˆ˜ í†µí•©
- **2024-12-31**: HPO (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”) ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
- **2024-12-30**: GBDT ëª¨ë¸ (XGBoost/CatBoost) ì¶”ê°€
- **2024-12-29**: DNN ë©€í‹° GPU (DDP) ì§€ì› ì¶”ê°€

## ğŸ› ë¬¸ì œ í•´ê²°

### torch ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
```bash
# í†µí•© í™˜ê²½ ì¬ì„¤ì¹˜
conda env remove -n toss-env
conda env create -f environment.yaml
conda activate toss-env
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GBDT: ë°ì´í„° ì„œë¸Œìƒ˜í”Œë§ ë˜ëŠ” ì‘ì€ batch_size ì‚¬ìš©
python train_and_predict_GBDT.py --val-ratio 0.1  # validation ë¹„ìœ¨ ì¤„ì´ê¸°

# DNN: GPU ê°œìˆ˜ ì¡°ì • ë˜ëŠ” batch size ê°ì†Œ
CUDA_VISIBLE_DEVICES=0,1 python train_and_predict_dnn_ddp.py
```

### cuDF string limit ì—ëŸ¬
Raw parquet íŒŒì¼ ì‚¬ìš© ì‹œ ìë™ìœ¼ë¡œ `seq` ì»¬ëŸ¼ì´ ì œì™¸ë©ë‹ˆë‹¤.

## ğŸ¤ ì°¸ê³ ì‚¬í•­

- ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¨ì¼ í†µí•© í™˜ê²½ (`toss-env`)ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤
- GBDTì™€ DNN ëª¨ë¸ ëª¨ë‘ ë™ì¼í•œ conda í™˜ê²½ ì‚¬ìš©
- GPUëŠ” í•„ìˆ˜ì´ë©°, CUDA 11.8+ í™˜ê²½ ê¶Œì¥
- ë°ì´í„° ë¶„ì„ ë„êµ¬ëŠ” `analysis/README.md` ì°¸ì¡°
